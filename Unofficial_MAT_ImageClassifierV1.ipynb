{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Official_MAT_ImageClassifierV1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DocLock17/ISAR_Final/blob/main/Unofficial_MAT_ImageClassifierV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_pSkXH4KNHy"
      },
      "source": [
        "# Modular Abstraction Tranfer Learning Image Classifier\n",
        "\n",
        "A DeepMind based experiment in feature abstraction as applied to Deep-Q Reinforcement learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkaYtGXuAaOv"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1EFy_NoE7Wl"
      },
      "source": [
        "#### Selector Menu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKej2sIyydrJ"
      },
      "source": [
        "# Import and Unzip Hosted Dataset\n",
        "CREATE_MISSING_DIRECTORIES = True #@param {type:\"boolean\"}\n",
        "IMPORT_MAIN_DATA = True #@param {type:\"boolean\"}\n",
        "IMPORT_SENTIMENT_DATA = True #@param {type:\"boolean\"}\n",
        "IMPORT_MODEL_DATA = True #@param {type:\"boolean\"}\n",
        "\n",
        "BATCH_SIZE =  128#@param {type:\"integer\"}\n",
        "\n",
        "# Ranges 0 - 1\n",
        "SCALED_ONE_ZERO = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Ranges -1 - 1\n",
        "SCALED_ONE_ONE = True #@param {type:\"boolean\"}\n",
        "\n",
        "IMAGE_SIZE = (104, 80)\n",
        "\n",
        "\n",
        "MODEL_NAME = \"m1_MAT_ImageClassifier_v3\" #@param {type:\"string\"}\n",
        "SLICE_ONE_NAME = \"s1_MAT_ImageClassifier_v3\" #@param {type:\"string\"}\n",
        "SLICE_TWO_NAME = \"s2_MAT_ImageClassifier_v3\" #@param {type:\"string\"}\n",
        "SLICE_THREE_NAME = \"s3_MAT_ImageClassifier_v3\" #@param {type:\"string\"}\n",
        "\n",
        "EXTRACT_SLICE_ONE = True #@param {type:\"boolean\"}\n",
        "EXTRACT_SLICE_TWO = True #@param {type:\"boolean\"}\n",
        "EXTRACT_SLICE_THREE = True #@param {type:\"boolean\"}\n",
        "\n",
        "TF_LITE_MODEL = True #@param {type:\"boolean\"}\n",
        "ZIP_OUTPUT = True #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGIl3BKliih1"
      },
      "source": [
        "#### Setup Model Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFzeNuWrURPP"
      },
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  COLAB = True\n",
        "  \n",
        "except:\n",
        "  !pip install gdown\n",
        "  COLAB = False\n",
        "\n",
        "model_dir = 'ISAR_Model_Data'\n",
        "data_dir = 'ISAR_Main_Classification'\n",
        "transfer_dir = 'ISAR_Sentiment_Transfer'\n",
        "\n",
        "\n",
        "if IMPORT_MAIN_DATA :\n",
        "    if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1P7o1x4ZpPbd16VQDwaMzllbN-tlfqqIH\n",
        "      !unzip ISAR_Main_Classification.zip\n",
        "      !rm ISAR_Main_Classification.zip\n",
        "if IMPORT_SENTIMENT_DATA:\n",
        "    if not os.path.exists(transfer_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1UDUNnw04q5cvms5ibM6pNn-wXtJphXxZ\n",
        "      !unzip ISAR_Sentiment_Transfer.zip\n",
        "      !rm ISAR_Sentiment_Transfer.zip\n",
        "if IMPORT_MODEL_DATA:\n",
        "    if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1IVw1bEAmMDiVeHfPPBWBfMj3h9WliumD\n",
        "      !unzip ISAR_Model_Data.zip\n",
        "      !rm ISAR_Model_Data.zip\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poXz23gKsczB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbd6e8e0-c01b-4e88-e990-cde8f36cb697"
      },
      "source": [
        "if CREATE_MISSING_DIRECTORIES:  \n",
        "  if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Main Data Directory\")\n",
        "    os.makedirs(data_dir)\n",
        "  else:\n",
        "    print(\"Main Data Directory Found\")\n",
        "  if not os.path.exists(transfer_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Transfer Data Directory\")\n",
        "    os.makedirs(transfer_dir)\n",
        "  else:\n",
        "    print(\"Transfer Data Directory Found\")\n",
        "  if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Model Directory\")\n",
        "    os.makedirs(model_dir)\n",
        "  else:\n",
        "    print(\"Model Directory Found\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Main Data Directory Found\n",
            "Creating Transfer Data Directory\n",
            "Model Directory Found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNp06xZqiWKD"
      },
      "source": [
        "#### Setup Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjxfLJqifsa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7238047a-2d9e-499d-cbc0-f388a97e0505"
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# from keras.layers import *\n",
        "# from keras.models import Sequential\n",
        "import keras\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.python.client import device_lib\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "# print(device_lib.list_local_devices())\n",
        "\n",
        "# Set Memory Growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Currently, memory growth needs to be the same across GPUs\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "      print(gpu)\n",
        "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version: 2.4.1\n",
            "Hub version: 0.11.0\n",
            "Num GPUs Available:  1\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "1 Physical GPUs, 1 Logical GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL54LWCHt5q5"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWcwpgf2GH1M"
      },
      "source": [
        "Originally we Looked at Using Color Photos. However, this would be a bit unfair to our bot because the data is presented in a stack of grayscaled images. \n",
        "\n",
        "![021921_030056_379.jpg](data:image/jpeg;base64,iVBORw0KGgoAAAANSUhEUgAAAUAAAADSCAIAAAC0K44BAAAERElEQVR4nO3bQYocRxAFUJ1CO++1npWOYfDK9xFzEN3B97NgwLRJVXb3VEZn/qz3iIUYwSdVFb+6JFpfvgAAAAAAAAAAAFDk7cfbkJn956iSfn1+vn8fMrPOX831uaO6AOkFS1ddAAW7Y1RhXnnmV0q/PqMK88ozv5Lrc0d1AdILlq66AAp2hwL0pV8fBehzfe6Y9Yn67NT86fe32ieqgoV9wlRLvz4K0Of63LHa32lXK1i61T5RFcwr6P+kX5+jhX52Zp2/mutzR3UB0guWrroACgYAAAAAAAAAAADAdby///Fr5MsnWP82n18C+Xvns4T2Rt7+pGKN5O+UzxI+bmT/ZsuXz0Jub2o78uWztKNXqVG3Wf7e+UzWf0KPWiD5u+YzWfoCyZ+bz2T9WzhqgeTvms80/Wfw+Se0/L3zAQAAAAAAAIAi1V+yk793PtMc3dpRt1n+3vlMdnsL25s6aoHk75rPNI88m888p+Xvnc9k/Sfx+ee0/L3zmezopvZvtnz5LKR9Eo99NsvfO5/J0hdI/tx8JktfIPlz85msfwvHLpD8/fIBAAAAAAAAAIBXa7/KJ3+nfDaU/l1f+VzU0eqMWiP5c/PZXLsWR69w5xdU/uvz2Vz6gsrn0vovaRWviPJfmc+FVD/15c/NZ1v9RTm/RvLn5nMJ6Z8w8gEAAAAAAAAAHnT7Nf2jr+zLr8uHTzpazVFrKh9KtCvY/vrMmsqHcumfYOn5MED1asqHQu06VhRAPgzWX8RRr6DyocTRP8y0vyu/Ih8GSH8FTc8HAAAAAAAAAAB2kP6/edLz4ZOOVnPUmsqHEkcrOGpN5UOh9AKk58Mpj7wcjiqAfCh0tI6jVlM+FDpaxLEFkA8l0guQng+npBcgPR+e1v973fl/npEPAAAAAAAAAPBp1V9OkA+Dtd8rGvtNI/lQov8lwaOfyB+VD6c88tkyqgDyoUR/+Ua9gsqHEukFSM+HAY4WcdSCyody6TVIz4dT0guQng8DtOs4dkHlAwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAwET//PXttzP7XI9yfi4tfYGcn4s6Wp2UNXJ+Li19gZyfS0tfIOfn0tIXyPm5tPQFcn4uLX2BnJ9LS18g5+fS0hfI+bmoR1Zn5TVKP/+H3JMzWXoB0s//IffkTHa0IilrpMBcmgKvIPfkTHa7HO2v11+j9AKnn5/JFHiu9PMz2SNrsfICPXI252dbCjxX+vkBAAAAAAAAAAAg1tuPt18z+xTAk26rq8YQ46OubWnVGAL0C6zGsKh+Ob1UQwyv0BCp/aTt/wRYyFFdvULD0vrV9dkLS/PyDL/x599fjTETZ5PS3n6KTj+MWWR+vn//b6YfpmgWLeSzo8CmHQWOGQU27ShwzCiwaUeBY0aBTTsKbIxZehTYmOBRYGOCR4GNCR4FNiZ4FNiY4FFgY4JHgY0JHgU2Jnj+BdgphhtQzVYKAAAAAElFTkSuQmCC)\n",
        "![022421_21520427.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYw-nQSwGcEB"
      },
      "source": [
        "After Some Thought we decided to execute initial classification training on the same stacked grayscale frames our bot can expect to see in the \"real\" world. Similiar to DeepMinds original implementation our bot will stack 3 gray scale images (DeepMind uses 4 but says 3-5 is ideal). This process allows the bot to process temporal changes in a single frame and we can train it with a standard image data generator.\n",
        "\n",
        "![Notebookpic.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5yMofoNEMcL"
      },
      "source": [
        "#### Setup Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umB5tswsfTEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cceb9a2-2921-46f9-cbc0-efacec972f3c"
      },
      "source": [
        "#### Setup Data Generator\n",
        "if SCALED_ONE_ZERO:\n",
        "  datagen_kwargs = dict(rescale=1./255, validation_split=.10)\n",
        "\n",
        "if SCALED_ONE_ONE:\n",
        "  datagen_kwargs = dict(featurewise_center=True, samplewise_center=True,\n",
        "                      rescale = 2/255, validation_split=.10)\n",
        "\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
        "                   interpolation=\"bilinear\")\n",
        "\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    **datagen_kwargs)\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"validation\", shuffle=True, **dataflow_kwargs)\n",
        "\n",
        "do_data_augmentation = True #@/param {type:\"boolean\"}\n",
        "if do_data_augmentation:\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rotation_range=30,\n",
        "      horizontal_flip=True,\n",
        "      width_shift_range=0.2, height_shift_range=0.2,\n",
        "      shear_range=0.2, zoom_range=0.2,\n",
        "      **datagen_kwargs)\n",
        "else:\n",
        "  train_datagen = valid_datagen\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 18000 images belonging to 12 classes.\n",
            "Found 162000 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjEs64QEGTmo"
      },
      "source": [
        "## Create and Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzwdPrnfx1QZ"
      },
      "source": [
        "#### Changes?\n",
        "\n",
        "### In the original architecture created by DeepMind frames were resized to 84 x 84, however, the ultimate goal of this project is to generalize across games so we will use 104 x 80 since it better represents the the game scale which will likely warp the representation of time the new scale isn't terribly computationally different and appears to train effectively in preliminary tests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u47XJEsjKt0"
      },
      "source": [
        "#### Define Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQEq_KpOSinl",
        "outputId": "e1c54290-bb26-4959-efe4-ed04b2580119"
      },
      "source": [
        "# Slice One\n",
        "s1_input_layer = tf.keras.Input(shape=IMAGE_SIZE + (3,))\n",
        "s1_conv1 = tf.keras.layers.Conv2D(32, kernel_size=(8, 8),strides=4, activation='relu', name='S1_Conv1', \\\n",
        "                            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s1_input_layer)\n",
        "s1_output = tf.keras.layers.Conv2D(64, kernel_size=(4, 4),strides=2, activation='relu', name='S1_Conv2', \\\n",
        "                            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s1_conv1)\n",
        "s1_model = tf.keras.Model(inputs=s1_input_layer, outputs=s1_output)\n",
        "s1_model.summary()\n",
        "\n",
        "\n",
        "# Slice Two\n",
        "s2_input_layer = tf.keras.Input(shape=(11, 8, 64))\n",
        "s2_conv1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),strides=1, activation='relu', name='S2_Conv1', \\\n",
        "                            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_input_layer)\n",
        "s2_flat_1 = tf.keras.layers.Flatten(name='S2_Flat1')(s2_conv1)\n",
        "s2_output = tf.keras.layers.Dense(512, activation='relu', name='s2_Dense_1', \\\n",
        "                            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_flat_1)\n",
        "s2_model = tf.keras.Model(inputs=s2_input_layer, outputs=s2_output)\n",
        "s2_model.summary()\n",
        "\n",
        "\n",
        "# Slice Three\n",
        "s3_input_layer = tf.keras.Input(shape=(512))\n",
        "s3_output = tf.keras.layers.Dense(train_generator.num_classes, activation='softmax', name ='Final_Out', \\\n",
        "                            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "s3_model.summary()\n",
        "\n",
        "\n",
        "model_input = tf.keras.Input(shape=(IMAGE_SIZE+(3,)))\n",
        "s1_pass = s1_model(model_input)\n",
        "s2_pass = s2_model(s1_pass)\n",
        "s3_final_output = s3_model(s2_pass)\n",
        "model = tf.keras.Model(inputs=model_input, outputs=s3_final_output)\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 104, 80, 3)]      0         \n",
            "_________________________________________________________________\n",
            "S1_Conv1 (Conv2D)            (None, 25, 19, 32)        6176      \n",
            "_________________________________________________________________\n",
            "S1_Conv2 (Conv2D)            (None, 11, 8, 64)         32832     \n",
            "=================================================================\n",
            "Total params: 39,008\n",
            "Trainable params: 39,008\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 11, 8, 64)]       0         \n",
            "_________________________________________________________________\n",
            "S2_Conv1 (Conv2D)            (None, 9, 6, 64)          36928     \n",
            "_________________________________________________________________\n",
            "S2_Flat1 (Flatten)           (None, 3456)              0         \n",
            "_________________________________________________________________\n",
            "s2_Dense_1 (Dense)           (None, 512)               1769984   \n",
            "=================================================================\n",
            "Total params: 1,806,912\n",
            "Trainable params: 1,806,912\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_3 (InputLayer)         [(None, 512)]             0         \n",
            "_________________________________________________________________\n",
            "Final_Out (Dense)            (None, 12)                6156      \n",
            "=================================================================\n",
            "Total params: 6,156\n",
            "Trainable params: 6,156\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 104, 80, 3)]      0         \n",
            "_________________________________________________________________\n",
            "model (Functional)           (None, 11, 8, 64)         39008     \n",
            "_________________________________________________________________\n",
            "model_1 (Functional)         (None, 512)               1806912   \n",
            "_________________________________________________________________\n",
            "model_2 (Functional)         (None, 12)                6156      \n",
            "=================================================================\n",
            "Total params: 1,852,076\n",
            "Trainable params: 1,852,076\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "#### Setup Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0mq4Md5ihWj"
      },
      "source": [
        "# Checkpoint\n",
        "model_checkpoint_path = model_dir + \"/ISAR_Production_Models/\" + MODEL_NAME + \"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
        "checkpoint = ModelCheckpoint(model_checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaJCJdDy4167"
      },
      "source": [
        "model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "s1_model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "s2_model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "s3_model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(lr=0.0042, momentum=0.87),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.25),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCiaRy6mHOD9"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_YKX2Qnfg6x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01958aaa-21e9-4ab4-f9bb-3837e4b33d5d"
      },
      "source": [
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=1, steps_per_epoch=steps_per_epoch, callbacks=callbacks_list,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=validation_steps).history"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  57/1265 [>.............................] - ETA: 7:26 - loss: 2.2554 - accuracy: 0.2812"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZEfkjSIrbkd"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "II0kaTuarbke"
      },
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJu-dEqqGoTw"
      },
      "source": [
        "#### Toggle Trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goiuF2ZMuxI5"
      },
      "source": [
        "## Lets try to push some of the distinctions to the back\n",
        "\n",
        "model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "s1_model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "s2_model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "s3_model.trainable = False #@param {type:\"boolean\"}\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(lr=0.0042, momentum=0.87),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.25),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSZGm6jWHQdV"
      },
      "source": [
        "#### Train More?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcKPSWRnuniT"
      },
      "source": [
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=1, steps_per_epoch=steps_per_epoch, callbacks=callbacks_list,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=validation_steps).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwYDUQAFrkAe"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaArgKnorkAf"
      },
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjGLDruLsGmi"
      },
      "source": [
        "#### Toggle Trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBZKm552Fd_O"
      },
      "source": [
        "# Couple more to let the model balance\n",
        "model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "s1_model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "s2_model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "s3_model.trainable = True #@param {type:\"boolean\"}\n",
        "\n",
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(lr=0.0042, momentum=0.87),\n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.25),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWPF_GpDFd_Y"
      },
      "source": [
        "#### Train More?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz_ooXcjFd_Y"
      },
      "source": [
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "hist = model.fit(\n",
        "    train_generator,\n",
        "    epochs=1, steps_per_epoch=steps_per_epoch, callbacks=callbacks_list,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=validation_steps).history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_xDuC8Wrm6Z"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogKBqudKrm6Z"
      },
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KFXxRwiGg-x"
      },
      "source": [
        "## Test and Validate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk0JjB46t6nI"
      },
      "source": [
        "#### Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW-IRVD-t6nH"
      },
      "source": [
        "def get_class_string_from_index(index):\n",
        "   for class_string, class_index in valid_generator.class_indices.items():\n",
        "      if class_index == index:\n",
        "         return class_string\n",
        "\n",
        "x, y = next(valid_generator)\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0])\n",
        "plt.imshow(np.clip(image, 0, 1))\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "print(\"True label: \" + get_class_string_from_index(true_index))\n",
        "print(\"Predicted label: \" + get_class_string_from_index(predicted_index))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et51VwKtjop8"
      },
      "source": [
        "#### Validate Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqx6E0NzKmuU"
      },
      "source": [
        "import math\n",
        "\n",
        "class_labels = sorted(valid_generator.class_indices.items(), key=lambda pair:pair[1])\n",
        "class_labels = np.array([key.title() for key, value in class_labels])\n",
        "\n",
        "for image_batch, label_batch in valid_generator:\n",
        "  print(\"Image batch shape: \", image_batch.shape)\n",
        "  print(\"Label batch shape: \", label_batch.shape)\n",
        "  break\n",
        "\n",
        "label_id = np.argmax(label_batch, axis=-1)\n",
        "predicted_class_floats = model.predict(image_batch)\n",
        "\n",
        "predicted_index = np.argmax(predicted_class_floats, axis=-1)\n",
        "predicted_label_batch = class_labels[predicted_index]\n",
        "\n",
        "sqr = math.sqrt(BATCH_SIZE)\n",
        "# if BATCH_SIZE % sqr == 0:\n",
        "#   plt.figure(figsize=(sqr+12,sqr+12))\n",
        "\n",
        "base = BATCH_SIZE/8\n",
        "if base % 1 != 0:\n",
        "  print(\"fix me\")\n",
        "else:\n",
        "  plt.figure(figsize=(20,20))\n",
        "  plt.subplots_adjust(hspace=0.5)\n",
        "  for n in range(BATCH_SIZE):\n",
        "    plt.subplot(8,base,n+1)\n",
        "    plt.imshow(np.clip(image_batch[n], 0, 1))\n",
        "    color = \"green\" if predicted_index[n] == label_id[n] else \"red\"\n",
        "    plt.title(predicted_label_batch[n].title(), color=color)\n",
        "    plt.axis('off')\n",
        "  _ = plt.suptitle(\"Model predictions (green: correct, red: incorrect)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI0_SNnzt6nI"
      },
      "source": [
        "## Saving the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBBC2xKHE2GW"
      },
      "source": [
        "#### Creating TFserve Version for API Deployment & Slices for transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGvTi69oIc2d"
      },
      "source": [
        "saved_model_path = model_dir + \"/ISAR_Production_Models/\" + MODEL_NAME\n",
        "tf.saved_model.save(model, saved_model_path)\n",
        "\n",
        "if EXTRACT_SLICE_ONE:\n",
        "    tf.saved_model.save(s1_model, model_dir + \"/ISAR_Production_Models/\" + SLICE_ONE_NAME)\n",
        "    \n",
        "if EXTRACT_SLICE_TWO:\n",
        "    tf.saved_model.save(s2_model, model_dir + \"/ISAR_Production_Models/\" + SLICE_TWO_NAME)\n",
        "\n",
        "if EXTRACT_SLICE_THREE:\n",
        "    tf.saved_model.save(s3_model, model_dir + \"/ISAR_Production_Models/\" + SLICE_THREE_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv0PrCSMt6nI"
      },
      "source": [
        "#### Creating a TFlite Version For Edge Device Deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va1Vo92fSyV6"
      },
      "source": [
        "if TF_LITE_MODEL:\n",
        "  optimize_lite_model = True  #@param {type:\"boolean\"}\n",
        "  num_calibration_examples = 128  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "  representative_dataset = None\n",
        "  if optimize_lite_model and num_calibration_examples:\n",
        "    representative_dataset = lambda: itertools.islice(\n",
        "        ([image[None, ...]] for batch, _ in train_generator for image in batch),\n",
        "        num_calibration_examples)\n",
        "\n",
        "  converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "  if optimize_lite_model:\n",
        "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "    if representative_dataset: \n",
        "      converter.representative_dataset = representative_dataset\n",
        "  lite_model_content = converter.convert()\n",
        "\n",
        "  with open(model_dir + \"/ISAR_Production_Models/\" + MODEL_NAME + \"_TF_LITE\", \"wb\") as f:\n",
        "    f.write(lite_model_content)\n",
        "  print(\"Wrote %sTFLite model of %d bytes.\" %\n",
        "        (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cd0PfP4mt6nJ"
      },
      "source": [
        "#### Load TFlite for Evalutation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wqEmD0xIqeG"
      },
      "source": [
        "if TF_LITE_MODEL:\n",
        "  interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
        "  def lite_model(images):\n",
        "    interpreter.allocate_tensors()\n",
        "    interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "    interpreter.invoke()\n",
        "    return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqC2GY_ot6nJ"
      },
      "source": [
        "#### Evaluate TFlite Version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMMK-fZrKrk8"
      },
      "source": [
        "if TF_LITE_MODEL:\n",
        "  num_eval_examples = 50  #@param {type:\"slider\", min:0, max:700}\n",
        "  eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
        "                  for batch in train_generator\n",
        "                  for (image, label) in zip(*batch))\n",
        "  count = 0\n",
        "  count_lite_tf_agree = 0\n",
        "  count_lite_correct = 0\n",
        "  for image, label in eval_dataset:\n",
        "    probs_lite = lite_model(image[None, ...])[0]\n",
        "    probs_tf = model(image[None, ...]).numpy()[0]\n",
        "    y_lite = np.argmax(probs_lite)\n",
        "    y_tf = np.argmax(probs_tf)\n",
        "    y_true = np.argmax(label)\n",
        "    count +=1\n",
        "    if y_lite == y_tf: count_lite_tf_agree += 1\n",
        "    if y_lite == y_true: count_lite_correct += 1\n",
        "    if count >= num_eval_examples: break\n",
        "  print(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n",
        "        (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
        "  print(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n",
        "        (count_lite_correct, count, 100.0 * count_lite_correct / count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJI-cSuX1uiz"
      },
      "source": [
        "%%capture\n",
        "if ZIP_OUTPUT:\n",
        "  zip = !zip -r ISAR_Model_Data_Output.zip ISAR_Model_Data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}