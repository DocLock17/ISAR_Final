{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModularAbstractionTransferPlaygroundV1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "n1EFy_NoE7Wl",
        "BGIl3BKliih1",
        "HNp06xZqiWKD",
        "tdX2lI15iCvS",
        "0vtDg55hojmL",
        "8z5loBrJiHON",
        "qgI4QKIOo6Nl"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXiHbkQgfw9a"
      },
      "source": [
        "# Modular Abstraction Transfer Playground \n",
        "\n",
        "### A Deep Mind Based Experiment In Abstaction And Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynM5BeDfWQvH"
      },
      "source": [
        "#### Interactive Playgound (run after notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrFuoAxKQclk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abfd906e-f921-4f08-b149-1690c23b965b"
      },
      "source": [
        "# # # Run From Top (Hint: It must be loaded commented first)\n",
        "myPlayground()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~{0_o}~0   ~{o_0}~0   ~{o_o}~0   \n",
            "\n",
            "~{0_0}~0   ~{o_0}~0   ~{0_o}~0   ~{o_0}~0   ~{o_0}~0   ~{o_o}~0   ~{o_0}~0   ~{0_o}~0   \n",
            "\n",
            "~{0_o}~0   ~{0_0}~0   ~{0_0}~1   ~{o_o}~1   ~{0_o}~1   ~{0_0}~1   ~{o_o}~1   ~{0_o}~1   \n",
            "\n",
            " `{x_X}~   \n",
            "\n",
            "Episode: 1/1500, Episode Score: 1.0, Avg Episode Score: 1.0, Survival Time: 620\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 620, Memory Size: 10811, Current Epsilon Value: 0.98\n",
            "\n",
            "\n",
            "~{0_0}~0   ~{0_o}~0   ~{0_o}~0   ~{0_o}~0   ~{o_o}~0   ~{o_o}~0   ~{0_o}~0   ~{0_0}~0   \n",
            "\n",
            "~{o_0}~0   ~{o_0}~0   ~{o_0}~0   ~{o_0}~0     {-_-}    ~{0_0}~0   ~{0_0}~0   ~{0_o}~0   \n",
            "\n",
            "~{o_o}~0   ~{o_0}~0   ~{o_0}~1   ~{o_0}~1   ~{o_0}~1   ~{o_o}~1    `{x_X}~   \n",
            "\n",
            "Episode: 2/1500, Episode Score: 1.0, Avg Episode Score: 1.0, Survival Time: 680\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 1300, Memory Size: 11491, Current Epsilon Value: 0.96\n",
            "\n",
            "\n",
            "~{0_o}~0   ~{0_0}~0   ~{o_0}~0   ~{o_0}~0   ~{0_0}~1   ~{o_0}~1   ~{o_0}~1   ~{o_o}~1   \n",
            "\n",
            "~{o_0}~1   ~{0_o}~1   ~{o_0}~1   ~{o_0}~1   ~{o_0}~1   ~{0_o}~1   ~{o_0}~1   ~{o_o}~1   \n",
            "\n",
            "~{o_0}~1   ~{o_o}~1   ~{0_0}~1   ~{0_o}~1   ~{0_o}~1   ~{o_0}~2     {-_-}    ~{0_0}~2   \n",
            "\n",
            "~{0_0}~2   ~{o_o}~2   ~{o_0}~2   ~{o_0}~2   ~{0_0}~3   ~{o_o}~3   ~{o_0}~3   ~{0_o}~3   \n",
            "\n",
            " `{x_X}~   \n",
            "\n",
            "Episode: 3/1500, Episode Score: 3.0, Avg Episode Score: 1.667, Survival Time: 999\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 2299, Memory Size: 12490, Current Epsilon Value: 0.93\n",
            "\n",
            "\n",
            "~{o_0}~0   ~{o_0}~0   ~{0_o}~0   ~{o_0}~0   ~{0_0}~0   ~{o_0}~0   ~{o_o}~1   ~{o_0}~1   \n",
            "\n",
            "~{o_0}~1   ~{0_o}~1   ~{0_o}~1   ~{0_o}~1   ~{0_o}~1   ~{o_0}~1   ~{0_0}~1   ~{o_0}~1   \n",
            "\n",
            "~{o_0}~1   ~{o_0}~1   ~{0_0}~1   ~{0_0}~1   ~{o_o}~1   ~{o_0}~1    `{x_X}~   \n",
            "\n",
            "Episode: 4/1500, Episode Score: 1.0, Avg Episode Score: 1.5, Survival Time: 690\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 2989, Memory Size: 13180, Current Epsilon Value: 0.91\n",
            "\n",
            "\n",
            "  {-_-}    ~{o_0}~0   ~{o_0}~0   ~{0_0}~0   ~{0_0}~0   ~{o_o}~0   ~{o_0}~0   ~{0_o}~0   \n",
            "\n",
            "~{0_o}~1   ~{o_0}~1   ~{o_o}~1   ~{o_o}~1   ~{o_0}~1   ~{o_o}~1   ~{o_0}~1   ~{0_o}~1   \n",
            "\n",
            "~{0_o}~2   ~{0_0}~2   ~{0_o}~2   ~{0_0}~2   ~{0_o}~2   ~{0_0}~2   ~{0_0}~2   ~{o_o}~2   \n",
            "\n",
            "~{0_0}~2   ~{o_0}~2    `{x_X}~   \n",
            "\n",
            "Episode: 5/1500, Episode Score: 2.0, Avg Episode Score: 1.6, Survival Time: 814\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 3803, Memory Size: 13994, Current Epsilon Value: 0.88\n",
            "\n",
            "\n",
            "~{o_o}~0   ~{0_0}~0   ~{o_o}~0   ~{0_o}~0   ~{o_o}~0   ~{0_o}~0     {-_-}    ~{0_o}~1   \n",
            "\n",
            "~{o_o}~1   ~{0_0}~1   ~{o_0}~1   ~{o_o}~1   ~{0_0}~2   ~{0_o}~2   ~{o_o}~2   ~{o_0}~2   \n",
            "\n",
            "~{o_0}~2   ~{0_o}~2   ~{0_o}~2   ~{o_o}~2   ~{0_0}~2   ~{0_0}~2   ~{0_o}~2   ~{o_0}~2   \n",
            "\n",
            "~{0_0}~2   ~{o_0}~2   ~{o_o}~2    `{x_X}~   \n",
            "\n",
            "Episode: 6/1500, Episode Score: 2.0, Avg Episode Score: 1.667, Survival Time: 811\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 4614, Memory Size: 14805, Current Epsilon Value: 0.86\n",
            "\n",
            "\n",
            "~{0_o}~0   ~{0_o}~0   ~{o_0}~0   ~{0_o}~0   ~{o_o}~0   ~{0_o}~0   ~{o_0}~0   ~{0_0}~0   \n",
            "\n",
            "~{o_0}~0   ~{o_o}~0   ~{0_o}~0   ~{0_0}~0     {-_-}    ~{0_0}~0   ~{0_0}~0   ~{o_o}~0   \n",
            "\n",
            " `{x_X}~   \n",
            "\n",
            "Episode: 7/1500, Episode Score: 0, Avg Episode Score: 1.429, Survival Time: 504\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 5118, Memory Size: 15309, Current Epsilon Value: 0.85\n",
            "\n",
            "\n",
            "~{0_0}~0   ~{0_0}~0   ~{o_0}~0   ~{o_o}~0   ~{o_0}~0   ~{o_o}~0   ~{0_0}~0   ~{0_o}~0   \n",
            "\n",
            "~{o_o}~0   ~{o_0}~0   ~{0_o}~0   ~{0_0}~0   ~{0_o}~0   ~{o_o}~0   ~{0_o}~0   ~{0_o}~1   \n",
            "\n",
            "~{o_0}~1   ~{o_0}~1   ~{0_o}~1   ~{o_0}~1   ~{o_0}~1   ~{0_o}~1    `{x_X}~   \n",
            "\n",
            "Episode: 8/1500, Episode Score: 1.0, Avg Episode Score: 1.375, Survival Time: 695\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 5813, Memory Size: 16004, Current Epsilon Value: 0.83\n",
            "\n",
            "\n",
            "~{0_o}~0   ~{0_o}~0   ~{0_0}~0   ~{0_o}~0   ~{0_o}~0   ~{o_0}~1     {-_-}    ~{0_0}~1   \n",
            "\n",
            "~{o_0}~1   ~{0_o}~1   ~{o_0}~1   ~{0_0}~2   ~{0_0}~2   ~{0_o}~2   ~{0_o}~2   ~{o_o}~2   \n",
            "\n",
            "~{0_0}~2   ~{0_o}~2   ~{0_o}~2   ~{o_0}~3   ~{0_o}~3   ~{o_0}~3   ~{0_0}~3   ~{0_o}~3   \n",
            "\n",
            "~{o_0}~3   ~{0_0}~4   ~{0_o}~4   ~{o_0}~4   ~{o_0}~4   ~{o_0}~4   ~{0_o}~4   ~{o_0}~4   \n",
            "\n",
            "~{o_0}~4   ~{o_0}~4   ~{0_o}~4   ~{0_0}~4   ~{o_0}~4   ~{0_o}~4     {-_-}    \n",
            "\n",
            " `{x_X}~   \n",
            "\n",
            "Episode: 9/1500, Episode Score: 4.0, Avg Episode Score: 1.667, Survival Time: 1189\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 7002, Memory Size: 17193, Current Epsilon Value: 0.8\n",
            "\n",
            "\n",
            "~{0_0}~0   ~{0_o}~0   ~{0_o}~0   ~{o_o}~0   ~{o_0}~0   ~{0_0}~0   ~{o_o}~0   ~{o_0}~0   \n",
            "\n",
            "~{o_0}~0   ~{o_0}~0   ~{o_o}~0   ~{o_o}~0   ~{o_o}~1   ~{0_0}~1   ~{0_o}~1   ~{o_o}~1   \n",
            "\n",
            "~{0_o}~1   ~{o_o}~1   ~{o_o}~1   ~{o_0}~1   ~{o_0}~1   ~{0_0}~1   ~{o_0}~1   ~{0_0}~1   \n",
            "\n",
            "~{0_o}~2   ~{0_o}~2   ~{o_0}~2   ~{0_o}~2    `{x_X}~   \n",
            "\n",
            "Episode: 10/1500, Episode Score: 2.0, Avg Episode Score: 1.7, Survival Time: 884\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 7886, Memory Size: 18077, Current Epsilon Value: 0.78\n",
            "\n",
            "\n",
            "~{o_0}~0   ~{o_o}~0   ~{0_0}~0     {-_-}    ~{0_o}~0   ~{o_0}~0   ~{0_o}~0   ~{o_o}~0   \n",
            "\n",
            "~{0_o}~0   ~{o_0}~1   ~{o_0}~1   ~{o_o}~1   ~{0_0}~1   ~{0_0}~1   ~{o_o}~1   ~{0_0}~1   \n",
            "\n",
            "~{0_o}~1   ~{0_0}~1   ~{0_0}~1   ~{o_o}~1   ~{o_0}~1   ~{o_0}~1   ~{0_0}~1   ~{0_0}~2   \n",
            "\n",
            "~{0_0}~2    `{x_X}~   \n",
            "\n",
            "Episode: 11/1500, Episode Score: 2.0, Avg Episode Score: 1.727, Survival Time: 786\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 8672, Memory Size: 18863, Current Epsilon Value: 0.76\n",
            "\n",
            "~{o_0}~2   \n",
            "~{0_o}~0   ~{0_0}~0   ~{o_0}~0   ~{o_0}~0   ~{0_o}~0   ~{o_0}~0   ~{0_o}~0   \n",
            "\n",
            "~{o_o}~0   ~{0_0}~0   ~{0_o}~0     {-_-}    ~{o_o}~0   ~{0_0}~0   ~{0_o}~0   ~{0_o}~0   \n",
            "\n",
            "~{o_0}~0    `{x_X}~   \n",
            "\n",
            "Episode: 12/1500, Episode Score: 0, Avg Episode Score: 1.583, Survival Time: 492\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 9164, Memory Size: 19355, Current Epsilon Value: 0.75\n",
            "\n",
            "\n",
            "~{0_o}~0   ~{o_0}~0   ~{o_o}~0   ~{0_0}~0   ~{o_0}~0   ~{0_o}~0   ~{0_0}~0   ~{o_0}~0   \n",
            "\n",
            "~{0_o}~0   ~{0_o}~0   ~{o_o}~0   ~{o_o}~0   ~{0_o}~0   ~{o_0}~0   ~{0_o}~1   ~{o_o}~1   \n",
            "\n",
            "~{o_0}~1   ~{o_o}~1   ~{o_o}~1   ~{o_o}~2   ~{o_0}~2   ~{o_o}~2   ~{o_0}~2   ~{o_o}~2   \n",
            "\n",
            " `{x_X}~   \n",
            "\n",
            "Episode: 13/1500, Episode Score: 2.0, Avg Episode Score: 1.615, Survival Time: 787\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 9951, Memory Size: 20142, Current Epsilon Value: 0.73\n",
            "\n",
            "\n",
            "~{0_o}~0   ~{0_0}~0     {-_-}    ~{0_0}~0   ~{o_0}~0   ~{0_o}~0   ~{0_o}~0   ~{o_0}~0   \n",
            "\n",
            "~{o_0}~0   ~{0_o}~0   ~{o_0}~0   ~{o_o}~0   ~{o_o}~0   ~{o_0}~0   ~{o_0}~0   ~{o_o}~0   \n",
            "\n",
            "~{o_0}~0    `{x_X}~   \n",
            "\n",
            "Episode: 14/1500, Episode Score: 0, Avg Episode Score: 1.5, Survival Time: 502\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 10453, Memory Size: 20644, Current Epsilon Value: 0.72\n",
            "\n",
            "\n",
            "~{o_0}~0   ~{0_0}~0   ~{0_o}~0   ~{o_o}~0   ~{0_0}~0   ~{o_0}~0   ~{0_o}~0   ~{o_o}~0   \n",
            "\n",
            "~{o_0}~0   ~{o_o}~0   ~{0_0}~0   ~{o_0}~0   ~{0_o}~0   ~{o_o}~0   ~{0_o}~0   ~{o_0}~0   \n",
            "\n",
            " `{x_X}~   \n",
            "\n",
            "Episode: 15/1500, Episode Score: 0, Avg Episode Score: 1.4, Survival Time: 493\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 10946, Memory Size: 21137, Current Epsilon Value: 0.71\n",
            "\n",
            "\n",
            "~{o_o}~0     {-_-}    ~{o_0}~0   ~{o_0}~0   ~{0_o}~0   ~{o_0}~0   ~{o_o}~0   ~{o_o}~0   \n",
            "\n",
            "~{o_0}~1   ~{o_0}~1   ~{o_0}~1   ~{o_o}~1   ~{0_o}~1   ~{o_0}~1   ~{o_0}~1   ~{0_0}~1   \n",
            "\n",
            "~{0_o}~1   ~{o_o}~1   ~{o_o}~1   ~{0_o}~1    `{x_X}~   \n",
            "\n",
            "Episode: 16/1500, Episode Score: 1.0, Avg Episode Score: 1.375, Survival Time: 615\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 11561, Memory Size: 21752, Current Epsilon Value: 0.69\n",
            "\n",
            "\n",
            "~{0_o}~0   ~{0_o}~0   ~{o_o}~0   ~{o_o}~0   ~{0_o}~0   ~{0_o}~0   ~{o_o}~0   ~{0_o}~0   \n",
            "\n",
            "~{0_o}~0   ~{o_o}~0   ~{0_o}~0   ~{0_o}~0   ~{0_0}~0     {-_-}    ~{0_0}~0   ~{0_o}~0   \n",
            "\n",
            "~{o_o}~0    `{x_X}~   \n",
            "\n",
            "Episode: 17/1500, Episode Score: 0, Avg Episode Score: 1.294, Survival Time: 522\n",
            "\n",
            "\\{^,^}~1\n",
            "\n",
            "Total Steps: 12083, Memory Size: 22274, Current Epsilon Value: 0.68\n",
            "\n",
            "\n",
            "~{o_0}~0   ~{o_0}~0   "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1EFy_NoE7Wl"
      },
      "source": [
        "#### Selector Menu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-9mrlfuSA23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5857ecdb-f3d5-4200-853f-1510757655ba"
      },
      "source": [
        "# Render gameplay in cell or viewer\n",
        "RENDER = False #@param {type:\"boolean\"}\n",
        "# Slow down game play for observation\n",
        "observation_mode = False #@param {type:\"boolean\"}\n",
        "# Provide GPU information\n",
        "Use_GPU_Support = True #@param {type:\"boolean\"}\n",
        "\n",
        "# Import and Unzip Hosted Dataset\n",
        "Import_Full_Data = False #@param {type:\"boolean\"}\n",
        "Import_Model_Data = True #@param {type:\"boolean\"}\n",
        "Create_Missing_Directories = False #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "##   Game Selector\n",
        "envName = 'BreakoutNoFrameskip-v4' #@param ['Atlantis-v0', 'DemonAttack-v0', 'Phoenix-v0', 'Riverraid-v0', 'Solaris-v0', 'Asterix-v0', 'Breakout-v0', 'Boxing-v0', 'Pong-v0', 'BattleZone-v0', 'SpaceInvaders-v0', 'BeamRider-v0','AtlantisNoFrameskip-v4', 'DemonAttackNoFrameskip-v4', 'PhoenixNoFrameskip-v4', 'RiverraidNoFrameskip-v4', 'SolarisNoFrameskip-v4', 'AsterixNoFrameskip-v4', 'BreakoutNoFrameskip-v4', 'BoxingNoFrameskip-v4', 'PongNoFrameskip-v4', 'BattleZoneNoFrameskip-v4', 'SpaceInvadersNoFrameskip-v4', 'BeamRiderNoFrameskip-v4']\n",
        "\n",
        "\n",
        "# Parameters\n",
        "n_episodes = 1500 #@param {type:\"integer\"}\n",
        "batch_size =  1024#@param {type:\"integer\"}\n",
        "action_steps =  32#@param {type:\"integer\"}\n",
        "skip_start = 1 #@param {type:\"integer\"}\n",
        "agent_gamma = 0.995 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "agent_epsilon = 1.0 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "agent_epsilon_decay = 0.999 #@param {type:\"slider\", min:0.5, max:1.000, step:0.0001}\n",
        "agent_epsilon_min = 0.1 #@param {type:\"slider\", min:0, max:1.000, step:0.0001}\n",
        "agent_learning_rate = 0.00042 #@param {type:\"number\", min:0.0000, max:0.0200, step:0.00001}\n",
        "# lr=0.00042 # lr=0.00125 #lr=0.00025\n",
        "memory_length = 500000 #@param {type:\"integer\"}\n",
        "starting_memory_size =  10000#@param {type:\"integer\"}\n",
        "u_freQuency = 1000 #@param {type:\"integer\"}\n",
        "use_priority_memory = False #@param {type:\"boolean\"}\n",
        "priority_memory_length =  2#@param {type:\"integer\"}\n",
        "\n",
        "# Sub-Model Setting\n",
        "slice_one_name = \"ISAR_Final_Slice_One\" #@param {type:\"string\"}\n",
        "sliceOneTrainable = True #@param {type:\"boolean\"}\n",
        "sliceOneCheckpoint = False #@param {type:\"boolean\"}\n",
        "\n",
        "slice_two_name = \"ISAR_Final_Slice_Two\" #@param {type:\"string\"}\n",
        "sliceTwoTrainable = True #@param {type:\"boolean\"}\n",
        "sliceTwoCheckpoint = False #@param {type:\"boolean\"}\n",
        "\n",
        "model_name = \"fullModel\" #@param {type:\"string\"}\n",
        "fullModelCheckpoint = False #@param {type:\"boolean\"}\n",
        "checkpointFrequency =  10#@param {type:\"integer\"}\n",
        "saveAgent = False #@param {type:\"boolean\"}\n",
        "\n",
        "Report_All_Updates = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Row for progress bot stacking ( 1, 8, 12 )\n",
        "rowCountThres = 8 #@param {type:\"integer\"}\n",
        "\n",
        "## Game selector feedback\n",
        "print(\"Selected Game: \" + envName)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected Game: BreakoutNoFrameskip-v4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGIl3BKliih1"
      },
      "source": [
        "#### Setup Model Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFzeNuWrURPP"
      },
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  model_dir = '/content/ISAR_Model_Data'\n",
        "  data_dir = '/content/ISAR_Main_Classification'\n",
        "  COLAB = True\n",
        "  \n",
        "except:\n",
        "  !pip install gdown\n",
        "  model_dir = 'ISAR_Model_Data'\n",
        "  data_dir = 'ISAR_Main_Classification'\n",
        "  COLAB = False\n",
        "\n",
        "if Import_Full_Data:\n",
        "    if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1qgaAMG_NUxQo0Mfg9vM0qQz0ruQcRMtD\n",
        "      !unzip ISAR_MainClassification.zip\n",
        "      !rm ISAR_MainClassification.zip\n",
        "if Import_Model_Data:\n",
        "    if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1DhTXHoMbQ-ycDXKp8SEwxlal6Q8xfHgj\n",
        "      !unzip ISAR_Model_Data.zip\n",
        "      !rm ISAR_Model_Data.zip"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poXz23gKsczB"
      },
      "source": [
        "if Create_Missing_Directories:  \n",
        "  if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Model Directory\")\n",
        "    os.makedirs(model_dir)\n",
        "  else:\n",
        "    print(\"Model Directory Found\")\n",
        "  if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Data Directory\")\n",
        "    os.makedirs(data_dir)\n",
        "  else:\n",
        "    print(\"Data Directory Found\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNp06xZqiWKD"
      },
      "source": [
        "#### Setup Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjxfLJqifsa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de592fa5-e9f3-4721-c3ef-e7052ac96843"
      },
      "source": [
        "## Begin by importing . . .  oh . . . everything!\n",
        "try:\n",
        "  import math\n",
        "  import random\n",
        "  import numpy as np\n",
        "\n",
        "  import glob\n",
        "  import io\n",
        "  import base64\n",
        "  from time import sleep\n",
        "\n",
        "  from collections import deque\n",
        "\n",
        "  import gym\n",
        "  import tensorflow as tf\n",
        "  import tensorflow_hub as hub\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "  from tensorflow.keras.optimizers import Nadam, Adam\n",
        "  from tensorflow import keras\n",
        "\n",
        "  import matplotlib\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "\n",
        "  from IPython.display import HTML\n",
        "  from IPython import display as ipythondisplay\n",
        "  from IPython.display import clear_output\n",
        "\n",
        "except:\n",
        "  %%capture\n",
        "  if COLAB:\n",
        "    ## For colab we must install some dependancies\n",
        "    !apt-get install -y xvfb x11-utils\n",
        "    ## Next we will need to install a virtual display and correct Open AI installation\n",
        "    !pip install gym[all]==0.17.3\n",
        "    !pip install pyvirtualdisplay==0.2.* \n",
        "    !pip install PyOpenGL==3.1.* \n",
        "    !pip install PyOpenGL-accelerate==3.1.*\n",
        "    !pip install pyglet\n",
        "    # So let's setup the virtual display\n",
        "    import pyvirtualdisplay\n",
        "    # use False with Xvfb\n",
        "    _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "    _ = _display.start()\n",
        "    # Now Check the Display\n",
        "    !echo $DISPLAY\n",
        "\n",
        "if Use_GPU_Support:\n",
        "  print(\"TF version:\", tf.__version__)\n",
        "  print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "  # Set Memory Growth\n",
        "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "  if gpus:\n",
        "    try:\n",
        "      # Currently, memory growth needs to be the same across GPUs\n",
        "      for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(gpu)\n",
        "      logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "      print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "      # Memory growth must be set before GPUs have been initialized\n",
        "      print(e)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version: 2.4.1\n",
            "Num GPUs Available:  2\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n",
            "2 Physical GPUs, 2 Logical GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdX2lI15iCvS"
      },
      "source": [
        "#### Setup Frame PreProcessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT-35yCogEXo"
      },
      "source": [
        "### Custom Pre-Processor\n",
        "# Stack frames and average on axis=-1 to produce a Single 210, 160, 1 greyscale image #old crop = img = frame[1:176:2, ::2]\n",
        "# Rescale to 105, 80, 1 then stack in three's (3) to produce a single RGB compatible representation of evironmental space-time\n",
        "flicker = deque(maxlen=2)\n",
        "temporal_stack = deque(maxlen=3)\n",
        "\n",
        "def process_frameX(frame):\n",
        "  while len(temporal_stack) < 3:\n",
        "    temporal_stack.append(frame[::2, ::2].mean(axis=2)) \n",
        "  temporal_image = np.expand_dims(np.concatenate(np.expand_dims((temporal_stack[0],\n",
        "                                                                  temporal_stack[1],\n",
        "                                                                  temporal_stack[2]), \n",
        "                                                                axis=-1), axis=-1), axis=0)/255\n",
        "  return temporal_image\n",
        "\n",
        "\n",
        "def process_frame_no_stack(frame):\n",
        "  return np.expand_dims((frame[::2, ::2]), axis=0)/255\n",
        "\n",
        "\n",
        "# def process_no_frameskip(frame): \n",
        "# def process_frame_stack(frame):\n",
        "def process_frame(frame):\n",
        "  while len(temporal_stack) < 3:\n",
        "    temporal_stack.append(frame[::2, ::2].mean(axis=2,keepdims=True)) \n",
        "  temporal_image = np.expand_dims(np.concatenate((temporal_stack[0],\n",
        "                                                  temporal_stack[1],\n",
        "                                                  temporal_stack[2]), \n",
        "                                                axis=-1), axis=0)/255\n",
        "  return temporal_image\n",
        "\n",
        "\n",
        "def process_frame_flicker_stack(frame):\n",
        "  while len(flicker) < 2:\n",
        "    flicker.append(frame[::2, ::2])\n",
        "  while len(temporal_stack) < 3:\n",
        "    temporal_stack.append(np.concatenate((flicker[0].mean(axis=2,keepdims=True),flicker[1].mean(axis=2,keepdims=True)), axis=-1).max(axis=2)) \n",
        "  temporal_image = np.expand_dims(np.concatenate(np.expand_dims((temporal_stack[0],\n",
        "                                                                  temporal_stack[1],\n",
        "                                                                  temporal_stack[2]), \n",
        "                                                                axis=-1), axis=-1), axis=0)/255\n",
        "  return temporal_image"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vtDg55hojmL"
      },
      "source": [
        "#### Setup Progress Bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyqPA-frPMS1"
      },
      "source": [
        "rowCount = 0\n",
        "\n",
        "def progressBot(progType, score=0, all=0, episode=0, n=0, survived=0, experiance=0):\n",
        "  global rowCount\n",
        "  if progType == 'training':\n",
        "      if np.random.rand() >= 0.69: # take random action\n",
        "          print(\"\"\"~{o_0}~\"\"\" + str(int(score))+\"   \", end='')\n",
        "      else:\n",
        "        if np.random.rand() <= 0.27: # take random action\n",
        "          print(\"\"\"~{0_0}~\"\"\" + str(int(score))+\"   \", end='')\n",
        "        else:\n",
        "          if np.random.rand() <= 0.42: # take random action\n",
        "            print(\"\"\"~{o_o}~\"\"\" + str(int(score))+\"   \", end='')\n",
        "          else:\n",
        "            print(\"\"\"~{0_o}~\"\"\" + str(int(score))+\"   \", end='')\n",
        "\n",
        "  if progType == 'q_update':\n",
        "      print(\"\"\"  {-_-}    \"\"\", end='')\n",
        "  \n",
        "  rowCount += 1\n",
        "  if rowCount >= rowCountThres:\n",
        "      print(\"\\n\")\n",
        "      rowCount = 0\n",
        "\n",
        "  if progType == 'dead':\n",
        "      print(\"\"\" `{x_X}~   \"\"\"+\"\\n\")\n",
        "      rowCount = 0\n",
        "      print(\"Episode: {}/{}, Episode Score: {}, Avg Episode Score: {:.4}, Survival Time: {}\"\n",
        "            .format(episode+1, n, score, all/(episode+1), survived)+\"\\n\")\n",
        "      print(\"\"\"\\{^,^}~\"\"\" + str(int(all/(episode+1)))+\"\\n\")\n",
        "      print(\"Total Steps: {}, Memory Size: {}, Current Epsilon Value: {:.2}\"\n",
        "            .format(experiance, len(agent.memory), agent.epsilon)+\"\\n\")\n",
        "      "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F5PijpNi9Wg"
      },
      "source": [
        "#### Define the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Nw-i27h9nR"
      },
      "source": [
        "class DQNAgent:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=memory_length)\n",
        "        self.priority_memory = deque(maxlen=priority_memory_length)\n",
        "        self.update_rate = u_freQuency # Number of steps until updating the target network\n",
        "\n",
        "        self.gamma = agent_gamma # decay or discount rate\n",
        "\n",
        "        self.epsilon = agent_epsilon # exploration rate\n",
        "        self.epsilon_decay = agent_epsilon_decay # exploration decay\n",
        "        self.epsilon_min = agent_epsilon_min # min exploration\n",
        "\n",
        "        self.learning_rate = agent_learning_rate # SGD or Nadam rate param\n",
        "        \n",
        "        self.model, self.sliceTwo = self._build_model() \n",
        "        self.target_model, self.target_sliceTwo = self._build_model()\n",
        "        self.target_model.set_weights(self.model.get_weights()) # create Q-target network\n",
        "        \n",
        "        self.model.summary()\n",
        "\n",
        "\n",
        "    def _build_model(self): # private method\n",
        "        # Neural Network Based on DeepMind Paper to Approximate State/Action Q-Values:\n",
        "        sliceOneInputs = keras.Input(shape=(105, 80, 3))\n",
        "        sliceOne = tf.keras.models.load_model(model_dir + '/' + slice_one_name)      \n",
        "        sliceOneOutputs = sliceOne(sliceOneInputs, training=False)\n",
        "\n",
        "        sliceTwoConv_1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),strides=1, activation='relu', \\\n",
        "                            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceOneOutputs)\n",
        "        sliceTwoFlatten_1 = tf.keras.layers.Flatten()(sliceTwoConv_1)\n",
        "\n",
        "        sliceTwoDense_1 = tf.keras.layers.Dense(512, activation='relu', \\\n",
        "                            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceTwoFlatten_1)\n",
        "\n",
        "        ## Linear\n",
        "        finalOutput = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                            kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceTwoDense_1)\n",
        "\n",
        "        # ## Softmax   #)(sliceTwoDense_1)\n",
        "        # finalOutput = Dense(self.action_size, activation='softmax', name='finalOutput', \\\n",
        "        #                     kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceTwoDense_1)\n",
        "        \n",
        "        # ## Sigmoid\n",
        "        # finalOutput = tf.keras.layers.Dense(self.action_size, activation='sigmoid', name='finalOutput')(sliceTwoDense_1)\n",
        "\n",
        "        # ## Relu\n",
        "        # finalOutput = tf.keras.layers.Dense(self.action_size, activation='relu', name='finalOutput')(sliceTwoDense_1)\n",
        "\n",
        "        # ## Leaky Relu\n",
        "        # interimOutput = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "        #                     kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceTwoDense_1)\n",
        "        # finalOutput = tf.keras.layers.LeakyReLU(alpha=0.1)(interimOutput)\n",
        "        \n",
        "        sliceTwo = tf.keras.Model(inputs=sliceOneInputs, outputs=sliceTwoDense_1)\n",
        "        model = tf.keras.Model(inputs=sliceOneInputs, outputs=finalOutput)\n",
        "\n",
        "        sliceOne.trainable = sliceOneTrainable # ref param above\n",
        "        sliceTwo.trainable = sliceTwoTrainable # ref param above\n",
        "\n",
        "        # model.compile(optimizer=tf.keras.optimizers.SGD(lr=self.learning_rate),loss='categorical_crossentropy')\n",
        "        # model.compile(optimizer=tf.keras.optimizers.Adam(lr=self.learning_rate),loss='categorical_crossentropy')\n",
        "        # model.compile(optimizer=tf.keras.optimizers.Nadam(lr=self.learning_rate),loss='categorical_crossentropy')\n",
        "\n",
        "        # ,clipvalue=1.0), \\\n",
        "        model.compile(optimizer=Nadam(lr=self.learning_rate, clipnorm=1.0), \n",
        "                      loss=tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM))\n",
        "        \n",
        "        # Long Form Huber\n",
        "        # huber_loss = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        # model.compile(optimizer=tf.keras.optimizers.Nadam(lr=self.learning_rate),loss=huber_loss) # 'Huber')\n",
        "        # model.compile(optimizer=tf.keras.optimizers.Nadam(lr=self.learning_rate),loss=tf.keras.losses.Huber())\n",
        "\n",
        "        # model.compile(optimizer=SGD(lr=self.learning_rate),loss='mse')\n",
        "        # model.compile(optimizer=Adam(lr=self.learning_rate),loss='mse')\n",
        "        # model.compile(optimizer=Nadam(lr=self.learning_rate),loss='mse')\n",
        "        return model, sliceTwo\n",
        "\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "\n",
        "    def rememberPriority(self, state, action, reward, next_state, done):\n",
        "        if reward < 0:\n",
        "          self.memory.append((state, action, reward, next_state, done))\n",
        "        if reward > 0:\n",
        "          self.memory.append((state, action, reward, next_state, done))\n",
        "        else:\n",
        "          if np.random.rand() >= (self.epsilon - self.epsilon_min): \n",
        "            self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "\n",
        "    def train(self, batch_size): # method that trains agent model\n",
        "        x_minibatch = np.zeros((batch_size, 105, 80, 3))\n",
        "        y_minibatch = np.zeros((batch_size, self.action_size))\n",
        "\n",
        "        ## Alternate implementation\n",
        "        # idx = 0\n",
        "        # batch = random.sample(self.memory, batch_size) # sample from memory\n",
        "        # for state, action, reward, next_state, done in batch: # extract data\n",
        "\n",
        "        for idx in range(0, batch_size):\n",
        "            sample = random.sample(self.memory, 1)\n",
        "            state, action, reward, next_state, done = sample[0]\n",
        "        \n",
        "            ## This is where all the magic happens\n",
        "            qValueUpdate = reward\n",
        "            qValueF = self.target_model.predict(next_state) # approximate future reward\n",
        "            if not done: # if not done, then discount reward\n",
        "                qValueUpdate = (reward + self.gamma * np.amax(qValueF))\n",
        "\n",
        "            # Make an action Mask to minimize grad calculation\n",
        "            one_hot_actions = np.expand_dims(tf.one_hot(action, self.action_size, on_value=qValueUpdate), axis=0)\n",
        "\n",
        "            # Batch up our data\n",
        "            x_minibatch[idx, ...] = state\n",
        "            y_minibatch[idx, ...] = one_hot_actions\n",
        "            idx = idx + 1\n",
        "            \n",
        "        self.model.fit(x=x_minibatch, y=y_minibatch, epochs=1, verbose=0, batch_size=batch_size)\n",
        "\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon: \n",
        "            return random.randrange(self.action_size) # Do something stupid! (i.e. take a random action)\n",
        "        act_values = self.model.predict(state) # Get action values based on current state\n",
        "        # return np.random.choice(np.arange(self.action_size), p=act_values) # I HOPE this will implement Thompson sampling\n",
        "        return np.argmax(act_values[0]) # This will implement epsilon greedy sampling\n",
        "\n",
        "\n",
        "    # Copies current model parameters to the target model\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw2g6PjZPS9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8a94d85-5bbe-446d-d661-95f2b9f36926"
      },
      "source": [
        "# Make Environment\n",
        "env = gym.make(envName)\n",
        "state_size = (105, 80, 3)\n",
        "action_size = env.action_space.n\n",
        "\n",
        "## Intialize Our Agent\n",
        "agent = DQNAgent(state_size, action_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 105, 80, 3)]      0         \n",
            "_________________________________________________________________\n",
            "model (Functional)           (None, 11, 8, 64)         39008     \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 9, 6, 64)          36928     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3456)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               1769984   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 1,847,972\n",
            "Trainable params: 1,847,972\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z5loBrJiHON"
      },
      "source": [
        "#### Define Training Loop Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8cbu_VJh-E4"
      },
      "source": [
        "def myPlayground():\n",
        "\n",
        "  # Empty Rewards Counter\n",
        "  reward = 0\n",
        "  all_rewards = 0\n",
        "  total_time = 0\n",
        "\n",
        "  # Display in Colab\n",
        "  if COLAB:\n",
        "      if RENDER == True:\n",
        "          done, ax = plt.subplots(1, 1)\n",
        "          img = ax.imshow(env.render('rgb_array'))\n",
        "  \n",
        "\n",
        "  # Make Some Starting data\n",
        "  state = process_frame(env.reset()) # Reset state for new episode\n",
        "  while len(agent.memory) < starting_memory_size:\n",
        "    action = random.randrange(agent.action_size)\n",
        "    next_frame, reward, done, _ = env.step(action) # Agent sends action to env wrapper and gets feedback\n",
        "    next_state = process_frame(next_frame)\n",
        "    if reward > 0.0: # Check and modify reward\n",
        "      reward = 1.0\n",
        "    if reward < 0.0:\n",
        "      reward = -1.0\n",
        "    if done:\n",
        "      reward = -1.0\n",
        "    agent.remember(state, action, reward, next_state, done) # Store sequence in replay memory\n",
        "    state = next_state # Update state\n",
        "\n",
        "\n",
        "  # Training Loop\n",
        "  for e in range(n_episodes): ## Go Eat Cherries\n",
        "      \n",
        "      done = False\n",
        "      time = 0\n",
        "      game_score = 0\n",
        "\n",
        "      state = process_frame(env.reset()) # Reset state for new episode\n",
        "      reward = 0\n",
        "      \n",
        "      for skip in range(skip_start): # Skip the start of each game\n",
        "          env.step(0)\n",
        "\n",
        "      # for timestep in range(1, max_steps_per_episode):\n",
        "      # for timestep in range(1, 10000):\n",
        "      while not done:    \n",
        "          time += 1\n",
        "          total_time += 1\n",
        "\n",
        "          # Display\n",
        "          if RENDER:\n",
        "              if COLAB:\n",
        "                  img.set_data(env.render(mode='rgb_array')) \n",
        "                  ax.axis('off')\n",
        "                  ipythondisplay.display(plt.gcf())\n",
        "                  ipythondisplay.clear_output(wait=True)\n",
        "              else:\n",
        "                  env.render()\n",
        "                  if observation_mode:\n",
        "                      sleep(0.02)\n",
        "\n",
        "          # Update target network\n",
        "          if total_time % agent.update_rate == 0:\n",
        "              agent.update_target_model()\n",
        "              progressBot('q_update')\n",
        "\n",
        "          # Transition Dynamics\n",
        "          action = agent.act(state) # Get action from agent\n",
        "          next_frame, reward, done, _ = env.step(action) # Agent sends action to env wrapper and gets feedback\n",
        "          next_state = process_frame(next_frame)\n",
        "\n",
        "          if reward > 0.0: # Check and modify reward\n",
        "            reward = 1.0\n",
        "            game_score += reward\n",
        "          if reward < 0.0:\n",
        "            reward = -1.0\n",
        "          if done:\n",
        "            reward = -1.0\n",
        "            all_rewards += game_score\n",
        "            progressBot('dead', score=game_score, all=all_rewards, episode=e, n=n_episodes,  survived=time, experiance=total_time)\n",
        "            # if use_priority_memory:\n",
        "            #     agent.rememberPriority(state, action, reward, next_state, done) # Store death in priority memory\n",
        "            # else:\n",
        "            #     agent.remember(state, action, reward, next_state, done) # Store death in replay memory\n",
        "            # break\n",
        "\n",
        "          if use_priority_memory:\n",
        "              agent.rememberPriority(state, action, reward, next_state, done) # Store sequence in priority memory\n",
        "          else:\n",
        "              agent.remember(state, action, reward, next_state, done) # Store sequence in replay memory\n",
        "          state = next_state # Update state\n",
        "\n",
        "          if len(agent.memory) > starting_memory_size:\n",
        "            if not observation_mode:\n",
        "              if total_time % action_steps == 0:\n",
        "                agent.train(batch_size)\n",
        "                progressBot(\"training\", game_score)\n",
        "\n",
        "      if e % checkpointFrequency == 0 and len(agent.memory) > starting_memory_size:\n",
        "        if sliceOneCheckpoint:\n",
        "            saved_model_path = model_dir + \"/ISAR_Production_Models/\" + envName + slice_one_name + '{:04d}'.format(e) \n",
        "            tf.saved_model.save(agent.sliceOne, saved_model_path)\n",
        "\n",
        "        if sliceTwoCheckpoint:\n",
        "            saved_model_path = model_dir + \"/ISAR_Production_Models/\" + envName + slice_two_name + '{:04d}'.format(e) \n",
        "            tf.saved_model.save(agent.sliceTwo, saved_model_path)\n",
        "\n",
        "        if fullModelCheckpoint:\n",
        "            saved_model_path = model_dir + \"/ISAR_Production_Models/\" + envName + model_name + '{:04d}'.format(e) \n",
        "            tf.saved_model.save(agent.model, saved_model_path)\n",
        "\n",
        "        if saveAgent:\n",
        "            agent.save(model_dir+ \"/ISAR_Production_Models/\" + envName + model_name + \"agent_weights_\" + '{:04d}'.format(e) + envName +\".hdf5\")\n",
        "      print(\"\")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgI4QKIOo6Nl"
      },
      "source": [
        "#### Interactive Playground (runs automatically)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i9UWy2w5nUs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "775cfd4d-f90c-43ea-8832-a877579828ce"
      },
      "source": [
        "## Run From Bottom (Can be run with one click)\n",
        "myPlayground()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "~{0_0}~0   ~{o_0}~0   ~{o_0}~0   "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function WeakKeyDictionary.__init__.<locals>.remove at 0x7fe905b70b80>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.8/weakref.py\", line 345, in remove\n",
            "    def remove(k, selfref=ref(self)):\n",
            "KeyboardInterrupt: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "~{0_o}~0   ~{0_o}~0   "
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-f09dad3c3173>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Run From Bottom (Can be run with one click)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmyPlayground\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-cca9b8018314>\u001b[0m in \u001b[0;36mmyPlayground\u001b[0;34m()\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobservation_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maction_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0mprogressBot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-e150ba087760>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m## This is where all the magic happens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mqValueUpdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mqValueF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# approximate future reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if not done, then discount reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0mqValueUpdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqValueF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1623\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1625\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1626\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
            "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
            "\u001b[0;32m/usr/lib/python3/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2969\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2971\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2972\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   2973\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwV5g8mVbT0Y"
      },
      "source": [
        "# agent.memory = deque(maxlen=memory_length)\n",
        "# agent.epsilon = 0.999"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK1d7bgWMLq4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}