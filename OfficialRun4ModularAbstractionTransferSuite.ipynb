{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "OfficialRun4ModularAbstractionTransferSuite.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ynM5BeDfWQvH",
        "BGIl3BKliih1",
        "HNp06xZqiWKD",
        "sYyAtezpCwYg",
        "0vtDg55hojmL",
        "uR4qY7HhnhNq",
        "1F5PijpNi9Wg",
        "8z5loBrJiHON",
        "4ZKKOtCtCcA7"
      ],
      "authorship_tag": "ABX9TyOK0KzI/EOR64bqLRlTnxcK",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DocLock17/ISAR_Final/blob/main/OfficialRun4ModularAbstractionTransferSuite.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXiHbkQgfw9a"
      },
      "source": [
        "# Modular Abstraction Transfer Suite\n",
        "\n",
        "### A Deep Mind Based Experimental Platform For Reasearch In Abstaction And Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIsSBMAUbeZ2"
      },
      "source": [
        "We set out to assemble a team of Algorithms to fight the evil of an unseen game. Unfortunately, this task became nearly insurmountable. First iterations of the code were based implementations using a deque for replay memory, and using a single instance training loop. These systems did show promise in early experiments but were so inefficient that it would have likely taken months of training to reach a respectable level let alone expert level game play.\n",
        "\n",
        "The solution it seems is multi-part, so we will spread out the explanations here.\n",
        "\n",
        "|\n",
        "Great References:\n",
        "\n",
        "https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756\n",
        "\n",
        "https://keras.io/examples/rl/deep_q_network_breakout/\n",
        "\n",
        "https://stackoverflow.com/questions/15455048/releasing-memory-in-python\n",
        "\n",
        "Official Citations in accompanying paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynM5BeDfWQvH"
      },
      "source": [
        "#### Interactive Playgound (run after notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrFuoAxKQclk"
      },
      "source": [
        "# # # Run From Top (Hint: It must be loaded commented first)\n",
        "# myPlayground()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1EFy_NoE7Wl"
      },
      "source": [
        "#### Selector Menu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-9mrlfuSA23",
        "outputId": "c3f0bea7-90e2-474b-d14b-1fff5e631105"
      },
      "source": [
        "##   Game Selector\n",
        "ENVIRONMENT_NAME = 'BreakoutNoFrameskip-v4' #@param ['Atlantis-v0', 'DemonAttack-v0', 'Phoenix-v0', 'Riverraid-v0', 'Solaris-v0', 'Asterix-v0', 'Breakout-v0', 'Boxing-v0', 'Pong-v0', 'BattleZone-v0', 'SpaceInvaders-v0', 'BeamRider-v0','AtlantisNoFrameskip-v4', 'DemonAttackNoFrameskip-v4', 'PhoenixNoFrameskip-v4', 'RiverraidNoFrameskip-v4', 'SolarisNoFrameskip-v4', 'AsterixNoFrameskip-v4', 'BreakoutNoFrameskip-v4', 'BoxingNoFrameskip-v4', 'PongNoFrameskip-v4', 'BattleZoneNoFrameskip-v4', 'SpaceInvadersNoFrameskip-v4', 'BeamRiderNoFrameskip-v4']\n",
        "\n",
        "# Render gameplay in cell or viewer\n",
        "RENDER = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Slow down game play and discontinue training for observation\n",
        "OBSERVATION_MODE = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Display Text Output to Observe Activations and Q Updates\n",
        "DIAGNOSTIC_MODE = False #@param {type:\"boolean\"}\n",
        "\n",
        "## Chippie the Progress Bot Settings       (Name courtesy of: Kylie Locker)\n",
        "CHIPPIE_PROGESS_REPORTS = True #@param {type:\"boolean\"}\n",
        "# Row for progress bot stacking ( 1, 8, 12 )\n",
        "MAX_CHIPPIES_ROW = 8 #@param {type:\"integer\"}\n",
        "\n",
        "# Provide GPU information\n",
        "USE_GPU_SUPPORT = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "## Data Settings\n",
        "# Import and Unzip Hosted Dataset\n",
        "CREATE_MISSING_DIRECTORIES = True #@param {type:\"boolean\"}\n",
        "IMPORT_MAIN_DATA = True #@param {type:\"boolean\"}\n",
        "IMPORT_SENTIMENT_DATA = True #@param {type:\"boolean\"}\n",
        "IMPORT_MODEL_DATA = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "## Agent Settings\n",
        "# Parameters\n",
        "n_episodes = 50000 #@param {type:\"integer\"}\n",
        "BATCH_SIZE =  32#@param {type:\"integer\"}\n",
        "action_steps =  4#@param {type:\"integer\"}\n",
        "skip_start = 1 #@param {type:\"integer\"}\n",
        "agent_gamma = 0.985 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "agent_epsilon = 1.0 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "agent_epsilon_decay = 0.9999 #@param {type:\"slider\", min:0.5, max:1.000, step:0.0001}\n",
        "agent_epsilon_min = 0.1 #@param {type:\"slider\", min:0, max:1.000, step:0.0001}\n",
        "agent_learning_rate = 0.00025 #@param {type:\"number\", min:0.0000, max:0.0200, step:0.00001}\n",
        "# lr=0.00042 # lr=0.00125 #lr=0.00025\n",
        "# agent_learning_rate = 0.00025 # works??\n",
        "## Buffer Settings\n",
        "STARTING_MEMORY_SIZE = 75000#@param {type:\"integer\"}\n",
        "MAX_MEMORY_LENGTH = 100000 #@param {type:\"integer\"}\n",
        "Q_UPDATE_FREQUENCY = 10000 #@param {type:\"integer\"}\n",
        "\n",
        "# Pre Buffer Guidance\n",
        "WINDOW_SIZE = (84, 84)\n",
        "\n",
        "# Sub-Model Settings\n",
        "SLICE_ONE_NAME = \"s1_MAT_ImageClassifier_v5\" #@param {type:\"string\"}\n",
        "SLICE_ONE_TRAINABLE = False #@param {type:\"boolean\"}\n",
        "SLICE_ONE_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "SLICE_TWO_NAME = \"s2_MAT_ImageClassifier_v5\" #@param {type:\"string\"}\n",
        "SLICE_TWO_TRAINABLE = False #@param {type:\"boolean\"}\n",
        "SLICE_TWO_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "SLICE_THREE_NAME = \"s3_MAT_ImageClassifier_v2\" #@param {type:\"string\"}\n",
        "SLICE_THREE_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "SLICE_THREE_CHECKPOINT = True #@param {type:\"boolean\"}\n",
        "\n",
        "FULL_MODEL_NAME = \"s2_MAT_Agent_Testing_v2\" #@param {type:\"string\"}\n",
        "FULL_MODEL_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "FULL_MODEL_CHECKPOINT = True #@param {type:\"boolean\"}\n",
        "\n",
        "CHECKPOINT_FREQUENCY =  1000#@param {type:\"integer\"}\n",
        "\n",
        "SAVE_AGENT = True #@param {type:\"boolean\"}\n",
        "\n",
        "## Game selector feedback\n",
        "print(\"Selected Game: \" + ENVIRONMENT_NAME)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected Game: BreakoutNoFrameskip-v4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGIl3BKliih1"
      },
      "source": [
        "#### Setup Model Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFzeNuWrURPP"
      },
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install baselines\n",
        "  COLAB = True\n",
        "  \n",
        "except:\n",
        "  !pip install gdown\n",
        "  COLAB = False\n",
        "\n",
        "model_dir = 'ISAR_Model_Data'\n",
        "data_dir = 'ISAR_Main_Classification'\n",
        "transfer_dir = 'ISAR_Sentiment_Transfer'\n",
        "\n",
        "\n",
        "if IMPORT_MAIN_DATA :\n",
        "    if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1P7o1x4ZpPbd16VQDwaMzllbN-tlfqqIH\n",
        "      !unzip ISAR_Main_Classification.zip\n",
        "      !rm ISAR_Main_Classification.zip\n",
        "if IMPORT_SENTIMENT_DATA:\n",
        "    if not os.path.exists(transfer_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1UDUNnw04q5cvms5ibM6pNn-wXtJphXxZ\n",
        "      !unzip ISAR_Sentiment_Transfer.zip\n",
        "      !rm ISAR_Sentiment_Transfer.zip\n",
        "if IMPORT_MODEL_DATA:\n",
        "    if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1DIc_J6XyKzNDSMjSbGkt3vzQep4YUEYU\n",
        "      !unzip ISAR_Model_Data.zip\n",
        "      !rm ISAR_Model_Data.zip\n",
        "      "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poXz23gKsczB",
        "outputId": "05819947-626e-494a-dd24-9278a9293366"
      },
      "source": [
        "if CREATE_MISSING_DIRECTORIES:  \n",
        "  if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Main Data Directory\")\n",
        "    os.makedirs(data_dir)\n",
        "  else:\n",
        "    print(\"Main Data Directory Found\")\n",
        "  if not os.path.exists(transfer_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Transfer Data Directory\")\n",
        "    os.makedirs(transfer_dir)\n",
        "  else:\n",
        "    print(\"Transfer Data Directory Found\")\n",
        "  if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Model Directory\")\n",
        "    os.makedirs(model_dir)\n",
        "  else:\n",
        "    print(\"Model Directory Found\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Main Data Directory Found\n",
            "Transfer Data Directory Found\n",
            "Model Directory Found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNp06xZqiWKD"
      },
      "source": [
        "#### Setup Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjxfLJqifsa5",
        "outputId": "9964e9e4-dde7-45ab-b7d5-12bbe44258df"
      },
      "source": [
        "## Begin by importing . . .  oh . . . everything!\n",
        "try:\n",
        "  import math\n",
        "  import random\n",
        "  import numpy as np\n",
        "\n",
        "  import glob\n",
        "  import io\n",
        "  import base64\n",
        "  from time import sleep\n",
        "\n",
        "  from collections import deque\n",
        "\n",
        "  import gym\n",
        "  import tensorflow as tf\n",
        "  import tensorflow_hub as hub\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "  from tensorflow.keras.optimizers import Nadam, Adam\n",
        "  from tensorflow import keras\n",
        "\n",
        "  import matplotlib\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "\n",
        "  from IPython.display import HTML\n",
        "  from IPython import display as ipythondisplay\n",
        "  from IPython.display import clear_output\n",
        "  \n",
        "  from gym.wrappers.atari_preprocessing import AtariPreprocessing\n",
        "  from baselines.common.atari_wrappers import make_atari, EpisodicLifeEnv, WarpFrame, ScaledFloatFrame, ClipRewardEnv, FrameStack, FireResetEnv\n",
        "\n",
        "\n",
        "\n",
        "except:\n",
        "  # %%capture\n",
        "  if COLAB:\n",
        "    ## For colab we must install some dependancies\n",
        "    # !pip install baselines\n",
        "    !apt-get install -y xvfb x11-utils\n",
        "    ## Next we will need to install a virtual display and correct Open AI installation\n",
        "    !pip install gym[all]==0.17.3\n",
        "    !pip install pyvirtualdisplay==0.2.* \n",
        "    !pip install PyOpenGL==3.1.* \n",
        "    !pip install PyOpenGL-accelerate==3.1.*\n",
        "    !pip install pyglet\n",
        "    # So let's setup the virtual display\n",
        "    import pyvirtualdisplay\n",
        "    # use False with Xvfb\n",
        "    _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "    _ = _display.start()\n",
        "    # Now Check the Display\n",
        "    !echo $DISPLAY\n",
        "\n",
        "if USE_GPU_SUPPORT:\n",
        "  print(\"TF version:\", tf.__version__)\n",
        "  print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "  # Set Memory Growth\n",
        "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "  if gpus:\n",
        "    try:\n",
        "      # Currently, memory growth needs to be the same across GPUs\n",
        "      for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(gpu)\n",
        "      logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "      print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "      # Memory growth must be set before GPUs have been initialized\n",
        "      print(e)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version: 2.4.1\n",
            "Num GPUs Available:  1\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "1 Physical GPUs, 1 Logical GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYyAtezpCwYg"
      },
      "source": [
        "#### The Wrappers\n",
        "So here we will use some wrappers from the baselines package. This broke my Tensorflow installation on my home server so I avoided wrappers during the majority of this project however, in the end using the optimized implementations in the wrapper drove speed up 1000% or so. So, if you are implementing your own version of this experiment, I cannot stress enough how important it is to study up on these wrappers to start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw2g6PjZPS9m",
        "outputId": "8bfde089-90e5-4701-b9ad-265f38b6887d"
      },
      "source": [
        "# Make Environment\n",
        "\n",
        "env = make_atari(ENVIRONMENT_NAME)\n",
        "env = EpisodicLifeEnv(env)\n",
        "\n",
        "if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "\n",
        "env = WarpFrame(env)\n",
        "\n",
        "env = ScaledFloatFrame(env)\n",
        "\n",
        "env = ClipRewardEnv(env)\n",
        "\n",
        "env = FrameStack(env, 3)\n",
        "\n",
        "# myWrappedEnv = FrameStack(atariWrap, 3)                  env_wrappers=[lambda env: ActionRepeat(env, times=4)])\n",
        "env.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "# env = gym.make(ENVIRONMENT_NAME)\n",
        "state_size = (84, 84, 3)\n",
        "action_size = env.action_space.n\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"s2_MAT_Agent_Testing_v2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 84, 84, 3)]       0         \n",
            "_________________________________________________________________\n",
            "s1_MAT_ImageClassifier_v5 (F (None, 9, 9, 64)          39008     \n",
            "_________________________________________________________________\n",
            "s2_MAT_ImageClassifier_v5 (F (None, 512)               1643072   \n",
            "_________________________________________________________________\n",
            "s3_MAT_ImageClassifier_v2 (F (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 1,684,132\n",
            "Trainable params: 1,684,132\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vtDg55hojmL"
      },
      "source": [
        "#### Setup Progress Bot\n",
        "\n",
        "Here we find Chippie the progress bot, I in the hours of waiting for code to test and the constant need to debug I created a simple progress bar that slowly turned into an entire diagnostic companion. However, as debugging reveiled greater and greater speed enhancements, Chippies output is limited to every 250 Episodes now and his diagnostic features removed or turned off.\n",
        "\n",
        "As one benefit, note that when using Gradient tape we don't get standard output from tensorflow so Chippie's periodic reports include leading and trailing averages of the last 1000, and previous 1000 loss calculations allowing us to see how our agent is progressing through the training process. If you are tinkering with the code you might try out the diagnostic mode to get a snap shot of the DQN learning process as it is actually happening."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyqPA-frPMS1"
      },
      "source": [
        "# import gc\n",
        "import sys\n",
        "\n",
        "class ChippieProgressBot:\n",
        "\n",
        "    def __init__(self, window_size=100, row_configuration=8, log_size=2000):\n",
        "        self.rowCount = 0\n",
        "        self.miniScore = 0\n",
        "        self.movingAverage = []\n",
        "        self.windowSize = window_size\n",
        "        self.facelist1 = [\"~{0_0}~\",\"~{o_o}~\",\"~{o_0}~\",\"~{0_o}~\"]\n",
        "        self.row_configuration = row_configuration\n",
        "        self.lossLog = []\n",
        "        self.logSize = log_size\n",
        "        self.lessChippies = True\n",
        "        self.skippedChippies = 250\n",
        "\n",
        "    def training(self, score):\n",
        "      if not self.lessChippies:\n",
        "        if self.miniScore < score:\n",
        "            self.miniScore += 1\n",
        "            print(self.facelist1[int(self.miniScore % 4)] + str(int(score))+\"   \", end='')\n",
        "            self.rowCount += 1\n",
        "            if self.rowCount >= self.row_configuration:\n",
        "                print(\"\\n\")\n",
        "                self.rowCount = 0\n",
        "        else:\n",
        "           return\n",
        "      else:\n",
        "        return\n",
        "\n",
        "\n",
        "    def q_update(self):\n",
        "      if not self.lessChippies:\n",
        "        print(\"\"\"  {-_-}    \"\"\", end='')\n",
        "        self.rowCount += 1\n",
        "        if self.rowCount >= MAX_CHIPPIES_ROW:\n",
        "            print(\"\\n\")\n",
        "            self.rowCount = 0\n",
        "      else:\n",
        "        return\n",
        "\n",
        "    def dead(self, score, totalScore, episode, completion_target,  survived, experiance, memory, epsilon):\n",
        "      \n",
        "        if len(self.movingAverage) >= self.windowSize:\n",
        "          del self.movingAverage[:1]\n",
        "        self.movingAverage.append(score)\n",
        "        if len(self.lossLog) > 2:\n",
        "          trailing = sum(self.lossLog[:len(self.lossLog)//2])/(len(self.lossLog)*0.5)\n",
        "\n",
        "          leading = sum(self.lossLog[len(self.lossLog)//2:])/(len(self.lossLog)*0.5)\n",
        "\n",
        "        self.rowCount = 0\n",
        "        self.miniScore = 0\n",
        "        if not self.lessChippies:\n",
        "\n",
        "          # running_reward = np.mean(episode_reward_history)\n",
        "          print(\"\"\" `{x_X}~   \"\"\"+\"\\n\")\n",
        "\n",
        "          print(\"Episode: {}/{}, Episode Score: {}, Avg Episode Score: {:.4}, Survival Time: {}\"\n",
        "          .format(episode+1, completion_target, score, totalScore/(episode+1), survived)+\"\\n\")\n",
        "\n",
        "          print(\"Your agent has a running average Score per 100 episodes of: \\{^,^}~\"+\"{:.3}\"\n",
        "          .format(sum(self.movingAverage)/len(self.movingAverage))+\"\\n\")\n",
        "\n",
        "          print(\"Total Steps: {}, Memory Size: {}, Current Epsilon Value: {:.2}, Leading Loss: {:.4}/ Trailing Loss: {:.4}\"\n",
        "          .format(experiance, memory, epsilon, leading, trailing)+\"\\n\\n\")\n",
        "\n",
        "        \n",
        "        else:\n",
        "          if (episode+1) % self.skippedChippies == 0:\n",
        "\n",
        "            print(\"\\n\"+\"Episode: {}/{}, Episode Score: {}, Avg Episode Score: {:.4}, Survival Time: {}\"\n",
        "            .format(episode+1, completion_target, score, totalScore/(episode+1), survived))\n",
        "\n",
        "            print(\"Your agent has a running average Score per 100 episodes of: \\{^,^}~\"+\"{:.3}\"\n",
        "            .format(sum(self.movingAverage)/len(self.movingAverage)))\n",
        "\n",
        "            print(\"Total Steps: {}, Memory Size: {}, Current Epsilon Value: {:.2}, Leading Loss: {:.4}/ Trailing Loss: {:.4}\"\n",
        "            .format(experiance, memory, epsilon, leading, trailing)) \n",
        "          else:\n",
        "            return\n",
        "\n",
        "\n",
        "    def logLoss(self, loss):\n",
        "        if len(self.lossLog) >= self.logSize:\n",
        "            del self.lossLog[:1]\n",
        "        self.lossLog.append(loss)\n",
        "        # trailing = sum(self.lossLog[:len(self.lossLog)//2])/(len(self.lossLog)*0.5)\n",
        "        # print(leading)\n",
        "        # leading = sum(self.lossLog[len(self.lossLog)//2:])/(len(self.lossLog)*0.5)\n",
        "        # print(trailing)\n",
        "\n",
        "# chippie.training(score)\n",
        "chippie = ChippieProgressBot(window_size=100, row_configuration=MAX_CHIPPIES_ROW)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR4qY7HhnhNq"
      },
      "source": [
        "#### The Frame PreProcessor and Ring Buffer that was: New Replay Memory\n",
        "\n",
        "After managing to make batches acceptable to the TensorFlow's .fit() method, we noticed a massive increase in training speed. The victory was short lived, however, as memory usage skyrocket outside the limits of both colaboratory and one local workstation (128GB). So, both the deque and image pre-processors had to be re-examined. This resulted in a home brewed ring buffer and numpy base preprocessor being implemented, which monumentally increased speed. Implementing the image pre-processor buffer using numpy arrays saved a few extra operations so I am confident this was all we cound do. However, in the final edit we realized that the FrameStack Wrapper from the Baselines package could be instantiated with 3 frame setting rendering this preprocessor no longer necessary and implemented a rotating list ring buffer based on Geron's implementation which provides the random access benefits of our ring buffer that doesn' waste cpu cycles moving indices."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxip_vgCmhJh"
      },
      "source": [
        "# Geron provides a great note on this buffer design that creates significant speed increases.\n",
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = np.empty(max_size, dtype=np.object)\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.randint(self.size, size=batch_size)\n",
        "        return self.buffer[indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F5PijpNi9Wg"
      },
      "source": [
        "#### Define the agent\n",
        "\n",
        "In the search for optimization, we found a few pointers to use variance scaling in layer declaration. I include the links for a deeper dive, however essentially this means that the initiallization of weights and biases are resricted to a range with a standard deviation of <= 2.0. This makes sense since the Q-values we are hoping for will yield need to be close to each other to promote further explorations.\n",
        "\n",
        "\n",
        "After, careful examination of a much faster implementation attempting to solve the traditional 4-frame 84x84 version was very helpful. So, we took time to implement the network using GradientTape(), which made the code run about twice as fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Nw-i27h9nR"
      },
      "source": [
        "class DQNAgent:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.replay_memory = ReplayMemory(max_size=100000)\n",
        "\n",
        "        \n",
        "        self.update_rate = Q_UPDATE_FREQUENCY # Number of steps until updating the target network\n",
        "\n",
        "        self.gamma = agent_gamma # decay or discount rate\n",
        "\n",
        "        self.epsilon = agent_epsilon # exploration rate\n",
        "        self.epsilon_decay = agent_epsilon_decay # exploration decay\n",
        "        self.epsilon_min = agent_epsilon_min # min exploration\n",
        "\n",
        "        self.learning_rate = agent_learning_rate # SGD or Nadam rate param\n",
        "\n",
        "        # self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_new_model()\n",
        "        # self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_new_model()\n",
        "\n",
        "        # self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_1S_transfer_model()\n",
        "        # self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_1S_transfer_model()\n",
        "\n",
        "        self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_2S_transfer_model()\n",
        "        self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_2S_transfer_model()\n",
        "\n",
        "        # self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_3S_transfer_model()\n",
        "        # self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_3S_transfer_model()\n",
        "\n",
        "        self.target_model.set_weights(self.model.get_weights()) # create Q-target network\n",
        "        \n",
        "        self.model.summary()\n",
        "\n",
        "  \n",
        "    def _build_new_model(self): # private method\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_conv1 = tf.keras.layers.Conv2D(32, kernel_size=(8, 8),strides=4, activation='relu', name='S1_Conv1', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s1_input_layer)\n",
        "        s1_output = tf.keras.layers.Conv2D(64, kernel_size=(4, 4),strides=2, activation='relu', name='S1_Conv2', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s1_conv1)\n",
        "        s1_model = tf.keras.Model(inputs=s1_input_layer, outputs=s1_output)\n",
        "        # s1_model.summary()\n",
        "\n",
        "       # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(9,9,64))\n",
        "        s2_conv1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),strides=1, activation='relu', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_input_layer)\n",
        "        s2_flat_1 = tf.keras.layers.Flatten(name='S2_Flat1')(s2_conv1)\n",
        "        s2_output = tf.keras.layers.Dense(512, activation='relu', name='s2_Dense_1', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_flat_1)\n",
        "        s2_model = tf.keras.Model(inputs=s2_input_layer, outputs=s2_output)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_output = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "        s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "        \n",
        "\n",
        "    def _build_1S_transfer_model(self): # private method    \n",
        "\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_model = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)\n",
        "        s1_model._name = SLICE_ONE_NAME\n",
        "        # s1_model.summary()\n",
        "\n",
        "        # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(9,9,64))\n",
        "        s2_conv1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),strides=1, activation='relu', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_input_layer)\n",
        "        s2_flat_1 = tf.keras.layers.Flatten(name='S2_Flat1')(s2_conv1)\n",
        "        s2_output = tf.keras.layers.Dense(512, activation='relu', name='s2_Dense_1', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_flat_1)\n",
        "        s2_model = tf.keras.Model(inputs=s2_input_layer, outputs=s2_output)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_output = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "        s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "\n",
        "\n",
        "    def _build_2S_transfer_model(self): # private method    \n",
        "\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE+(3,))\n",
        "        s1_model = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)\n",
        "        s1_model._name = SLICE_ONE_NAME\n",
        "        # s1_model.summary()\n",
        "\n",
        "        # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(9,9,64))\n",
        "        # s2_input_layer = tf.keras.Input(shape=(s1_model.shape[1::1]))\n",
        "        s2_model = tf.keras.models.load_model(model_dir + '/' + SLICE_TWO_NAME)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_output = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "        s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "\n",
        "\n",
        "    def _build_3S_transfer_model(self): # private method    \n",
        "\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_model = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)\n",
        "        s1_model._name = SLICE_ONE_NAME\n",
        "        # s1_model.summary()\n",
        "\n",
        "        # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(9,9,64))\n",
        "        s2_model = tf.keras.models.load_model(model_dir + '/' + SLICE_TWO_NAME)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three                                   #### ndim 4?\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_model = tf.keras.models.load_model(model_dir + '/' + SLICE_THREE_NAME)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # My newly reimplemented memory, Hopefully it makes us faster\n",
        "        self.replay_memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def jit_sampler(self, batch_size):\n",
        "        batch = self.replay_memory.sample(batch_size)\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = [\n",
        "                np.array([exp[fidx] for exp in batch])\n",
        "                for fidx in range(5)]\n",
        "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "\n",
        "\n",
        "    def train(self, batch_size): # method that trains agent model\n",
        "        ### The New Implementation\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.jit_sampler(batch_size)\n",
        "\n",
        "        ## This is where all the magic happens\n",
        "        qValueF = self.target_model.predict_on_batch(next_state_batch) # approximate future reward\n",
        "\n",
        "        qValueUpdate = reward_batch + self.gamma * tf.reduce_max(qValueF, axis=1)\n",
        "\n",
        "        qValueUpdate = qValueUpdate * (1 - done_batch) - done_batch\n",
        "\n",
        "        oneHotMask = tf.one_hot(action_batch, self.action_size)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Train the model on the states and updated Q-values\n",
        "            OqValue = self.model(state_batch)\n",
        "\n",
        "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "            OqValue_action = tf.reduce_sum(tf.multiply(OqValue, oneHotMask), axis=1)\n",
        "\n",
        "            # Calculate loss between new Q-value and old Q-value\n",
        "            loss = self.loss_function(qValueUpdate, OqValue_action)\n",
        "            if CHIPPIE_PROGESS_REPORTS:\n",
        "                  chippie.logLoss(loss)\n",
        "\n",
        "        if DIAGNOSTIC_MODE:\n",
        "          print(\"Q-Values: \", qValueF[1])\n",
        "          print(\"Q-Values Update: \", qValueUpdate[1])\n",
        "          print(\"Masked Update:   \", oneHotMask[1])\n",
        "          print(\"Old Activations: \", OqValue[1])\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "        if DIAGNOSTIC_MODE:\n",
        "          newQ = self.model.predict_step(state_batch)\n",
        "          print(\"New Activations: \", newQ[1], \"\\n\")\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon: \n",
        "            return random.randrange(self.action_size) # Do something stupid! (i.e. take a random action)\n",
        "        action_values = self.model.predict_step(tf.expand_dims(tf.convert_to_tensor(state), 0))\n",
        "        return tf.argmax(action_values[0]).numpy()\n",
        "\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "## Intialize Our Agent\n",
        "agent = DQNAgent(state_size, action_size)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z5loBrJiHON"
      },
      "source": [
        "#### Define Training Loop Environment\n",
        "\n",
        "Getting your reward function is pretty paramount. In this case we will clip rewards to +1 and -1, with -1 being awarded for reaching a terminal state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8cbu_VJh-E4"
      },
      "source": [
        "def myPlayground():\n",
        "\n",
        "    # Empty Rewards Counter\n",
        "    reward = 0\n",
        "    all_rewards = 0\n",
        "    total_time = 0\n",
        "    done = False\n",
        "\n",
        "    # Display in Colab\n",
        "    if COLAB:\n",
        "        if RENDER == True:\n",
        "            done, ax = plt.subplots(1, 1)\n",
        "            img = ax.imshow(env.render('rgb_array'))    \n",
        "\n",
        "    # Make Some Starting data\n",
        "    state = env.reset() # Reset state for new episode\n",
        "    while agent.replay_memory.size < STARTING_MEMORY_SIZE:\n",
        "      action = random.randrange(agent.action_size)\n",
        "      next_frame, reward, done, _ = env.step(action) # Agent sends action to env wrapper and gets feedback\n",
        "\n",
        "      next_state = next_frame\n",
        "      if reward > 0.0: # Check and modify reward\n",
        "        reward = 1.0\n",
        "      if reward < 0.0:\n",
        "        reward = -1.0\n",
        "      if done:\n",
        "        reward = -1.0\n",
        "      agent.remember(state, action, reward, next_state, done) # Store sequence in replay memory\n",
        "      state = next_state # Update state\n",
        "      if done:\n",
        "        state = env.reset()\n",
        "    print(\"Starting Frames Acquired\")\n",
        "\n",
        "    # Training Loop\n",
        "    for e in range(n_episodes): ## Go Eat Cherries\n",
        "        done = False\n",
        "        time = 0\n",
        "        game_score = 0\n",
        "        reward = 0\n",
        "\n",
        "\n",
        "        state = env.reset() # Reset state for new episode\n",
        "        \n",
        "        for skip in range(skip_start): # Skip the start of each game\n",
        "            env.step(0)\n",
        "\n",
        "        while not done:\n",
        "        \n",
        "            time += 1\n",
        "            total_time += 1\n",
        "\n",
        "            # Display\n",
        "            if RENDER:\n",
        "                if COLAB:\n",
        "                    img.set_data(env.render(mode='rgb_array')) \n",
        "                    ax.axis('off')\n",
        "                    ipythondisplay.display(plt.gcf())\n",
        "                    ipythondisplay.clear_output(wait=True)\n",
        "                else:\n",
        "                    env.render()\n",
        "                    if OBSERVATION_MODE:\n",
        "                        sleep(0.02)\n",
        "\n",
        "            # Update Target Network\n",
        "            if total_time % agent.update_rate == 0:\n",
        "                agent.update_target_model()\n",
        "                if CHIPPIE_PROGESS_REPORTS:\n",
        "                  chippie.q_update()\n",
        "\n",
        "            # Transition Dynamics\n",
        "            action = agent.act(state) # Get action from agent\n",
        "            next_frame, reward, done, _ = env.step(action) # Agent sends action to env wrapper and gets feedback\n",
        "            next_state = next_frame\n",
        "            \n",
        "            # Sternly Validate Reward\n",
        "            if reward > 0.0: \n",
        "              reward = 1.0\n",
        "              game_score += reward\n",
        "            if reward < 0.0:\n",
        "              reward = -1.0\n",
        "            if done:\n",
        "              reward = -1.0\n",
        "              all_rewards += game_score\n",
        "              if CHIPPIE_PROGESS_REPORTS:\n",
        "                chippie.dead(score=game_score, totalScore=all_rewards, episode=e, completion_target=n_episodes, survived=time, \n",
        "                              experiance=total_time, memory=agent.replay_memory.size, epsilon=agent.epsilon)\n",
        "              agent.remember(state, action, reward, next_state, done) # Store death in replay memory\n",
        "              break\n",
        "\n",
        "            agent.remember(state, action, reward, next_state, done) # Store sequence in replay memory\n",
        "            state = next_state # Update state\n",
        "\n",
        "            if agent.replay_memory.size > STARTING_MEMORY_SIZE:\n",
        "              if not OBSERVATION_MODE:\n",
        "                if total_time % action_steps == 0:\n",
        "                  agent.train(BATCH_SIZE)\n",
        "                  if CHIPPIE_PROGESS_REPORTS:\n",
        "                    chippie.training(game_score)\n",
        "\n",
        "        if e % CHECKPOINT_FREQUENCY == 0 and agent.replay_memory.size > STARTING_MEMORY_SIZE+100:\n",
        "          print(\"\")\n",
        "          if SLICE_ONE_CHECKPOINT:\n",
        "              saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + SLICE_ONE_NAME + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY)) \n",
        "              tf.saved_model.save(agent.sliceOne, saved_model_path)\n",
        "\n",
        "          if SLICE_TWO_CHECKPOINT:\n",
        "              saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + SLICE_TWO_NAME + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY))\n",
        "              tf.saved_model.save(agent.sliceTwo, saved_model_path)\n",
        "          \n",
        "          if SLICE_THREE_CHECKPOINT:\n",
        "              saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + SLICE_TWO_NAME + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY))\n",
        "              tf.saved_model.save(agent.sliceTwo, saved_model_path)\n",
        "\n",
        "          if FULL_MODEL_CHECKPOINT:\n",
        "              saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + FULL_MODEL_NAME + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY))\n",
        "              tf.saved_model.save(agent.model, saved_model_path)\n",
        "\n",
        "          if SAVE_AGENT:\n",
        "              agent.save(model_dir+ \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + FULL_MODEL_NAME + \"agent_weights_\" + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY)) +\".hdf5\")\n",
        "          \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgI4QKIOo6Nl"
      },
      "source": [
        "#### Interactive Playground (runs automatically)\n",
        "\n",
        "As the agent finally blitzed through the code, we are getting close to the deadline and what began as an awesome undertaking in abstraction might formally (and temporarily) become a pure comparision of how the transfer learned weights from notebook 1 affect training speed. However, there is still yet another problem, because with only 9 days left to finish, and each test taking a full 12 hours to see results the results come in and the model is still showing no progress. Luckily, one more day of debugging and preparation found a simple bug causing enourmous amounts of pre-buffer data to be labelled terminal. After this fix, the agent has begun to train comparably to the only other working example we found (linked above). This implementation is still slower, however, in that example the baselines wrapper provided by DeepMind is used, which contains significant speed and memory optimizations which unfortunately, only appear to offer frame stacking at 4 frames. Since this would impair our transfer learning basis this is currently the fastest implementation we can provide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN3eI0eIloW-"
      },
      "source": [
        "##### Launch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsXYj6N7lsmo"
      },
      "source": [
        "# #### Run From Bottom\n",
        "## This will take about 3 minutes in colab to gather starting data and output some feedback\n",
        "## after this you can expect chippieBot limited updates about every 5 minutes for the next 7-12 hours.\n",
        "myPlayground()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZKKOtCtCcA7"
      },
      "source": [
        "#### Output\n",
        "\n",
        "When we are all done building this last box will zip our progress for download so we can continue our work later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE2np3HdCla9"
      },
      "source": [
        "ZIP_OUTPUT = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOaTq8Gi-I68"
      },
      "source": [
        "%%capture\n",
        "if ZIP_OUTPUT:\n",
        "  zip = !zip -r ISAR_Model_Data_Output.zip ISAR_Model_Data"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}