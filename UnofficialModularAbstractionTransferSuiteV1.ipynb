{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ModularAbstractionTransferSuiteV1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ynM5BeDfWQvH",
        "n1EFy_NoE7Wl",
        "BGIl3BKliih1",
        "HNp06xZqiWKD",
        "tdX2lI15iCvS",
        "0vtDg55hojmL",
        "1F5PijpNi9Wg",
        "8z5loBrJiHON",
        "x8r9t3hNQCf3"
      ],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOxBJxxpqKvZSDq8rTkvf51",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DocLock17/ISAR_Final/blob/main/UnofficialModularAbstractionTransferSuiteV1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXiHbkQgfw9a"
      },
      "source": [
        "# Modular Abstraction Transfer Suite\n",
        "\n",
        "### A Deep Mind Based Experimental Platform For Reasearch In Abstaction And Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynM5BeDfWQvH"
      },
      "source": [
        "#### Interactive Playgound (run after notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrFuoAxKQclk"
      },
      "source": [
        "# # # Run From Top (Hint: It must be loaded commented first)\n",
        "# myPlayground()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1EFy_NoE7Wl"
      },
      "source": [
        "#### Selector Menu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-9mrlfuSA23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "343670ee-9a21-4b23-e0c2-625b7439144f"
      },
      "source": [
        "##   Game Selector\n",
        "ENVIRONMENT_NAME = 'BreakoutNoFrameskip-v4' #@param ['Atlantis-v0', 'DemonAttack-v0', 'Phoenix-v0', 'Riverraid-v0', 'Solaris-v0', 'Asterix-v0', 'Breakout-v0', 'Boxing-v0', 'Pong-v0', 'BattleZone-v0', 'SpaceInvaders-v0', 'BeamRider-v0','AtlantisNoFrameskip-v4', 'DemonAttackNoFrameskip-v4', 'PhoenixNoFrameskip-v4', 'RiverraidNoFrameskip-v4', 'SolarisNoFrameskip-v4', 'AsterixNoFrameskip-v4', 'BreakoutNoFrameskip-v4', 'BoxingNoFrameskip-v4', 'PongNoFrameskip-v4', 'BattleZoneNoFrameskip-v4', 'SpaceInvadersNoFrameskip-v4', 'BeamRiderNoFrameskip-v4']\n",
        "\n",
        "# Render gameplay in cell or viewer\n",
        "RENDER = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Slow down game play and discontinue training for observation\n",
        "OBSERVATION_MODE = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Display Text Output to Observe Activations and Q Updates\n",
        "DIAGNOSTIC_MODE = False #@param {type:\"boolean\"}\n",
        "\n",
        "## Chippie the Progress Bot Settings       (Name courtesy of: Kylie Locker)\n",
        "CHIPPIE_PROGESS_REPORTS = True #@param {type:\"boolean\"}\n",
        "# Row for progress bot stacking ( 1, 8, 12 )\n",
        "MAX_CHIPPIES_ROW = 8 #@param {type:\"integer\"}\n",
        "\n",
        "# Provide GPU information\n",
        "USE_GPU_SUPPORT = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "## Data Settings\n",
        "# Import and Unzip Hosted Dataset\n",
        "CREATE_MISSING_DIRECTORIES = True #@param {type:\"boolean\"}\n",
        "IMPORT_MAIN_DATA = True #@param {type:\"boolean\"}\n",
        "IMPORT_SENTIMENT_DATA = True #@param {type:\"boolean\"}\n",
        "IMPORT_MODEL_DATA = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "## Agent Settings\n",
        "# Parameters\n",
        "n_episodes = 15000 #@param {type:\"integer\"}\n",
        "BATCH_SIZE =  32#@param {type:\"integer\"}\n",
        "action_steps =  4#@param {type:\"integer\"}\n",
        "skip_start = 1 #@param {type:\"integer\"}\n",
        "agent_gamma = 0.985 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "agent_epsilon = 1.0 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "agent_epsilon_decay = 0.9999 #@param {type:\"slider\", min:0.5, max:1.000, step:0.0001}\n",
        "agent_epsilon_min = 0.1 #@param {type:\"slider\", min:0, max:1.000, step:0.0001}\n",
        "agent_learning_rate = 0.00025 #@param {type:\"number\", min:0.0000, max:0.0200, step:0.00001}\n",
        "# lr=0.00042 # lr=0.00125 #lr=0.00025\n",
        "\n",
        "## Buffer Settings\n",
        "STARTING_MEMORY_SIZE = 25000#@param {type:\"integer\"}\n",
        "MAX_MEMORY_LENGTH = 100000 #@param {type:\"integer\"}\n",
        "Q_UPDATE_FREQUENCY = 5000 #@param {type:\"integer\"}\n",
        "\n",
        "# Pre Buffer Guidance\n",
        "WINDOW_SIZE = (104, 80)\n",
        "\n",
        "# Sub-Model Settings\n",
        "SLICE_ONE_NAME = \"s1_MAT_ImageClassifier_v3\" #@p\\aram {type:\"string\"}\n",
        "SLICE_ONE_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "SLICE_ONE_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "SLICE_TWO_NAME = \"s2_MAT_ImageClassifier_v3\" #@param {type:\"string\"}\n",
        "SLICE_TWO_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "SLICE_TWO_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "SLICE_THREE_NAME = \"s3_MAT_ImageClassifier_v3\" #@param {type:\"string\"}\n",
        "SLICE_THREE_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "SLICE_THREE_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "FULL_MODEL_NAME = \"s2_MAT_Agent_Testing_v1\" #@param {type:\"string\"}\n",
        "FULL_MODEL_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "FULL_MODEL_CHECKPOINT = False #@param {type:\"boolean\"}\n",
        "\n",
        "CHECKPOINT_FREQUENCY =  10#@param {type:\"integer\"}\n",
        "\n",
        "SAVE_AGENT = False #@param {type:\"boolean\"}\n",
        "\n",
        "# ZIP_OUTPUT = True #@param {type:\"boolean\"}\n",
        "\n",
        "## Game selector feedback\n",
        "print(\"Selected Game: \" + ENVIRONMENT_NAME)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected Game: BreakoutNoFrameskip-v4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5MNi-KuyiGF"
      },
      "source": [
        "# MODEL_NAME = \"m1_MAT_ImageClassifier_v3\" #@par\\am {type:\"string\"}\n",
        "\n",
        "# = \"s2_MAT_ImageClassifier_v3\" #@par\\am {type:\"string\"}\n",
        "# SLICE_THREE_NAME = \"s3_MAT_ImageClassifier_v3\" #@pa\\ram {type:\"string\"}\n",
        "# EXTRACT_SLICE_ONE = True #@par\\am {type:\"boolean\"}\n",
        "# EXTRACT_SLICE_TWO = True #@par\\am {type:\"boolean\"}\n",
        "# EXTRACT_SLICE_THREE = True #@p\\aram {type:\"bool"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGIl3BKliih1"
      },
      "source": [
        "#### Setup Model Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFzeNuWrURPP"
      },
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  COLAB = True\n",
        "  \n",
        "except:\n",
        "  !pip install gdown\n",
        "  COLAB = False\n",
        "\n",
        "model_dir = 'ISAR_Model_Data'\n",
        "data_dir = 'ISAR_Main_Classification'\n",
        "transfer_dir = 'ISAR_Sentiment_Transfer'\n",
        "\n",
        "\n",
        "if IMPORT_MAIN_DATA :\n",
        "    if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1P7o1x4ZpPbd16VQDwaMzllbN-tlfqqIH\n",
        "      !unzip ISAR_Main_Classification.zip\n",
        "      !rm ISAR_Main_Classification.zip\n",
        "if IMPORT_SENTIMENT_DATA:\n",
        "    if not os.path.exists(transfer_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1UDUNnw04q5cvms5ibM6pNn-wXtJphXxZ\n",
        "      !unzip ISAR_Sentiment_Transfer.zip\n",
        "      !rm ISAR_Sentiment_Transfer.zip\n",
        "if IMPORT_MODEL_DATA:\n",
        "    if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1IVw1bEAmMDiVeHfPPBWBfMj3h9WliumD\n",
        "      !unzip ISAR_Model_Data.zip\n",
        "      !rm ISAR_Model_Data.zip\n",
        "      "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poXz23gKsczB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c32c41b-0f0d-4ece-ebdd-ae4d181a43a0"
      },
      "source": [
        "if CREATE_MISSING_DIRECTORIES:  \n",
        "  if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Main Data Directory\")\n",
        "    os.makedirs(data_dir)\n",
        "  else:\n",
        "    print(\"Main Data Directory Found\")\n",
        "  if not os.path.exists(transfer_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Transfer Data Directory\")\n",
        "    os.makedirs(transfer_dir)\n",
        "  else:\n",
        "    print(\"Transfer Data Directory Found\")\n",
        "  if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Model Directory\")\n",
        "    os.makedirs(model_dir)\n",
        "  else:\n",
        "    print(\"Model Directory Found\")\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Main Data Directory Found\n",
            "Transfer Data Directory Found\n",
            "Model Directory Found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNp06xZqiWKD"
      },
      "source": [
        "#### Setup Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjxfLJqifsa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2c2d80f-ea90-4dd5-bf44-69c7fca9c795"
      },
      "source": [
        "## Begin by importing . . .  oh . . . everything!\n",
        "try:\n",
        "  import math\n",
        "  import random\n",
        "  import numpy as np\n",
        "\n",
        "  import glob\n",
        "  import io\n",
        "  import base64\n",
        "  from time import sleep\n",
        "\n",
        "  from collections import deque\n",
        "\n",
        "  import gym\n",
        "  import tensorflow as tf\n",
        "  import tensorflow_hub as hub\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "  from tensorflow.keras.optimizers import Nadam, Adam\n",
        "  from tensorflow import keras\n",
        "\n",
        "  import matplotlib\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "\n",
        "  from IPython.display import HTML\n",
        "  from IPython import display as ipythondisplay\n",
        "  from IPython.display import clear_output\n",
        "\n",
        "except:\n",
        "  %%capture\n",
        "  if COLAB:\n",
        "    ## For colab we must install some dependancies\n",
        "    !apt-get install -y xvfb x11-utils\n",
        "    ## Next we will need to install a virtual display and correct Open AI installation\n",
        "    !pip install gym[all]==0.17.3\n",
        "    !pip install pyvirtualdisplay==0.2.* \n",
        "    !pip install PyOpenGL==3.1.* \n",
        "    !pip install PyOpenGL-accelerate==3.1.*\n",
        "    !pip install pyglet\n",
        "    # So let's setup the virtual display\n",
        "    import pyvirtualdisplay\n",
        "    # use False with Xvfb\n",
        "    _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "    _ = _display.start()\n",
        "    # Now Check the Display\n",
        "    !echo $DISPLAY\n",
        "\n",
        "if USE_GPU_SUPPORT:\n",
        "  print(\"TF version:\", tf.__version__)\n",
        "  print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "  # Set Memory Growth\n",
        "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "  if gpus:\n",
        "    try:\n",
        "      # Currently, memory growth needs to be the same across GPUs\n",
        "      for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(gpu)\n",
        "      logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "      print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "      # Memory growth must be set before GPUs have been initialized\n",
        "      print(e)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version: 2.4.1\n",
            "Num GPUs Available:  1\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "1 Physical GPUs, 1 Logical GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdX2lI15iCvS"
      },
      "source": [
        "#### Setup Frame PreProcessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT-35yCogEXo"
      },
      "source": [
        "### Custom Pre-Processor\n",
        "# Stack frames and average on axis=-1 to produce a Single 210, 160, 1 greyscale image #old crop = img = frame[1:176:2, ::2]\n",
        "# Rescale to 104, 80, 1 then stack in three's (3) to produce a single RGB compatible representation of evironmental space-time\n",
        "class TFramePreBuffer:\n",
        "\n",
        "    def __init__(self, t_size=3, setting=0, scaling=1):\n",
        "        self.un_flicker_memory = []\n",
        "        self.temporal_memory = []\n",
        "        self.temporal_size = t_size\n",
        "        self.processor_setting = setting\n",
        "        self.scale = scaling\n",
        "\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        if self.scale == 0:\n",
        "            if self.processor_setting == 0:\n",
        "                return np.array(frame[1:209:2, ::2])*(1/255.0)\n",
        "\n",
        "            if self.processor_setting == 1:\n",
        "                while len(self.temporal_memory) < self.temporal_size:\n",
        "                    self.temporal_memory.append(frame[1:209:2, ::2].mean(axis=2,keepdims=True)) \n",
        "                temporal_image = np.concatenate((self.temporal_memory[0],\n",
        "                                                  self.temporal_memory[1],\n",
        "                                                  self.temporal_memory[2]), \n",
        "                                                axis=-1)*(1/255.0)\n",
        "                if len(self.temporal_memory) >= self.temporal_size:\n",
        "                    del self.temporal_memory[:1]\n",
        "                return temporal_image\n",
        "\n",
        "            if self.processor_setting == 2:\n",
        "                while len(self.un_flicker_memory) < 2:\n",
        "                    self.un_flicker_memory.append(frame[1:209:2, ::2])\n",
        "                while len(self.temporal_memory) < self.temporal_size:\n",
        "                    self.temporal_memory.append(np.concatenate((self.un_flicker_memory[0].mean(axis=2,keepdims=True),\n",
        "                                                                self.un_flicker_memory[1].mean(axis=2,keepdims=True)),\n",
        "                                                                axis=-1).max(axis=2))\n",
        "                temporal_image = np.concatenate(np.expand_dims((self.temporal_memory[0], \n",
        "                                                                self.temporal_memory[1], \n",
        "                                                                self.temporal_memory[2]), \n",
        "                                                              axis=-1), axis=-1)*(1/255.0)\n",
        "                if len(self.temporal_memory) >= self.temporal_size:\n",
        "                    del self.temporal_memory[:1]\n",
        "                return temporal_image\n",
        "\n",
        "\n",
        "        if self.scale == 1:\n",
        "            if self.processor_setting == 0:\n",
        "                return (np.array(frame[1:209:2, ::2])-128)*(1/128.0)\n",
        "\n",
        "            if self.processor_setting == 1:\n",
        "                while len(self.temporal_memory) < self.temporal_size:\n",
        "                    self.temporal_memory.append(frame[1:209:2, ::2].mean(axis=2,keepdims=True)) \n",
        "                temporal_image = (np.concatenate((self.temporal_memory[0],\n",
        "                                                  self.temporal_memory[1],\n",
        "                                                  self.temporal_memory[2]), \n",
        "                                                axis=-1)-128)*(1/128.0)\n",
        "                if len(self.temporal_memory) >= self.temporal_size:\n",
        "                    del self.temporal_memory[:1]\n",
        "                return temporal_image\n",
        "\n",
        "            if self.processor_setting == 2:\n",
        "                while len(self.un_flicker_memory) < 2:\n",
        "                    self.un_flicker_memory.append(frame[1:209:2, ::2])\n",
        "                while len(self.temporal_memory) < self.temporal_size:\n",
        "                    self.temporal_memory.append(np.concatenate((self.un_flicker_memory[0].mean(axis=2,keepdims=True),\n",
        "                                                                self.un_flicker_memory[1].mean(axis=2,keepdims=True)),\n",
        "                                                                axis=-1).max(axis=2))\n",
        "                temporal_image = (np.concatenate(np.expand_dims((self.temporal_memory[0], \n",
        "                                                                self.temporal_memory[1], \n",
        "                                                                self.temporal_memory[2]), \n",
        "                                                              axis=-1), axis=-1)-128)/128\n",
        "                if len(self.temporal_memory) >= self.temporal_size:\n",
        "                    del self.temporal_memory[:1]\n",
        "                return temporal_image\n",
        "\n",
        "\n",
        "#### Instantiate and call with\n",
        "# tFrame.process_frame(frame)\n",
        "\n",
        "## Resize Only\n",
        "# tFrame = TFramePreBuffer(t_size=3, setting=0, scaling=1)\n",
        "\n",
        "## Resize Grayscale and Stack Temporally\n",
        "tFrame = TFramePreBuffer(t_size=3, setting=1, scaling=1)\n",
        "\n",
        "# ## Resize Grayscale De-Flicker and Stack Temporally\n",
        "# tFrame = TFramePreBuffer(t_size=3, setting=2, scaling=1)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vtDg55hojmL"
      },
      "source": [
        "#### Setup Progress Bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyqPA-frPMS1"
      },
      "source": [
        "class ChippieProgressBot:\n",
        "\n",
        "    def __init__(self, window_size=100, row_configuration=8):\n",
        "        self.rowCount = 0\n",
        "        self.miniScore = 0\n",
        "        self.movingAverage = []\n",
        "        self.windowSize = window_size\n",
        "        self.facelist1 = [\"~{0_0}~\",\"~{o_o}~\",\"~{o_0}~\",\"~{0_o}~\"]\n",
        "        self.row_configuration = row_configuration\n",
        "\n",
        "    def training(self, score):\n",
        "        if self.miniScore < score:\n",
        "            self.miniScore += 1\n",
        "            print(self.facelist1[int(self.miniScore % 4)] + str(int(score))+\"   \", end='')\n",
        "            self.rowCount += 1\n",
        "            if self.rowCount >= self.row_configuration:\n",
        "                print(\"\\n\")\n",
        "                self.rowCount = 0\n",
        "        else:\n",
        "           return\n",
        "\n",
        "\n",
        "    def q_update(self):\n",
        "        print(\"\"\"  {-_-}    \"\"\", end='')\n",
        "        self.rowCount += 1\n",
        "        if self.rowCount >= MAX_CHIPPIES_ROW:\n",
        "            print(\"\\n\")\n",
        "            self.rowCount = 0\n",
        "\n",
        "    def dead(self, score, totalScore, episode, completion_target,  survived, experiance, memory, epsilon):\n",
        "        \n",
        "        if len(self.movingAverage) >= self.windowSize:\n",
        "          del self.movingAverage[:1]\n",
        "        self.movingAverage.append(score)\n",
        "\n",
        "        print(\"\"\" `{x_X}~   \"\"\"+\"\\n\")\n",
        "\n",
        "        print(\"Episode: {}/{}, Episode Score: {}, Avg Episode Score: {:.4}, Survival Time: {}\"\n",
        "        .format(episode+1, completion_target, score, totalScore/(episode+1), survived)+\"\\n\")\n",
        "\n",
        "        print(\"\\{^,^}~\"+\"{:.3}\"\n",
        "        .format(sum(self.movingAverage)/len(self.movingAverage))+\"\\n\")\n",
        "\n",
        "        print(\"Total Steps: {}, Memory Size: {}, Current Epsilon Value: {:.2}\"\n",
        "        .format(experiance, memory, epsilon)+\"\\n\")\n",
        "\n",
        "        self.rowCount = 0\n",
        "        self.miniScore = 0\n",
        "\n",
        "\n",
        "     \n",
        "# chippie.training(score)\n",
        "chippie = ChippieProgressBot(window_size=100, row_configuration=MAX_CHIPPIES_ROW)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F5PijpNi9Wg"
      },
      "source": [
        "#### Define the agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Nw-i27h9nR"
      },
      "source": [
        "class DQNAgent:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        \n",
        "        self.state_memory = []\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []\n",
        "        self.next_state_memory = []\n",
        "        self.done_memory = [] \n",
        "        \n",
        "        self.update_rate = Q_UPDATE_FREQUENCY # Number of steps until updating the target network\n",
        "\n",
        "        self.gamma = agent_gamma # decay or discount rate\n",
        "\n",
        "        self.epsilon = agent_epsilon # exploration rate\n",
        "        self.epsilon_decay = agent_epsilon_decay # exploration decay\n",
        "        self.epsilon_min = agent_epsilon_min # min exploration\n",
        "\n",
        "        self.learning_rate = agent_learning_rate # SGD or Nadam rate param\n",
        "\n",
        "        # self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_new_model()\n",
        "        # self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_new_model()\n",
        "\n",
        "        # self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_1S_transfer_model()\n",
        "        # self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_1S_transfer_model()\n",
        "\n",
        "        self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_2S_transfer_model()\n",
        "        self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_2S_transfer_model()\n",
        "\n",
        "        # self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_3S_transfer_model()\n",
        "        # self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_3S_transfer_model()\n",
        "\n",
        "        self.target_model.set_weights(self.model.get_weights()) # create Q-target network\n",
        "        \n",
        "        self.model.summary()\n",
        "\n",
        "  \n",
        "    def _build_new_model(self): # private method\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_conv1 = tf.keras.layers.Conv2D(32, kernel_size=(8, 8),strides=4, activation='relu', name='S1_Conv1', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s1_input_layer)\n",
        "        s1_output = tf.keras.layers.Conv2D(64, kernel_size=(4, 4),strides=2, activation='relu', name='S1_Conv2', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s1_conv1)\n",
        "        s1_model = tf.keras.Model(inputs=s1_input_layer, outputs=s1_output)\n",
        "        # s1_model.summary()\n",
        "\n",
        "       # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(11, 8, 64))\n",
        "        s2_conv1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),strides=1, activation='relu', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_input_layer)\n",
        "        s2_flat_1 = tf.keras.layers.Flatten(name='S2_Flat1')(s2_conv1)\n",
        "        s2_output = tf.keras.layers.Dense(512, activation='relu', name='s2_Dense_1', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_flat_1)\n",
        "        s2_model = tf.keras.Model(inputs=s2_input_layer, outputs=s2_output)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_output = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "        s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "        \n",
        "\n",
        "    def _build_1S_transfer_model(self): # private method    \n",
        "\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_model = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)\n",
        "        s1_model._name = SLICE_ONE_NAME\n",
        "        # s1_model.summary()\n",
        "\n",
        "        # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(11, 8, 64))\n",
        "        s2_conv1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),strides=1, activation='relu', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_input_layer)\n",
        "        s2_flat_1 = tf.keras.layers.Flatten(name='S2_Flat1')(s2_conv1)\n",
        "        s2_output = tf.keras.layers.Dense(512, activation='relu', name='s2_Dense_1', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_flat_1)\n",
        "        s2_model = tf.keras.Model(inputs=s2_input_layer, outputs=s2_output)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_output = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "        s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "\n",
        "\n",
        "    def _build_2S_transfer_model(self): # private method    \n",
        "\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_model = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)\n",
        "        s1_model._name = SLICE_ONE_NAME\n",
        "        # s1_model.summary()\n",
        "\n",
        "        # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(11, 8, 64))\n",
        "        s2_model = tf.keras.models.load_model(model_dir + '/' + SLICE_TWO_NAME)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_output = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "        s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "\n",
        "\n",
        "    def _build_3S_transfer_model(self): # private method    \n",
        "\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_model = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)\n",
        "        s1_model._name = SLICE_ONE_NAME\n",
        "        # s1_model.summary()\n",
        "\n",
        "        # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(11, 8, 64))\n",
        "        s2_model = tf.keras.models.load_model(model_dir + '/' + SLICE_TWO_NAME)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three                                   #### ndim 4?\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_model = tf.keras.models.load_model(model_dir + '/' + SLICE_THREE_NAME)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # My newly implemented memory, Hopefully it makes us faster\n",
        "        self.state_memory.append(state)\n",
        "        self.action_memory.append(action)\n",
        "        self.reward_memory.append(reward)\n",
        "        self.next_state_memory.append(next_state)\n",
        "        self.done_memory.append(done)\n",
        "        if len(self.reward_memory) > MAX_MEMORY_LENGTH:\n",
        "            del self.reward_memory[:1]\n",
        "            del self.state_memory[:1]\n",
        "            del self.next_state_memory[:1]\n",
        "            del self.action_memory[:1]\n",
        "            del self.done_memory[:1]\n",
        "\n",
        "    def train(self, batch_size): # method that trains agent model\n",
        "\n",
        "        ### The New Implementation\n",
        "        idx = np.random.choice(range(len(self.reward_memory)), size=batch_size)\n",
        "\n",
        "        state_batch = np.array([self.state_memory[i] for i in idx])\n",
        "        action_batch = [self.action_memory[i] for i in idx]\n",
        "        reward_batch = [self.reward_memory[i] for i in idx]\n",
        "        next_state_batch = np.array([self.next_state_memory[i] for i in idx])\n",
        "        done_batch = tf.convert_to_tensor([float(self.done_memory[i]) for i in idx])\n",
        "        \n",
        "        ## This is where all the magic happens\n",
        "        qValueF = self.target_model.predict(next_state_batch) # approximate future reward\n",
        "\n",
        "        qValueUpdate = reward_batch + self.gamma * tf.reduce_max(qValueF, axis=1)\n",
        "\n",
        "        qValueUpdate = qValueUpdate * (1 - done_batch) - done_batch\n",
        "\n",
        "        oneHotMask = tf.one_hot(action_batch, self.action_size)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Train the model on the states and updated Q-values\n",
        "            OqValue = self.model(state_batch)\n",
        "\n",
        "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "            OqValue_action = tf.reduce_sum(tf.multiply(OqValue, oneHotMask), axis=1)\n",
        "\n",
        "            # Calculate loss between new Q-value and old Q-value\n",
        "            loss = self.loss_function(qValueUpdate, OqValue_action)\n",
        "\n",
        "        if DIAGNOSTIC_MODE:\n",
        "          print(\"Q-Values: \", qValueF[1])\n",
        "          print(\"Q-Values Update: \", qValueUpdate[1])\n",
        "          print(\"Masked Update:   \", oneHotMask[1])\n",
        "          print(\"Old Activations: \", OqValue[1])\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "        if DIAGNOSTIC_MODE:\n",
        "          newQ = self.model.predict_step(state_batch)\n",
        "          print(\"New Activations: \", newQ[1], \"\\n\")\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon: \n",
        "            return random.randrange(self.action_size) # Do something stupid! (i.e. take a random action)\n",
        "        action_values = self.model(tf.expand_dims(tf.convert_to_tensor(state), 0), training=False)\n",
        "        return tf.argmax(action_values[0]).numpy()\n",
        "\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw2g6PjZPS9m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70089d35-f32d-447f-d575-48482db154e1"
      },
      "source": [
        "# Make Environment\n",
        "env = gym.make(ENVIRONMENT_NAME)\n",
        "state_size = (105, 80, 3)\n",
        "action_size = env.action_space.n\n",
        "\n",
        "## Intialize Our Agent\n",
        "agent = DQNAgent(state_size, action_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"s2_MAT_Agent_Testing_v1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 104, 80, 3)]      0         \n",
            "_________________________________________________________________\n",
            "s1_MAT_ImageClassifier_v3 (F (None, 11, 8, 64)         39008     \n",
            "_________________________________________________________________\n",
            "s2_MAT_ImageClassifier_v3 (F (None, 512)               1806912   \n",
            "_________________________________________________________________\n",
            "s3_MAT_ImageClassifier_v3 (F (None, 12)                6156      \n",
            "=================================================================\n",
            "Total params: 1,852,076\n",
            "Trainable params: 1,852,076\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z5loBrJiHON"
      },
      "source": [
        "#### Define Training Loop Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8cbu_VJh-E4"
      },
      "source": [
        "def myPlayground():\n",
        "\n",
        "  # Empty Rewards Counter\n",
        "  reward = 0\n",
        "  all_rewards = 0\n",
        "  total_time = 0\n",
        "  done = False\n",
        "  \n",
        "  # Display in Colab\n",
        "  if COLAB:\n",
        "      if RENDER == True:\n",
        "          done, ax = plt.subplots(1, 1)\n",
        "          img = ax.imshow(env.render('rgb_array'))    \n",
        "\n",
        "  # Make Some Starting data\n",
        "  state = tFrame.process_frame(env.reset()) # Reset state for new episode\n",
        "  while len(agent.reward_memory) < STARTING_MEMORY_SIZE:\n",
        "    action = random.randrange(agent.action_size)\n",
        "    next_frame, reward, done, _ = env.step(action) # Agent sends action to env wrapper and gets feedback\n",
        "    next_state = tFrame.process_frame(next_frame)\n",
        "    if reward > 0.0: # Check and modify reward\n",
        "      reward = 1.0\n",
        "    if reward < 0.0:\n",
        "      reward = -1.0\n",
        "    if done:\n",
        "      reward = -1.0\n",
        "    agent.remember(state, action, reward, next_state, done) # Store sequence in replay memory\n",
        "    state = next_state # Update state\n",
        "    if done:\n",
        "      state = tFrame.process_frame(env.reset())\n",
        "\n",
        "\n",
        "  # Training Loop\n",
        "  for e in range(n_episodes): ## Go Eat Cherries\n",
        "      \n",
        "      done = False\n",
        "      time = 0\n",
        "      game_score = 0\n",
        "      reward = 0\n",
        "\n",
        "      state = tFrame.process_frame(env.reset()) # Reset state for new episode\n",
        "      \n",
        "      for skip in range(skip_start): # Skip the start of each game\n",
        "          env.step(0)\n",
        "\n",
        "      while not done:\n",
        "      \n",
        "          time += 1\n",
        "          total_time += 1\n",
        "\n",
        "          # Display\n",
        "          if RENDER:\n",
        "              if COLAB:\n",
        "                  img.set_data(env.render(mode='rgb_array')) \n",
        "                  ax.axis('off')\n",
        "                  ipythondisplay.display(plt.gcf())\n",
        "                  ipythondisplay.clear_output(wait=True)\n",
        "              else:\n",
        "                  env.render()\n",
        "                  if OBSERVATION_MODE:\n",
        "                      sleep(0.02)\n",
        "\n",
        "          # Update Target Network\n",
        "          if total_time % agent.update_rate == 0:\n",
        "              agent.update_target_model()\n",
        "              if CHIPPIE_PROGESS_REPORTS:\n",
        "                chippie.q_update()\n",
        "\n",
        "          # Transition Dynamics\n",
        "          action = agent.act(state) # Get action from agent\n",
        "          next_frame, reward, done, _ = env.step(action) # Agent sends action to env wrapper and gets feedback\n",
        "          next_state = tFrame.process_frame(next_frame)\n",
        "          \n",
        "          # Sternly Validate Reward\n",
        "          if reward > 0.0: \n",
        "            reward = 1.0\n",
        "            game_score += reward\n",
        "          if reward < 0.0:\n",
        "            reward = -1.0\n",
        "          if done:\n",
        "            reward = -1.0\n",
        "            all_rewards += game_score\n",
        "            if CHIPPIE_PROGESS_REPORTS:\n",
        "              chippie.dead(score=game_score, totalScore=all_rewards, episode=e, completion_target=n_episodes, survived=time, \n",
        "                           experiance=total_time, memory=len(agent.reward_memory), epsilon=agent.epsilon)\n",
        "            agent.remember(state, action, reward, next_state, done) # Store death in replay memory\n",
        "            break\n",
        "\n",
        "          agent.remember(state, action, reward, next_state, done) # Store sequence in replay memory\n",
        "          state = next_state # Update state\n",
        "\n",
        "          if len(agent.reward_memory) > STARTING_MEMORY_SIZE:\n",
        "            if not OBSERVATION_MODE:\n",
        "              if total_time % action_steps == 0:\n",
        "                agent.train(BATCH_SIZE)\n",
        "                if CHIPPIE_PROGESS_REPORTS:\n",
        "                  chippie.training(game_score)\n",
        "\n",
        "      if e % CHECKPOINT_FREQUENCY == 0 and len(agent.reward_memory) > STARTING_MEMORY_SIZE:\n",
        "        if SLICE_ONE_CHECKPOINT:\n",
        "            saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + SLICE_ONE_NAME + '{:08d}'.format(e) \n",
        "            tf.saved_model.save(agent.sliceOne, saved_model_path)\n",
        "\n",
        "        if SLICE_TWO_CHECKPOINT:\n",
        "            saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + SLICE_TWO_NAME + '{:08d}'.format(e) \n",
        "            tf.saved_model.save(agent.sliceTwo, saved_model_path)\n",
        "        \n",
        "        if SLICE_THREE_CHECKPOINT:\n",
        "            saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + SLICE_TWO_NAME + '{:08d}'.format(e) \n",
        "            tf.saved_model.save(agent.sliceTwo, saved_model_path)\n",
        "\n",
        "        if FULL_MODEL_CHECKPOINT:\n",
        "            saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + FULL_MODEL_NAME + '{:08d}'.format(e) \n",
        "            tf.saved_model.save(agent.model, saved_model_path)\n",
        "\n",
        "        if SAVE_AGENT:\n",
        "            agent.save(model_dir+ \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + FULL_MODEL_NAME + \"agent_weights_\" + '{:08d}'.format(e) +\".hdf5\")\n",
        "      print(\"\")\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgI4QKIOo6Nl"
      },
      "source": [
        "#### Interactive Playground (runs automatically)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i9UWy2w5nUs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "outputId": "7d68ba78-a01a-486d-fe4a-6e333ccc22cb"
      },
      "source": [
        "#### Run From Bottom (Can be run with one click)\n",
        "## # agent.epsilon = 0.999\n",
        "myPlayground()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-107b57a8989b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### Run From Bottom (Can be run with one click)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## # agent.epsilon = 0.999\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmyPlayground\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-07e3943ca148>\u001b[0m in \u001b[0;36mmyPlayground\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mOBSERVATION_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mtotal_time\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0maction_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mCHIPPIE_PROGESS_REPORTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                   \u001b[0mchippie\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f2aa8836ae47>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;31m# Apply the masks to the Q-values to get the Q-value for action taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mOqValue_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOqValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moneHotMask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;31m# Calculate loss between new Q-value and old Q-value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    516\u001b[0m   \"\"\"\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6066\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6067\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6068\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6069\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6070\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6860\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6861\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6862\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6863\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [32,12] vs. [32,4] [Op:Mul]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8r9t3hNQCf3"
      },
      "source": [
        "#### Notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ftcF2RH6vtP"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "        # self.model, self.sliceTwo = self._build_model() \n",
        "        # self.target_model, self.target_sliceTwo = self._build_model()\n",
        "\n",
        "\n",
        "    # def _build_model(self): # private method\n",
        "    #     # Neural Network Based on DeepMind Paper to Approximate State/Action Q-Values:\n",
        "    #     sliceOneInputs = keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "    #     sliceOne = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)      \n",
        "    #     sliceOneOutputs = sliceOne(sliceOneInputs, training=False)\n",
        "\n",
        "    #     sliceTwoConv_1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),strides=1, activation='relu', \\\n",
        "    #                         kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceOneOutputs)\n",
        "    #     sliceTwoFlatten_1 = tf.keras.layers.Flatten()(sliceTwoConv_1)\n",
        "\n",
        "    #     sliceTwoDense_1 = tf.keras.layers.Dense(512, activation='relu', \\\n",
        "    #                         kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceTwoFlatten_1)\n",
        "\n",
        "    #     ## Linear\n",
        "    #     finalOutput = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "    #                         kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceTwoDense_1)\n",
        "\n",
        "    #     # ## Softmax   #)(sliceTwoDense_1)\n",
        "    #     # finalOutput = Dense(self.action_size, activation='softmax', name='finalOutput', \\\n",
        "    #     #                     kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceTwoDense_1)\n",
        "        \n",
        "    #     # ## Sigmoid\n",
        "    #     # finalOutput = tf.keras.layers.Dense(self.action_size, activation='sigmoid', name='finalOutput')(sliceTwoDense_1)\n",
        "\n",
        "    #     # ## Relu\n",
        "    #     # finalOutput = tf.keras.layers.Dense(self.action_size, activation='relu', name='finalOutput')(sliceTwoDense_1)\n",
        "\n",
        "    #     # ## Leaky Relu\n",
        "    #     # interimOutput = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "    #     #                     kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(sliceTwoDense_1)\n",
        "    #     # finalOutput = tf.keras.layers.LeakyReLU(alpha=0.1)(interimOutput)\n",
        "        \n",
        "    #     sliceTwo = tf.keras.Model(inputs=sliceOneInputs, outputs=sliceTwoDense_1)\n",
        "    #     model = tf.keras.Model(inputs=sliceOneInputs, outputs=finalOutput)\n",
        "\n",
        "    #     sliceOne.trainable = sliceOneTrainable # ref param above\n",
        "    #     sliceTwo.trainable = sliceTwoTrainable # ref param above\n",
        "    #     model.trainable = fullModelTrainable # ref param above\n",
        "\n",
        "    #     self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "\n",
        "    #     # # Using Huber loss for stability\n",
        "    #     self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "\n",
        "    #     return model, sliceTwo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9-wJRd_cHqu"
      },
      "source": [
        "# running reward: 0.38 at episode 293, frame count 10000\n",
        "# running reward: 0.31 at episode 589, frame count 20000\n",
        "# running reward: 0.19 at episode 905, frame count 30000\n",
        "# running reward: 0.23 at episode 1214, frame count 40000\n",
        "# running reward: 0.39 at episode 1512, frame count 50000\n",
        "# running reward: 0.33 at episode 1802, frame count 60000\n",
        "# running reward: 0.32 at episode 2087, frame count 70000\n",
        "# running reward: 0.23 at episode 2390, frame count 80000\n",
        "# running reward: 0.33 at episode 2680, frame count 90000\n",
        "# running reward: 0.17 at episode 2977, frame count 100000\n",
        "# running reward: 0.19 at episode 3274, frame count 110000\n",
        "# running reward: 0.28 at episode 3558, frame count 120000\n",
        "# running reward: 0.38 at episode 3839, frame count 130000\n",
        "# running reward: 0.39 at episode 4110, frame count 140000\n",
        "# running reward: 0.48 at episode 4354, frame count 150000\n",
        "# running reward: 0.38 at episode 4600, frame count 160000\n",
        "# running reward: 0.37 at episode 4858, frame count 170000\n",
        "# running reward: 0.47 at episode 5099, frame count 180000\n",
        "# running reward: 0.47 at episode 5315, frame count 190000\n",
        "# running reward: 0.60 at episode 5529, frame count 200000\n",
        "# running reward: 0.60 at episode 5744, frame count 210000\n",
        "# running reward: 0.63 at episode 5956, frame count 220000\n",
        "# running reward: 0.59 at episode 6181, frame count 230000\n",
        "# running reward: 0.59 at episode 6407, frame count 240000\n",
        "# running reward: 0.53 at episode 6625, frame count 250000\n",
        "# running reward: 0.62 at episode 6842, frame count 260000\n",
        "# running reward: 0.62 at episode 7057, frame count 270000\n",
        "# running reward: 0.50 at episode 7285, frame count 280000\n",
        "# running reward: 0.68 at episode 7505, frame count 290000\n",
        "# running reward: 0.55 at episode 7726, frame count 300000\n",
        "# running reward: 0.65 at episode 7930, frame count 310000\n",
        "# running reward: 0.70 at episode 8136, frame count 320000\n",
        "# running reward: 0.86 at episode 8321, frame count 330000\n",
        "# running reward: 0.57 at episode 8535, frame count 340000\n",
        "# running reward: 0.69 at episode 8732, frame count 350000\n",
        "# running reward: 0.51 at episode 8953, frame count 360000\n",
        "# running reward: 0.76 at episode 9144, frame count 370000\n",
        "# running reward: 0.72 at episode 9357, frame count 380000\n",
        "# running reward: 0.86 at episode 9550, frame count 390000\n",
        "# running reward: 0.63 at episode 9750, frame count 400000\n",
        "# running reward: 0.55 at episode 9989, frame count 410000\n",
        "# running reward: 0.88 at episode 10175, frame count 420000\n",
        "# running reward: 0.68 at episode 10371, frame count 430000\n",
        "# running reward: 0.78 at episode 10569, frame count 440000\n",
        "# running reward: 0.83 at episode 10736, frame count 450000\n",
        "# running reward: 1.00 at episode 10900, frame count 460000\n",
        "# running reward: 0.82 at episode 11090, frame count 470000\n",
        "# running reward: 1.11 at episode 11237, frame count 480000\n",
        "# running reward: 1.11 at episode 11393, frame count 490000\n",
        "# running reward: 1.16 at episode 11543, frame count 500000\n",
        "# running reward: 1.38 at episode 11686, frame count 510000\n",
        "# running reward: 1.23 at episode 11833, frame count 520000\n",
        "# running reward: 1.31 at episode 11976, frame count 530000\n",
        "# running reward: 1.32 at episode 12122, frame count 540000\n",
        "# running reward: 1.59 at episode 12246, frame count 550000\n",
        "# running reward: 1.65 at episode 12371, frame count 560000\n",
        "# running reward: 1.53 at episode 12500, frame count 570000\n",
        "# running reward: 1.44 at episode 12643, frame count 580000\n",
        "# running reward: 1.54 at episode 12776, frame count 590000\n",
        "# running reward: 1.61 at episode 12900, frame count 600000\n",
        "# running reward: 1.83 at episode 13012, frame count 610000\n",
        "# running reward: 1.98 at episode 13123, frame count 620000\n",
        "# running reward: 2.08 at episode 13230, frame count 630000\n",
        "# running reward: 1.96 at episode 13336, frame count 640000\n",
        "# running reward: 2.01 at episode 13449, frame count 650000\n",
        "# running reward: 2.21 at episode 13548, frame count 660000\n",
        "# running reward: 2.29 at episode 13648, frame count 670000\n",
        "# running reward: 2.35 at episode 13744, frame count 680000\n",
        "# running reward: 2.40 at episode 13837, frame count 690000\n",
        "# running reward: 2.29 at episode 13934, frame count 700000\n",
        "# running reward: 2.54 at episode 14026, frame count 710000\n",
        "# running reward: 2.30 at episode 14121, frame count 720000\n",
        "# running reward: 2.20 at episode 14218, frame count 730000\n",
        "# running reward: 2.79 at episode 14301, frame count 740000\n",
        "# running reward: 2.73 at episode 14387, frame count 750000\n",
        "# running reward: 2.93 at episode 14465, frame count 760000\n",
        "# running reward: 2.72 at episode 14553, frame count 770000\n",
        "# running reward: 2.57 at episode 14640, frame count 780000\n",
        "# running reward: 2.60 at episode 14719, frame count 790000\n",
        "# running reward: 2.79 at episode 14797, frame count 800000\n",
        "# running reward: 3.20 at episode 14865, frame count 810000\n",
        "# running reward: 3.27 at episode 14938, frame count 820000\n",
        "# running reward: 3.06 at episode 15006, frame count 830000\n",
        "# running reward: 3.50 at episode 15073, frame count 840000\n",
        "# running reward: 3.63 at episode 15139, frame count 850000\n",
        "# running reward: 3.78 at episode 15203, frame count 860000\n",
        "# running reward: 3.67 at episode 15270, frame count 870000\n",
        "# running reward: 3.69 at episode 15333, frame count 880000\n",
        "# running reward: 4.40 at episode 15384, frame count 890000\n",
        "# running reward: 4.70 at episode 15439, frame count 900000\n",
        "# running reward: 5.28 at episode 15482, frame count 910000\n",
        "# running reward: 5.98 at episode 15525, frame count 920000\n",
        "# running reward: 5.74 at episode 15575, frame count 930000\n",
        "# running reward: 6.22 at episode 15612, frame count 940000\n",
        "# running reward: 5.76 at episode 15662, frame count 950000\n",
        "# running reward: 6.37 at episode 15695, frame count 960000\n",
        "# running reward: 7.02 at episode 15736, frame count 970000\n",
        "# running reward: 7.24 at episode 15777, frame count 980000\n",
        "# running reward: 7.35 at episode 15816, frame count 990000\n",
        "# running reward: 7.41 at episode 15852, frame count 1000000\n",
        "# running reward: 8.06 at episode 15889, frame count 1010000\n",
        "# running reward: 8.18 at episode 15928, frame count 1020000\n",
        "# running reward: 8.00 at episode 15965, frame count 1030000\n",
        "# running reward: 7.38 at episode 16008, frame count 1040000\n",
        "# running reward: 7.24 at episode 16048, frame count 1050000\n",
        "# running reward: 6.93 at episode 16091, frame count 1060000\n",
        "# running reward: 7.73 at episode 16125, frame count 1070000\n",
        "# running reward: 7.16 at episode 16172, frame count 1080000\n",
        "# running reward: 7.08 at episode 16210, frame count 1090000\n",
        "# running reward: 7.20 at episode 16246, frame count 1100000\n",
        "# running reward: 7.89 at episode 16285, frame count 1110000\n",
        "# running reward: 7.83 at episode 16321, frame count 1120000\n",
        "# running reward: 7.19 at episode 16366, frame count 1130000\n",
        "# running reward: 7.99 at episode 16399, frame count 1140000\n",
        "# running reward: 7.90 at episode 16440, frame count 1150000\n",
        "# running reward: 8.47 at episode 16474, frame count 1160000\n",
        "# running reward: 8.40 at episode 16511, frame count 1170000\n",
        "# running reward: 7.72 at episode 16555, frame count 1180000\n",
        "# running reward: 8.15 at episode 16591, frame count 1190000\n",
        "# running reward: 8.84 at episode 16628, frame count 1200000\n",
        "# running reward: 8.93 at episode 16662, frame count 1210000\n",
        "# running reward: 9.66 at episode 16695, frame count 1220000\n",
        "# running reward: 9.07 at episode 16733, frame count 1230000\n",
        "# running reward: 8.91 at episode 16769, frame count 1240000\n",
        "# running reward: 8.84 at episode 16803, frame count 1250000\n",
        "# running reward: 9.18 at episode 16838, frame count 1260000\n",
        "# running reward: 8.79 at episode 16875, frame count 1270000\n",
        "# running reward: 9.07 at episode 16908, frame count 1280000\n",
        "# running reward: 9.27 at episode 16943, frame count 1290000\n",
        "# running reward: 9.23 at episode 16979, frame count 1300000\n",
        "# running reward: 8.59 at episode 17015, frame count 1310000\n",
        "# running reward: 8.49 at episode 17055, frame count 1320000\n",
        "# running reward: 8.39 at episode 17091, frame count 1330000\n",
        "# running reward: 9.11 at episode 17128, frame count 1340000\n",
        "# running reward: 9.36 at episode 17164, frame count 1350000\n",
        "# running reward: 9.98 at episode 17198, frame count 1360000\n",
        "# running reward: 9.91 at episode 17236, frame count 1370000\n",
        "# running reward: 10.31 at episode 17272, frame count 1380000\n",
        "# running reward: 10.09 at episode 17305, frame count 1390000\n",
        "# running reward: 10.39 at episode 17337, frame count 1400000\n",
        "# running reward: 9.91 at episode 17371, frame count 1410000\n",
        "# running reward: 10.86 at episode 17402, frame count 1420000\n",
        "# running reward: 10.22 at episode 17438, frame count 1430000\n",
        "# running reward: 10.34 at episode 17474, frame count 1440000\n",
        "# running reward: 10.38 at episode 17505, frame count 1450000\n",
        "# running reward: 10.91 at episode 17538, frame count 1460000\n",
        "# running reward: 10.25 at episode 17576, frame count 1470000\n",
        "# running reward: 9.19 at episode 17613, frame count 1480000\n",
        "# running reward: 9.26 at episode 17652, frame count 1490000\n",
        "# running reward: 9.89 at episode 17685, frame count 1500000\n",
        "# running reward: 9.43 at episode 17724, frame count 1510000\n",
        "# running reward: 9.27 at episode 17758, frame count 1520000\n",
        "# running reward: 9.21 at episode 17791, frame count 1530000\n",
        "# running reward: 9.28 at episode 17831, frame count 1540000\n",
        "# running reward: 9.60 at episode 17866, frame count 1550000\n",
        "# running reward: 9.39 at episode 17902, frame count 1560000\n",
        "# running reward: 10.39 at episode 17937, frame count 1570000\n",
        "# running reward: 9.72 at episode 17975, frame count 1580000\n",
        "# running reward: 10.11 at episode 18005, frame count 1590000\n",
        "# running reward: 9.97 at episode 18040, frame count 1600000\n",
        "# running reward: 10.07 at episode 18077, frame count 1610000\n",
        "# running reward: 9.36 at episode 18114, frame count 1620000\n",
        "# running reward: 9.33 at episode 18151, frame count 1630000\n",
        "# running reward: 8.81 at episode 18191, frame count 1640000\n",
        "# running reward: 7.91 at episode 18230, frame count 1650000\n",
        "# running reward: 8.76 at episode 18264, frame count 1660000\n",
        "# running reward: 9.51 at episode 18304, frame count 1670000\n",
        "# running reward: 8.86 at episode 18346, frame count 1680000\n",
        "# running reward: 8.52 at episode 18383, frame count 1690000\n",
        "# running reward: 8.88 at episode 18416, frame count 1700000\n",
        "# running reward: 9.83 at episode 18451, frame count 1710000\n",
        "# running reward: 9.74 at episode 18487, frame count 1720000\n",
        "# running reward: 10.33 at episode 18520, frame count 1730000\n",
        "# running reward: 10.37 at episode 18551, frame count 1740000\n",
        "# running reward: 10.67 at episode 18590, frame count 1750000\n",
        "# running reward: 9.78 at episode 18624, frame count 1760000\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}