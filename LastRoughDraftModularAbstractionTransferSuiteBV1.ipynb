{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "OfficialDraft1ModularAbstractionTransferSuiteBV1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ynM5BeDfWQvH",
        "BGIl3BKliih1",
        "HNp06xZqiWKD",
        "tdX2lI15iCvS",
        "0vtDg55hojmL",
        "rP9pyqvrmi6o",
        "1F5PijpNi9Wg",
        "sYyAtezpCwYg",
        "8z5loBrJiHON",
        "4ZKKOtCtCcA7",
        "3CVQf5Yt-t5z"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXiHbkQgfw9a"
      },
      "source": [
        "# Modular Abstraction Transfer Suite\n",
        "\n",
        "### A Deep Mind Based Experimental Platform For Reasearch In Abstaction And Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIsSBMAUbeZ2"
      },
      "source": [
        "We set out to assemble a team of Algorithms to fight the evil of an unseen game. Unfortunately, this task became nearly insurmountable. First iterations of the code were based implementations using a deque for replay memory, and using a single instance training loop. These systems did show promise in early experiments but were so inefficient that it would have likely taken months of training to reach a respectable level let alone expert level game play.\n",
        "\n",
        "The solution it seems is multi-part, so we will spread out the explanations here.\n",
        "\n",
        "|\n",
        "Great References:\n",
        "\n",
        "https://towardsdatascience.com/tutorial-double-deep-q-learning-with-dueling-network-architectures-4c1b3fb7f756\n",
        "\n",
        "https://keras.io/examples/rl/deep_q_network_breakout/\n",
        "\n",
        "https://stackoverflow.com/questions/15455048/releasing-memory-in-python\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynM5BeDfWQvH"
      },
      "source": [
        "#### Interactive Playgound (run after notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrFuoAxKQclk"
      },
      "source": [
        "# # # Run From Top (Hint: It must be loaded commented first)\n",
        "# myPlayground()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1EFy_NoE7Wl"
      },
      "source": [
        "#### Selector Menu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-9mrlfuSA23",
        "outputId": "c3f0bea7-90e2-474b-d14b-1fff5e631105"
      },
      "source": [
        "##   Game Selector\n",
        "ENVIRONMENT_NAME = 'BreakoutNoFrameskip-v4' #@param ['Atlantis-v0', 'DemonAttack-v0', 'Phoenix-v0', 'Riverraid-v0', 'Solaris-v0', 'Asterix-v0', 'Breakout-v0', 'Boxing-v0', 'Pong-v0', 'BattleZone-v0', 'SpaceInvaders-v0', 'BeamRider-v0','AtlantisNoFrameskip-v4', 'DemonAttackNoFrameskip-v4', 'PhoenixNoFrameskip-v4', 'RiverraidNoFrameskip-v4', 'SolarisNoFrameskip-v4', 'AsterixNoFrameskip-v4', 'BreakoutNoFrameskip-v4', 'BoxingNoFrameskip-v4', 'PongNoFrameskip-v4', 'BattleZoneNoFrameskip-v4', 'SpaceInvadersNoFrameskip-v4', 'BeamRiderNoFrameskip-v4']\n",
        "\n",
        "# Render gameplay in cell or viewer\n",
        "RENDER = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Slow down game play and discontinue training for observation\n",
        "OBSERVATION_MODE = False #@param {type:\"boolean\"}\n",
        "\n",
        "# Display Text Output to Observe Activations and Q Updates\n",
        "DIAGNOSTIC_MODE = False #@param {type:\"boolean\"}\n",
        "\n",
        "## Chippie the Progress Bot Settings       (Name courtesy of: Kylie Locker)\n",
        "CHIPPIE_PROGESS_REPORTS = True #@param {type:\"boolean\"}\n",
        "# Row for progress bot stacking ( 1, 8, 12 )\n",
        "MAX_CHIPPIES_ROW = 8 #@param {type:\"integer\"}\n",
        "\n",
        "# Provide GPU information\n",
        "USE_GPU_SUPPORT = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "## Data Settings\n",
        "# Import and Unzip Hosted Dataset\n",
        "CREATE_MISSING_DIRECTORIES = True #@param {type:\"boolean\"}\n",
        "IMPORT_MAIN_DATA = True #@param {type:\"boolean\"}\n",
        "IMPORT_SENTIMENT_DATA = True #@param {type:\"boolean\"}\n",
        "IMPORT_MODEL_DATA = True #@param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "## Agent Settings\n",
        "# Parameters\n",
        "n_episodes = 50000 #@param {type:\"integer\"}\n",
        "BATCH_SIZE =  32#@param {type:\"integer\"}\n",
        "action_steps =  4#@param {type:\"integer\"}\n",
        "skip_start = 1 #@param {type:\"integer\"}\n",
        "agent_gamma = 0.985 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "agent_epsilon = 1.0 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "agent_epsilon_decay = 0.9999 #@param {type:\"slider\", min:0.5, max:1.000, step:0.0001}\n",
        "agent_epsilon_min = 0.1 #@param {type:\"slider\", min:0, max:1.000, step:0.0001}\n",
        "agent_learning_rate = 0.00025 #@param {type:\"number\", min:0.0000, max:0.0200, step:0.00001}\n",
        "# lr=0.00042 # lr=0.00125 #lr=0.00025\n",
        "# agent_learning_rate = 0.00025 # works??\n",
        "## Buffer Settings\n",
        "STARTING_MEMORY_SIZE = 75000#@param {type:\"integer\"}\n",
        "MAX_MEMORY_LENGTH = 100000 #@param {type:\"integer\"}\n",
        "Q_UPDATE_FREQUENCY = 10000 #@param {type:\"integer\"}\n",
        "\n",
        "# Pre Buffer Guidance\n",
        "WINDOW_SIZE = (84, 84)\n",
        "\n",
        "# Sub-Model Settings\n",
        "SLICE_ONE_NAME = \"s1_MAT_ImageClassifier_v5\" #@param {type:\"string\"}\n",
        "SLICE_ONE_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "SLICE_ONE_CHECKPOINT = True #@param {type:\"boolean\"}\n",
        "\n",
        "SLICE_TWO_NAME = \"s2_MAT_ImageClassifier_v5\" #@param {type:\"string\"}\n",
        "SLICE_TWO_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "SLICE_TWO_CHECKPOINT = True #@param {type:\"boolean\"}\n",
        "\n",
        "SLICE_THREE_NAME = \"s3_MAT_ImageClassifier_v2\" #@param {type:\"string\"}\n",
        "SLICE_THREE_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "SLICE_THREE_CHECKPOINT = True #@param {type:\"boolean\"}\n",
        "\n",
        "FULL_MODEL_NAME = \"s2_MAT_Agent_Testing_v2\" #@param {type:\"string\"}\n",
        "FULL_MODEL_TRAINABLE = True #@param {type:\"boolean\"}\n",
        "FULL_MODEL_CHECKPOINT = True #@param {type:\"boolean\"}\n",
        "\n",
        "CHECKPOINT_FREQUENCY =  1000#@param {type:\"integer\"}\n",
        "\n",
        "SAVE_AGENT = True #@param {type:\"boolean\"}\n",
        "\n",
        "## Game selector feedback\n",
        "print(\"Selected Game: \" + ENVIRONMENT_NAME)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected Game: BreakoutNoFrameskip-v4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGIl3BKliih1"
      },
      "source": [
        "#### Setup Model Output Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFzeNuWrURPP"
      },
      "source": [
        "%%capture\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  !pip install baselines\n",
        "  COLAB = True\n",
        "  \n",
        "except:\n",
        "  !pip install gdown\n",
        "  COLAB = False\n",
        "\n",
        "model_dir = 'ISAR_Model_Data'\n",
        "data_dir = 'ISAR_Main_Classification'\n",
        "transfer_dir = 'ISAR_Sentiment_Transfer'\n",
        "\n",
        "\n",
        "if IMPORT_MAIN_DATA :\n",
        "    if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1P7o1x4ZpPbd16VQDwaMzllbN-tlfqqIH\n",
        "      !unzip ISAR_Main_Classification.zip\n",
        "      !rm ISAR_Main_Classification.zip\n",
        "if IMPORT_SENTIMENT_DATA:\n",
        "    if not os.path.exists(transfer_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1UDUNnw04q5cvms5ibM6pNn-wXtJphXxZ\n",
        "      !unzip ISAR_Sentiment_Transfer.zip\n",
        "      !rm ISAR_Sentiment_Transfer.zip\n",
        "if IMPORT_MODEL_DATA:\n",
        "    if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "      !gdown https://drive.google.com/uc?id=1DIc_J6XyKzNDSMjSbGkt3vzQep4YUEYU\n",
        "      !unzip ISAR_Model_Data.zip\n",
        "      !rm ISAR_Model_Data.zip\n",
        "      "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poXz23gKsczB",
        "outputId": "05819947-626e-494a-dd24-9278a9293366"
      },
      "source": [
        "if CREATE_MISSING_DIRECTORIES:  \n",
        "  if not os.path.exists(data_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Main Data Directory\")\n",
        "    os.makedirs(data_dir)\n",
        "  else:\n",
        "    print(\"Main Data Directory Found\")\n",
        "  if not os.path.exists(transfer_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Transfer Data Directory\")\n",
        "    os.makedirs(transfer_dir)\n",
        "  else:\n",
        "    print(\"Transfer Data Directory Found\")\n",
        "  if not os.path.exists(model_dir): ## Make it if it doesn't exist\n",
        "    print(\"Creating Model Directory\")\n",
        "    os.makedirs(model_dir)\n",
        "  else:\n",
        "    print(\"Model Directory Found\")\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Main Data Directory Found\n",
            "Transfer Data Directory Found\n",
            "Model Directory Found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNp06xZqiWKD"
      },
      "source": [
        "#### Setup Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjxfLJqifsa5",
        "outputId": "9964e9e4-dde7-45ab-b7d5-12bbe44258df"
      },
      "source": [
        "## Begin by importing . . .  oh . . . everything!\n",
        "try:\n",
        "  import math\n",
        "  import random\n",
        "  import numpy as np\n",
        "\n",
        "  import glob\n",
        "  import io\n",
        "  import base64\n",
        "  from time import sleep\n",
        "\n",
        "  from collections import deque\n",
        "\n",
        "  import gym\n",
        "  import tensorflow as tf\n",
        "  import tensorflow_hub as hub\n",
        "  from tensorflow.keras.models import Sequential\n",
        "  from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
        "  from tensorflow.keras.optimizers import Nadam, Adam\n",
        "  from tensorflow import keras\n",
        "\n",
        "  import matplotlib\n",
        "  import matplotlib.pyplot as plt\n",
        "  %matplotlib inline\n",
        "\n",
        "  from IPython.display import HTML\n",
        "  from IPython import display as ipythondisplay\n",
        "  from IPython.display import clear_output\n",
        "\n",
        "except:\n",
        "  %%capture\n",
        "  if COLAB:\n",
        "    ## For colab we must install some dependancies\n",
        "    !apt-get install -y xvfb x11-utils\n",
        "    ## Next we will need to install a virtual display and correct Open AI installation\n",
        "    !pip install gym[all]==0.17.3\n",
        "    !pip install pyvirtualdisplay==0.2.* \n",
        "    !pip install PyOpenGL==3.1.* \n",
        "    !pip install PyOpenGL-accelerate==3.1.*\n",
        "    !pip install pyglet\n",
        "    # So let's setup the virtual display\n",
        "    import pyvirtualdisplay\n",
        "    # use False with Xvfb\n",
        "    _display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "    _ = _display.start()\n",
        "    # Now Check the Display\n",
        "    !echo $DISPLAY\n",
        "\n",
        "if USE_GPU_SUPPORT:\n",
        "  print(\"TF version:\", tf.__version__)\n",
        "  print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "\n",
        "  # Set Memory Growth\n",
        "  gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "  if gpus:\n",
        "    try:\n",
        "      # Currently, memory growth needs to be the same across GPUs\n",
        "      for gpu in gpus:\n",
        "        tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(gpu)\n",
        "      logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "      print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
        "    except RuntimeError as e:\n",
        "      # Memory growth must be set before GPUs have been initialized\n",
        "      print(e)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TF version: 2.4.1\n",
            "Num GPUs Available:  1\n",
            "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
            "1 Physical GPUs, 1 Logical GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdX2lI15iCvS"
      },
      "source": [
        "#### Setup Frame PreProcessor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uR4qY7HhnhNq"
      },
      "source": [
        "After managing to make batches acceptable to the TensorFlow's .fit() method, we noticed a massive increase in training speed. The victory was short lived, however, as memory usage skyrocket outside the limits of both colaboratory and one local workstation (128GB). So, both the deque and image pre-processors had to be re-examined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT-35yCogEXo"
      },
      "source": [
        "### Custom Pre-Processor\n",
        "# Stack frames and average on axis=-1 to produce a Single 210, 160, 1 greyscale image #old crop = img = frame[1:176:2, ::2]\n",
        "# Rescale to 104, 80, 1 then stack in three's (3) to produce a single RGB compatible representation of evironmental space-time\n",
        "class TFramePreBuffer:\n",
        "\n",
        "    def __init__(self, t_size=3, setting=0, scaling=1):\n",
        "        self.un_flicker_memory = []\n",
        "        self.temporal_memory = []\n",
        "        self.temporal_size = t_size\n",
        "        self.processor_setting = setting\n",
        "        self.scale = scaling\n",
        "\n",
        "\n",
        "    def process_frame(self, frame):\n",
        "        if self.scale == 0:\n",
        "            if self.processor_setting == 0:\n",
        "                return np.array(frame[1:209:2, ::2]).astype(np.uint8)*(1/255.0)\n",
        "\n",
        "            if self.processor_setting == 1:\n",
        "                if len(self.temporal_memory) >= self.temporal_size:\n",
        "                      del self.temporal_memory[:1]\n",
        "                while len(self.temporal_memory) < self.temporal_size:\n",
        "                    self.temporal_memory.append(frame[1:209:2, ::2].mean(axis=2,keepdims=True).astype(np.uint8)) \n",
        "                temporal_image = np.concatenate((self.temporal_memory[0],\n",
        "                                                  self.temporal_memory[1],\n",
        "                                                  self.temporal_memory[2]), \n",
        "                                                axis=-1)*(1/255.0)\n",
        "                \n",
        "                return temporal_image\n",
        "\n",
        "            if self.processor_setting == 2:\n",
        "                if len(self.temporal_memory) >= self.temporal_size:\n",
        "                    del self.temporal_memory[:1]\n",
        "                while len(self.un_flicker_memory) < 2:\n",
        "                    self.un_flicker_memory.append(frame[1:209:2, ::2].astype(np.uint8))\n",
        "                while len(self.temporal_memory) < self.temporal_size:\n",
        "                    self.temporal_memory.append(np.concatenate((self.un_flicker_memory[0].mean(axis=2,keepdims=True),\n",
        "                                                                self.un_flicker_memory[1].mean(axis=2,keepdims=True)),\n",
        "                                                                axis=-1).max(axis=2))\n",
        "                temporal_image = np.concatenate(np.expand_dims((self.temporal_memory[0], \n",
        "                                                                self.temporal_memory[1], \n",
        "                                                                self.temporal_memory[2]), \n",
        "                                                              axis=-1), axis=-1)*(1/255.0)\n",
        "\n",
        "                return temporal_image\n",
        "\n",
        "\n",
        "        if self.scale == 1:\n",
        "            if self.processor_setting == 0:\n",
        "                return (np.array(frame[1:209:2, ::2]).astype(np.uint8)-128)*(1/128.0)\n",
        "\n",
        "            if self.processor_setting == 1:\n",
        "                if len(self.temporal_memory) >= self.temporal_size:\n",
        "                    del self.temporal_memory[:1]\n",
        "                    # self.temporal_memory.pop(1)\n",
        "                while len(self.temporal_memory) < self.temporal_size:\n",
        "                    self.temporal_memory.append(frame[1:209:2, ::2].mean(axis=2,keepdims=True).astype(np.uint8)) \n",
        "                temporal_image = (np.concatenate((self.temporal_memory[0],\n",
        "                                                  self.temporal_memory[1],\n",
        "                                                  self.temporal_memory[2]), \n",
        "                                                axis=-1)-128)*(1/128.0)\n",
        "                return temporal_image\n",
        "\n",
        "            if self.processor_setting == 2:\n",
        "                if len(self.temporal_memory) >= self.temporal_size:\n",
        "                    del self.temporal_memory[:1]\n",
        "                while len(self.un_flicker_memory) < 2:\n",
        "                    self.un_flicker_memory.append(frame[1:209:2, ::2].astype(np.uint8))\n",
        "                while len(self.temporal_memory) < self.temporal_size:\n",
        "                    self.temporal_memory.append(np.concatenate((self.un_flicker_memory[0].mean(axis=2,keepdims=True),\n",
        "                                                                self.un_flicker_memory[1].mean(axis=2,keepdims=True)),\n",
        "                                                                axis=-1).max(axis=2))\n",
        "                temporal_image = (np.concatenate(np.expand_dims((self.temporal_memory[0], \n",
        "                                                                self.temporal_memory[1], \n",
        "                                                                self.temporal_memory[2]), \n",
        "                                                              axis=-1), axis=-1)-128)/128\n",
        "                return temporal_image\n",
        "\n",
        "\n",
        "#### Instantiate and call with\n",
        "# tFrame.process_frame(frame)\n",
        "\n",
        "## Resize Only\n",
        "# tFrame = TFramePreBuffer(t_size=3, setting=0, scaling=1)\n",
        "\n",
        "## Resize Grayscale and Stack Temporally\n",
        "tFrame = TFramePreBuffer(t_size=3, setting=1, scaling=1)\n",
        "\n",
        "# ## Resize Grayscale De-Flicker and Stack Temporally\n",
        "# tFrame = TFramePreBuffer(t_size=3, setting=2, scaling=1)\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3za4h0s9nziQ"
      },
      "source": [
        "Implementing the image pre-processor buffer using numpy arrays saved a few extra operations so I am confident this helped alot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vtDg55hojmL"
      },
      "source": [
        "#### Setup Progress Bot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyqPA-frPMS1"
      },
      "source": [
        "# import gc\n",
        "import sys\n",
        "\n",
        "class ChippieProgressBot:\n",
        "\n",
        "    def __init__(self, window_size=100, row_configuration=8, log_size=2000):\n",
        "        self.rowCount = 0\n",
        "        self.miniScore = 0\n",
        "        self.movingAverage = []\n",
        "        self.windowSize = window_size\n",
        "        self.facelist1 = [\"~{0_0}~\",\"~{o_o}~\",\"~{o_0}~\",\"~{0_o}~\"]\n",
        "        self.row_configuration = row_configuration\n",
        "        self.lossLog = []\n",
        "        self.logSize = log_size\n",
        "        self.lessChippies = True\n",
        "        self.lessesChippies = 250\n",
        "        self.chippieCount = 0\n",
        "\n",
        "    def training(self, score):\n",
        "      if not self.lessChippies:\n",
        "        if self.miniScore < score:\n",
        "            self.miniScore += 1\n",
        "            print(self.facelist1[int(self.miniScore % 4)] + str(int(score))+\"   \", end='')\n",
        "            self.rowCount += 1\n",
        "            if self.rowCount >= self.row_configuration:\n",
        "                print(\"\\n\")\n",
        "                self.rowCount = 0\n",
        "        else:\n",
        "           return\n",
        "      else:\n",
        "        return\n",
        "\n",
        "\n",
        "    def q_update(self):\n",
        "      if not self.lessChippies:\n",
        "        print(\"\"\"  {-_-}    \"\"\", end='')\n",
        "        self.rowCount += 1\n",
        "        if self.rowCount >= MAX_CHIPPIES_ROW:\n",
        "            print(\"\\n\")\n",
        "            self.rowCount = 0\n",
        "      else:\n",
        "        return\n",
        "\n",
        "    def dead(self, score, totalScore, episode, completion_target,  survived, experiance, memory, epsilon):\n",
        "      \n",
        "        if len(self.movingAverage) >= self.windowSize:\n",
        "          del self.movingAverage[:1]\n",
        "        self.movingAverage.append(score)\n",
        "        if len(self.lossLog) > 2:\n",
        "          trailing = sum(self.lossLog[:len(self.lossLog)//2])/(len(self.lossLog)*0.5)\n",
        "\n",
        "          leading = sum(self.lossLog[len(self.lossLog)//2:])/(len(self.lossLog)*0.5)\n",
        "\n",
        "        self.rowCount = 0\n",
        "        self.miniScore = 0\n",
        "        if not self.lessChippies:\n",
        "\n",
        "          # running_reward = np.mean(episode_reward_history)\n",
        "          print(\"\"\" `{x_X}~   \"\"\"+\"\\n\")\n",
        "\n",
        "          print(\"Episode: {}/{}, Episode Score: {}, Avg Episode Score: {:.4}, Survival Time: {}\"\n",
        "          .format(episode+1, completion_target, score, totalScore/(episode+1), survived)+\"\\n\")\n",
        "\n",
        "          print(\"Your agent has a running average Score per 100 episodes of: \\{^,^}~\"+\"{:.3}\"\n",
        "          .format(sum(self.movingAverage)/len(self.movingAverage))+\"\\n\")\n",
        "\n",
        "          print(\"Total Steps: {}, Memory Size: {}, Current Epsilon Value: {:.2}, Leading Loss: {:.4}/ Trailing Loss: {:.4}\"\n",
        "          .format(experiance, memory, epsilon, leading, trailing)+\"\\n\\n\")\n",
        "\n",
        "        \n",
        "        else:\n",
        "          if (episode+1) % self.lessesChippies == 0:\n",
        "\n",
        "            print(\"\\n\"+\"Episode: {}/{}, Episode Score: {}, Avg Episode Score: {:.4}, Survival Time: {}\"\n",
        "            .format(episode+1, completion_target, score, totalScore/(episode+1), survived))\n",
        "\n",
        "            print(\"Your agent has a running average Score per 100 episodes of: \\{^,^}~\"+\"{:.3}\"\n",
        "            .format(sum(self.movingAverage)/len(self.movingAverage)))\n",
        "\n",
        "            print(\"Total Steps: {}, Memory Size: {}, Current Epsilon Value: {:.2}, Leading Loss: {:.4}/ Trailing Loss: {:.4}\"\n",
        "            .format(experiance, memory, epsilon, leading, trailing)) \n",
        "          else:\n",
        "            # self.chippieCount += 1\n",
        "            return\n",
        "\n",
        "        # gc.collect()\n",
        "        # print(sys.getrefcount(tFrame.temporal_memory))\n",
        "        # print(sys.getrefcount(agent.done_memory[0]))\n",
        "\n",
        "    def logLoss(self, loss):\n",
        "        if len(self.lossLog) >= self.logSize:\n",
        "            del self.lossLog[:1]\n",
        "        self.lossLog.append(loss)\n",
        "        # trailing = sum(self.lossLog[:len(self.lossLog)//2])/(len(self.lossLog)*0.5)\n",
        "        # print(leading)\n",
        "        # leading = sum(self.lossLog[len(self.lossLog)//2:])/(len(self.lossLog)*0.5)\n",
        "        # print(trailing)\n",
        "\n",
        "# chippie.training(score)\n",
        "chippie = ChippieProgressBot(window_size=100, row_configuration=MAX_CHIPPIES_ROW)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rP9pyqvrmi6o"
      },
      "source": [
        "#### New Replay Memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxip_vgCmhJh"
      },
      "source": [
        "class ReplayMemory:\n",
        "    def __init__(self, max_size):\n",
        "        self.buffer = np.empty(max_size, dtype=np.object)\n",
        "        self.max_size = max_size\n",
        "        self.index = 0\n",
        "        self.size = 0\n",
        "\n",
        "    def append(self, obj):\n",
        "        self.buffer[self.index] = obj\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        self.index = (self.index + 1) % self.max_size\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.randint(self.size, size=batch_size)\n",
        "        return self.buffer[indices]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F5PijpNi9Wg"
      },
      "source": [
        "#### Define the agent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn6IwclooJrE"
      },
      "source": [
        "In the search for optimization, we found a few pointers to use variance scaling in layer declaration. I include the links for a deeper dive, however essentially this means that the initiallization of weights and biases are resricted to a range with a standard deviation of <= 2.0. This makes sense since the Q-values we are hoping for will yield need to be close to each other to promote further explorations.\n",
        "\n",
        "\n",
        "After, careful examination of a much faster implementation attempting to solve the traditional 4-frame 84x84 version was very helpful. So, we took time to implement the network using GradientTape(), which made the code run about twice as fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2Nw-i27h9nR"
      },
      "source": [
        "class DQNAgent:\n",
        "\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        \n",
        "        self.state_memory = []\n",
        "        self.action_memory = []\n",
        "        self.reward_memory = []\n",
        "        self.next_state_memory = []\n",
        "        self.done_memory = []\n",
        "        self.replay_memory = ReplayMemory(max_size=100000)\n",
        "        # self.replay_memory = deque(max_size=100000)\n",
        "        \n",
        "        self.update_rate = Q_UPDATE_FREQUENCY # Number of steps until updating the target network\n",
        "\n",
        "        self.gamma = agent_gamma # decay or discount rate\n",
        "\n",
        "        self.epsilon = agent_epsilon # exploration rate\n",
        "        self.epsilon_decay = agent_epsilon_decay # exploration decay\n",
        "        self.epsilon_min = agent_epsilon_min # min exploration\n",
        "\n",
        "        self.learning_rate = agent_learning_rate # SGD or Nadam rate param\n",
        "\n",
        "        # self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_new_model()\n",
        "        # self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_new_model()\n",
        "\n",
        "        # self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_1S_transfer_model()\n",
        "        # self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_1S_transfer_model()\n",
        "\n",
        "        self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_2S_transfer_model()\n",
        "        self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_2S_transfer_model()\n",
        "\n",
        "        # self.model, self.sliceOne, self.sliceTwo, self.sliceThree = self._build_3S_transfer_model()\n",
        "        # self.target_model, self.target_sliceOne, self.target_sliceTwo, self.target_sliceThree = self._build_3S_transfer_model()\n",
        "\n",
        "        self.target_model.set_weights(self.model.get_weights()) # create Q-target network\n",
        "        \n",
        "        self.model.summary()\n",
        "\n",
        "  \n",
        "    def _build_new_model(self): # private method\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_conv1 = tf.keras.layers.Conv2D(32, kernel_size=(8, 8),strides=4, activation='relu', name='S1_Conv1', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s1_input_layer)\n",
        "        s1_output = tf.keras.layers.Conv2D(64, kernel_size=(4, 4),strides=2, activation='relu', name='S1_Conv2', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s1_conv1)\n",
        "        s1_model = tf.keras.Model(inputs=s1_input_layer, outputs=s1_output)\n",
        "        # s1_model.summary()\n",
        "\n",
        "       # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(11, 8, 64))\n",
        "        s2_conv1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),strides=1, activation='relu', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_input_layer)\n",
        "        s2_flat_1 = tf.keras.layers.Flatten(name='S2_Flat1')(s2_conv1)\n",
        "        s2_output = tf.keras.layers.Dense(512, activation='relu', name='s2_Dense_1', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_flat_1)\n",
        "        s2_model = tf.keras.Model(inputs=s2_input_layer, outputs=s2_output)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_output = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "        s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "        \n",
        "\n",
        "    def _build_1S_transfer_model(self): # private method    \n",
        "\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_model = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)\n",
        "        s1_model._name = SLICE_ONE_NAME\n",
        "        # s1_model.summary()\n",
        "\n",
        "        # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(11, 8, 64))\n",
        "        s2_conv1 = tf.keras.layers.Conv2D(64, kernel_size=(3, 3),strides=1, activation='relu', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_input_layer)\n",
        "        s2_flat_1 = tf.keras.layers.Flatten(name='S2_Flat1')(s2_conv1)\n",
        "        s2_output = tf.keras.layers.Dense(512, activation='relu', name='s2_Dense_1', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s2_flat_1)\n",
        "        s2_model = tf.keras.Model(inputs=s2_input_layer, outputs=s2_output)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_output = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "        s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "\n",
        "\n",
        "    def _build_2S_transfer_model(self): # private method    \n",
        "\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=(3,)+ WINDOW_SIZE)\n",
        "        s1_model = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)\n",
        "        s1_model._name = SLICE_ONE_NAME\n",
        "        # s1_model.summary()\n",
        "\n",
        "        # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(9,9,64))\n",
        "        # s2_input_layer = tf.keras.Input(shape=(s1_model.shape[1::1]))\n",
        "        s2_model = tf.keras.models.load_model(model_dir + '/' + SLICE_TWO_NAME)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_output = tf.keras.layers.Dense(self.action_size, activation='linear', \\\n",
        "                                    kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2.0))(s3_input_layer)\n",
        "        s3_model = tf.keras.Model(inputs=s3_input_layer, outputs=s3_output)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "\n",
        "\n",
        "    def _build_3S_transfer_model(self): # private method    \n",
        "\n",
        "        # Slice One\n",
        "        s1_input_layer = tf.keras.Input(shape=WINDOW_SIZE + (3,))\n",
        "        s1_model = tf.keras.models.load_model(model_dir + '/' + SLICE_ONE_NAME)\n",
        "        s1_model._name = SLICE_ONE_NAME\n",
        "        # s1_model.summary()\n",
        "\n",
        "        # Slice Two\n",
        "        s2_input_layer = tf.keras.Input(shape=(11, 8, 64))\n",
        "        s2_model = tf.keras.models.load_model(model_dir + '/' + SLICE_TWO_NAME)\n",
        "        s2_model._name = SLICE_TWO_NAME\n",
        "        # s2_model.summary()\n",
        "\n",
        "        # Slice Three                                   #### ndim 4?\n",
        "        s3_input_layer = tf.keras.Input(shape=(512))\n",
        "        s3_model = tf.keras.models.load_model(model_dir + '/' + SLICE_THREE_NAME)\n",
        "        s3_model._name = SLICE_THREE_NAME\n",
        "        # s3_model.summary()\n",
        "\n",
        "        full_model_input = tf.keras.Input(shape=(WINDOW_SIZE+(3,)))\n",
        "        s1_pass = s1_model(full_model_input) #, training=False)\n",
        "        s2_pass = s2_model(s1_pass)\n",
        "        s3_final_output = s3_model(s2_pass)\n",
        "\n",
        "        \n",
        "        full_model = tf.keras.Model(inputs=full_model_input, outputs=s3_final_output)\n",
        "        full_model._name = FULL_MODEL_NAME\n",
        "        # full_model.summary()\n",
        "\n",
        "        s1_model.trainable = SLICE_ONE_TRAINABLE # ref param above\n",
        "        s2_model.trainable = SLICE_TWO_TRAINABLE # ref param above\n",
        "        s3_model.trainable = SLICE_THREE_TRAINABLE # ref param above\n",
        "        full_model.trainable = FULL_MODEL_TRAINABLE # ref param above\n",
        "        \n",
        "        # # Using Nadam because its awesome\n",
        "        self.optimizer = keras.optimizers.Nadam(learning_rate=self.learning_rate, clipnorm=1.0)\n",
        "        # # Using Huber loss for stability\n",
        "        self.loss_function = keras.losses.Huber(reduction=tf.keras.losses.Reduction.SUM)\n",
        "        \n",
        "        return full_model, s1_model, s2_model, s3_model\n",
        "\n",
        "\n",
        "    # def rememberOld(self, state, action, reward, next_state, done):\n",
        "    #     # My newly implemented memory, Hopefully it makes us faster\n",
        "    #     self.state_memory.append(state)\n",
        "    #     self.action_memory.append(action)\n",
        "    #     self.reward_memory.append(reward)\n",
        "    #     self.next_state_memory.append(next_state)\n",
        "    #     self.done_memory.append(done)\n",
        "    #     if len(self.reward_memory) > MAX_MEMORY_LENGTH:\n",
        "    #         del self.reward_memory[:1]\n",
        "    #         del self.state_memory[:1]\n",
        "    #         del self.next_state_memory[:1]\n",
        "    #         del self.action_memory[:1]\n",
        "    #         del self.done_memory[:1]\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        # My newly reimplemented memory, Hopefully it makes us faster\n",
        "        self.replay_memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def jit_sampler(self, batch_size):\n",
        "        # idx = np.random.choice(range(len(self.reward_memory)), size=batch_size)\n",
        "        # idx = np.random.randint(self.replay_memory.size, size=batch_size)\n",
        "        batch = self.replay_memory.sample(batch_size)\n",
        "\n",
        "        # batch = [self.replay_memory[i] for i in idx]\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = [\n",
        "                np.array([exp[fidx] for exp in batch])\n",
        "                for fidx in range(5)]\n",
        "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
        "\n",
        "                 \n",
        "        #  = np.array([self.state_memory[i] for i in idx])\n",
        "        #  = [self.action_memory[i] for i in idx]\n",
        "        #  = [self.reward_memory[i] for i in idx]\n",
        "        #  = np.array([self.next_state_memory[i] for i in idx])\n",
        "        #  = tf.convert_to_tensor([float(self.done_memory[i]) for i in idx])\n",
        "\n",
        "    def train(self, batch_size): # method that trains agent model\n",
        "\n",
        "        ### The New Implementation\n",
        "        # idx = np.random.choice(range(len(self.reward_memory)), size=batch_size)\n",
        "\n",
        "        # state_batch = np.array([self.state_memory[i] for i in idx])\n",
        "        # action_batch = [self.action_memory[i] for i in idx]\n",
        "        # reward_batch = [self.reward_memory[i] for i in idx]\n",
        "        # next_state_batch = np.array([self.next_state_memory[i] for i in idx])\n",
        "        # done_batch = tf.convert_to_tensor([float(self.done_memory[i]) for i in idx])\n",
        "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = self.jit_sampler(batch_size)\n",
        "        ## This is where all the magic happens\n",
        "        qValueF = self.target_model.predict_on_batch(next_state_batch) # approximate future reward\n",
        "\n",
        "        qValueUpdate = reward_batch + self.gamma * tf.reduce_max(qValueF, axis=1)\n",
        "\n",
        "        qValueUpdate = qValueUpdate * (1 - done_batch) - done_batch\n",
        "\n",
        "        oneHotMask = tf.one_hot(action_batch, self.action_size)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Train the model on the states and updated Q-values\n",
        "            OqValue = self.model(state_batch)\n",
        "\n",
        "            # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "            OqValue_action = tf.reduce_sum(tf.multiply(OqValue, oneHotMask), axis=1)\n",
        "\n",
        "            # Calculate loss between new Q-value and old Q-value\n",
        "            loss = self.loss_function(qValueUpdate, OqValue_action)\n",
        "            if CHIPPIE_PROGESS_REPORTS:\n",
        "                  chippie.logLoss(loss)\n",
        "\n",
        "        if DIAGNOSTIC_MODE:\n",
        "          print(\"Q-Values: \", qValueF[1])\n",
        "          print(\"Q-Values Update: \", qValueUpdate[1])\n",
        "          print(\"Masked Update:   \", oneHotMask[1])\n",
        "          print(\"Old Activations: \", OqValue[1])\n",
        "\n",
        "        # Backpropagation\n",
        "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
        "\n",
        "        if DIAGNOSTIC_MODE:\n",
        "          newQ = self.model.predict_step(state_batch)\n",
        "          print(\"New Activations: \", newQ[1], \"\\n\")\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon: \n",
        "            return random.randrange(self.action_size) # Do something stupid! (i.e. take a random action)\n",
        "        action_values = self.model.predict_step(tf.expand_dims(tf.convert_to_tensor(state), 0))\n",
        "        # action_values = self.model(tf.expand_dims(tf.convert_to_tensor(state), 0), training=False)\n",
        "        return tf.argmax(action_values[0]).numpy()\n",
        "\n",
        "\n",
        "    def update_target_model(self):\n",
        "        self.target_model.set_weights(self.model.get_weights())\n",
        "    \n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n",
        "\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qnyFasRfoBoT"
      },
      "source": [
        "At the very end, in a last ditch effort to get enough speed to succeed the replay buffer was re-implemented using numpy and slicing notation. This change provided about ten times as much speed and allowed the first test to reach 2500 episodes before a timeout or crash!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYyAtezpCwYg"
      },
      "source": [
        "#### The Wrappers\n",
        "So here we will use some wrappers from the baselines package. This broke my Tensorflow installation on my home server so I avoided wrappers during the majority of this project however, in the end using the optimized implementations in the wrapper drove speed up 1000% or so. So, if you are implementing your own version of this experiment, I cannot stress enough how important it is to study up on these wrappers to start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw2g6PjZPS9m",
        "outputId": "8bfde089-90e5-4701-b9ad-265f38b6887d"
      },
      "source": [
        "# Make Environment\n",
        "\n",
        "from gym.wrappers.atari_preprocessing import AtariPreprocessing\n",
        "# from gym.wrappers.frame_stack import FrameStack\n",
        "from baselines.common.atari_wrappers import make_atari, EpisodicLifeEnv, WarpFrame, ScaledFloatFrame, ClipRewardEnv, FrameStack, FireResetEnv\n",
        "\n",
        "# env = FrameStack(AtariPreprocessing(gym.make(ENVIRONMENT_NAME), frame_skip=4, \n",
        "#                                   screen_size=(84), \n",
        "#                                   terminal_on_life_loss=True, \n",
        "#                                   grayscale_obs=True),3)\n",
        "env = make_atari(ENVIRONMENT_NAME)\n",
        "env = EpisodicLifeEnv(env)\n",
        "\n",
        "if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "\n",
        "env = WarpFrame(env)\n",
        "\n",
        "env = ScaledFloatFrame(env)\n",
        "\n",
        "env = ClipRewardEnv(env)\n",
        "\n",
        "env = FrameStack(env, 3)\n",
        "\n",
        "# myWrappedEnv = FrameStack(atariWrap, 3)                  env_wrappers=[lambda env: ActionRepeat(env, times=4)])\n",
        "env.seed(42)\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "# env = gym.make(ENVIRONMENT_NAME)\n",
        "state_size = (84, 84, 3)\n",
        "action_size = env.action_space.n\n",
        "\n",
        "## Intialize Our Agent\n",
        "agent = DQNAgent(state_size, action_size)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"s2_MAT_Agent_Testing_v2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         [(None, 84, 84, 3)]       0         \n",
            "_________________________________________________________________\n",
            "s1_MAT_ImageClassifier_v5 (F (None, 9, 9, 64)          39008     \n",
            "_________________________________________________________________\n",
            "s2_MAT_ImageClassifier_v5 (F (None, 512)               1643072   \n",
            "_________________________________________________________________\n",
            "s3_MAT_ImageClassifier_v2 (F (None, 4)                 2052      \n",
            "=================================================================\n",
            "Total params: 1,684,132\n",
            "Trainable params: 1,684,132\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8z5loBrJiHON"
      },
      "source": [
        "#### Define Training Loop Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B387ooz8nWEF"
      },
      "source": [
        "Getting your reward function is pretty paramount. In this case we will clip rewards to +1 and -1, with -1 being awarded for reaching a terminal state."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8cbu_VJh-E4"
      },
      "source": [
        "def myPlayground():\n",
        "\n",
        "    # Empty Rewards Counter\n",
        "    reward = 0\n",
        "    all_rewards = 0\n",
        "    total_time = 0\n",
        "    done = False\n",
        "    # print(\"checkpoint 1\")\n",
        "    # Display in Colab\n",
        "    if COLAB:\n",
        "        if RENDER == True:\n",
        "            done, ax = plt.subplots(1, 1)\n",
        "            img = ax.imshow(env.render('rgb_array'))    \n",
        "\n",
        "    # Make Some Starting data\n",
        "    # state = tFrame.process_frame(env.reset())\n",
        "    state = env.reset() # Reset state for new episode\n",
        "    # print(type(state))\n",
        "    # print(state.shape)\n",
        "    while agent.replay_memory.size < STARTING_MEMORY_SIZE:\n",
        "      action = random.randrange(agent.action_size)\n",
        "      next_frame, reward, done, _ = env.step(action) # Agent sends action to env wrapper and gets feedback\n",
        "      # next_state = tFrame.process_frame(next_frame)\n",
        "      next_state = next_frame\n",
        "      if reward > 0.0: # Check and modify reward\n",
        "        reward = 1.0\n",
        "      if reward < 0.0:\n",
        "        reward = -1.0\n",
        "      if done:\n",
        "        reward = -1.0\n",
        "      agent.remember(state, action, reward, next_state, done) # Store sequence in replay memory\n",
        "      state = next_state # Update state\n",
        "      if done:\n",
        "        # state = tFrame.process_frame(env.reset())\n",
        "        state = env.reset()\n",
        "    print(\"Starting Frames Acquired\")\n",
        "\n",
        "    # Training Loop\n",
        "    for e in range(n_episodes): ## Go Eat Cherries\n",
        "        # print(\"checkpoint 3\")\n",
        "        done = False\n",
        "        time = 0\n",
        "        game_score = 0\n",
        "        reward = 0\n",
        "\n",
        "        # state = tFrame.process_frame(env.reset()) # Reset state for new episode\n",
        "        state = env.reset()\n",
        "        \n",
        "        for skip in range(skip_start): # Skip the start of each game\n",
        "            env.step(0)\n",
        "\n",
        "        while not done:\n",
        "        \n",
        "            time += 1\n",
        "            total_time += 1\n",
        "\n",
        "            # Display\n",
        "            if RENDER:\n",
        "                if COLAB:\n",
        "                    img.set_data(env.render(mode='rgb_array')) \n",
        "                    ax.axis('off')\n",
        "                    ipythondisplay.display(plt.gcf())\n",
        "                    ipythondisplay.clear_output(wait=True)\n",
        "                else:\n",
        "                    env.render()\n",
        "                    if OBSERVATION_MODE:\n",
        "                        sleep(0.02)\n",
        "\n",
        "            # Update Target Network\n",
        "            if total_time % agent.update_rate == 0:\n",
        "                agent.update_target_model()\n",
        "                if CHIPPIE_PROGESS_REPORTS:\n",
        "                  chippie.q_update()\n",
        "\n",
        "            # Transition Dynamics\n",
        "            action = agent.act(state) # Get action from agent\n",
        "            next_frame, reward, done, _ = env.step(action) # Agent sends action to env wrapper and gets feedback\n",
        "            # next_state = tFrame.process_frame(next_frame)\n",
        "            next_state = next_frame\n",
        "            \n",
        "            # Sternly Validate Reward\n",
        "            if reward > 0.0: \n",
        "              reward = 1.0\n",
        "              game_score += reward\n",
        "            if reward < 0.0:\n",
        "              reward = -1.0\n",
        "            if done:\n",
        "              reward = -1.0\n",
        "              all_rewards += game_score\n",
        "              if CHIPPIE_PROGESS_REPORTS:\n",
        "                chippie.dead(score=game_score, totalScore=all_rewards, episode=e, completion_target=n_episodes, survived=time, \n",
        "                              experiance=total_time, memory=agent.replay_memory.size, epsilon=agent.epsilon)\n",
        "              agent.remember(state, action, reward, next_state, done) # Store death in replay memory\n",
        "              break\n",
        "\n",
        "            agent.remember(state, action, reward, next_state, done) # Store sequence in replay memory\n",
        "            state = next_state # Update state\n",
        "\n",
        "            if agent.replay_memory.size > STARTING_MEMORY_SIZE:\n",
        "              if not OBSERVATION_MODE:\n",
        "                if total_time % action_steps == 0:\n",
        "                  agent.train(BATCH_SIZE)\n",
        "                  if CHIPPIE_PROGESS_REPORTS:\n",
        "                    chippie.training(game_score)\n",
        "\n",
        "        if e % CHECKPOINT_FREQUENCY == 0 and agent.replay_memory.size > STARTING_MEMORY_SIZE+100:\n",
        "          print(\"\")\n",
        "          if SLICE_ONE_CHECKPOINT:\n",
        "              saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + SLICE_ONE_NAME + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY)) \n",
        "              tf.saved_model.save(agent.sliceOne, saved_model_path)\n",
        "\n",
        "          if SLICE_TWO_CHECKPOINT:\n",
        "              saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + SLICE_TWO_NAME + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY))\n",
        "              tf.saved_model.save(agent.sliceTwo, saved_model_path)\n",
        "          \n",
        "          if SLICE_THREE_CHECKPOINT:\n",
        "              saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + SLICE_TWO_NAME + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY))\n",
        "              tf.saved_model.save(agent.sliceTwo, saved_model_path)\n",
        "\n",
        "          if FULL_MODEL_CHECKPOINT:\n",
        "              saved_model_path = model_dir + \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + FULL_MODEL_NAME + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY))\n",
        "              tf.saved_model.save(agent.model, saved_model_path)\n",
        "\n",
        "          if SAVE_AGENT:\n",
        "              agent.save(model_dir+ \"/ISAR_Production_Models/\" + ENVIRONMENT_NAME + FULL_MODEL_NAME + \"agent_weights_\" + '{:04d}'.format(int(e/CHECKPOINT_FREQUENCY)) +\".hdf5\")\n",
        "          \n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgI4QKIOo6Nl"
      },
      "source": [
        "#### Interactive Playground (runs automatically)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bw1oZlgnoNY"
      },
      "source": [
        "As the agent finally blitzed through the code, we are getting close to the deadline and what began as an awesome undertaking in abstraction might formally (and temporarily) become a pure comparision of how the transfer learned weights from notebook 1 affect training speed. However, there is still yet another problem, because with only 9 days left to finish, and each test taking a full 12 hours to see results the results come in and the model is still showing no progress. Luckily, one more day of debugging and preparation found a simple bug causing enourmous amounts of pre-buffer data to be labelled terminal. After this fix, the agent has begun to train comparably to the only other working example we found (linked above). This implementation is still slower, however, in that example the baselines wrapper provided by DeepMind is used, which contains significant speed and memory optimizations which unfortunately, only appear to offer frame stacking at 4 frames. Since this would impair our transfer learning basis this is currently the fastest implementation we can provide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i9UWy2w5nUs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aeac425-74aa-434e-8d7f-204353786a00"
      },
      "source": [
        "# #### Run From Bottom (Can be run with one click)\n",
        "# agent.epsilon = 0.999\n",
        "# agent.learning_rate = 0.000125\n",
        "\n",
        "## This will take about 3 minutes in colab to gather starting data and output some feedback\n",
        "## after this you can expect chippieBot limited updates about every 5 minutes for the next 7-12 hours.\n",
        "myPlayground()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Frames Acquired\n",
            "\n",
            "Episode: 250/50000, Episode Score: 0, Avg Episode Score: 0.224, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.26\n",
            "Total Steps: 7840, Memory Size: 82839, Current Epsilon Value: 0.83, Leading Loss: 0.08731/ Trailing Loss: 0.1067\n",
            "\n",
            "Episode: 500/50000, Episode Score: 1.0, Avg Episode Score: 0.266, Survival Time: 50\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.25\n",
            "Total Steps: 16389, Memory Size: 91388, Current Epsilon Value: 0.67, Leading Loss: 0.08261/ Trailing Loss: 0.08596\n",
            "\n",
            "Episode: 750/50000, Episode Score: 0, Avg Episode Score: 0.2467, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.25\n",
            "Total Steps: 23820, Memory Size: 98819, Current Epsilon Value: 0.56, Leading Loss: 0.08357/ Trailing Loss: 0.08526\n",
            "\n",
            "Episode: 1000/50000, Episode Score: 0, Avg Episode Score: 0.247, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.22\n",
            "Total Steps: 31786, Memory Size: 100000, Current Epsilon Value: 0.46, Leading Loss: 0.07711/ Trailing Loss: 0.08629\n",
            "\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s1_MAT_ImageClassifier_v50001/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50001/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50001/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_Agent_Testing_v20001/assets\n",
            "\n",
            "Episode: 1250/50000, Episode Score: 0, Avg Episode Score: 0.2408, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.31\n",
            "Total Steps: 39393, Memory Size: 100000, Current Epsilon Value: 0.39, Leading Loss: 0.07711/ Trailing Loss: 0.07425\n",
            "\n",
            "Episode: 1500/50000, Episode Score: 1.0, Avg Episode Score: 0.2493, Survival Time: 50\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.27\n",
            "Total Steps: 47647, Memory Size: 100000, Current Epsilon Value: 0.32, Leading Loss: 0.06844/ Trailing Loss: 0.07374\n",
            "\n",
            "Episode: 1750/50000, Episode Score: 0, Avg Episode Score: 0.2571, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.27\n",
            "Total Steps: 56057, Memory Size: 100000, Current Epsilon Value: 0.26, Leading Loss: 0.06473/ Trailing Loss: 0.06747\n",
            "\n",
            "Episode: 2000/50000, Episode Score: 1.0, Avg Episode Score: 0.271, Survival Time: 66\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.36\n",
            "Total Steps: 64973, Memory Size: 100000, Current Epsilon Value: 0.21, Leading Loss: 0.0594/ Trailing Loss: 0.06171\n",
            "\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s1_MAT_ImageClassifier_v50002/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50002/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50002/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_Agent_Testing_v20002/assets\n",
            "\n",
            "Episode: 2250/50000, Episode Score: 0, Avg Episode Score: 0.2676, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.23\n",
            "Total Steps: 72605, Memory Size: 100000, Current Epsilon Value: 0.17, Leading Loss: 0.05718/ Trailing Loss: 0.06067\n",
            "\n",
            "Episode: 2500/50000, Episode Score: 0, Avg Episode Score: 0.26, Survival Time: 20\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.16\n",
            "Total Steps: 79802, Memory Size: 100000, Current Epsilon Value: 0.14, Leading Loss: 0.05781/ Trailing Loss: 0.05579\n",
            "\n",
            "Episode: 2750/50000, Episode Score: 0, Avg Episode Score: 0.2655, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.26\n",
            "Total Steps: 88177, Memory Size: 100000, Current Epsilon Value: 0.12, Leading Loss: 0.05/ Trailing Loss: 0.05005\n",
            "\n",
            "Episode: 3000/50000, Episode Score: 0, Avg Episode Score: 0.2693, Survival Time: 20\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.35\n",
            "Total Steps: 96586, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.0386/ Trailing Loss: 0.04332\n",
            "\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s1_MAT_ImageClassifier_v50003/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50003/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50003/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_Agent_Testing_v20003/assets\n",
            "\n",
            "Episode: 3250/50000, Episode Score: 0, Avg Episode Score: 0.2677, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.27\n",
            "Total Steps: 104273, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.03554/ Trailing Loss: 0.03647\n",
            "\n",
            "Episode: 3500/50000, Episode Score: 0, Avg Episode Score: 0.2769, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.45\n",
            "Total Steps: 113327, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.02741/ Trailing Loss: 0.03293\n",
            "\n",
            "Episode: 3750/50000, Episode Score: 0, Avg Episode Score: 0.2789, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.27\n",
            "Total Steps: 121843, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.0203/ Trailing Loss: 0.02408\n",
            "\n",
            "Episode: 4000/50000, Episode Score: 0, Avg Episode Score: 0.304, Survival Time: 22\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.65\n",
            "Total Steps: 134272, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.01765/ Trailing Loss: 0.01734\n",
            "\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s1_MAT_ImageClassifier_v50004/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50004/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50004/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_Agent_Testing_v20004/assets\n",
            "\n",
            "Episode: 4250/50000, Episode Score: 0, Avg Episode Score: 0.3233, Survival Time: 20\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.62\n",
            "Total Steps: 146153, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.01548/ Trailing Loss: 0.01707\n",
            "\n",
            "Episode: 4500/50000, Episode Score: 2.0, Avg Episode Score: 0.3427, Survival Time: 95\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~0.78\n",
            "Total Steps: 158576, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.01432/ Trailing Loss: 0.01643\n",
            "\n",
            "Episode: 4750/50000, Episode Score: 1.0, Avg Episode Score: 0.3964, Survival Time: 54\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~1.66\n",
            "Total Steps: 177196, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.01746/ Trailing Loss: 0.01843\n",
            "\n",
            "Episode: 5000/50000, Episode Score: 1.0, Avg Episode Score: 0.4744, Survival Time: 74\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~2.13\n",
            "Total Steps: 201278, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.02185/ Trailing Loss: 0.02035\n",
            "\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s1_MAT_ImageClassifier_v50005/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50005/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50005/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_Agent_Testing_v20005/assets\n",
            "\n",
            "Episode: 5250/50000, Episode Score: 1.0, Avg Episode Score: 0.5636, Survival Time: 54\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~2.39\n",
            "Total Steps: 227449, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.02575/ Trailing Loss: 0.02649\n",
            "\n",
            "Episode: 5500/50000, Episode Score: 2.0, Avg Episode Score: 0.6636, Survival Time: 82\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~2.8\n",
            "Total Steps: 257547, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.0281/ Trailing Loss: 0.02947\n",
            "\n",
            "Episode: 5750/50000, Episode Score: 3.0, Avg Episode Score: 0.7638, Survival Time: 171\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~2.92\n",
            "Total Steps: 289146, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.02883/ Trailing Loss: 0.03029\n",
            "\n",
            "Episode: 6000/50000, Episode Score: 1.0, Avg Episode Score: 0.8855, Survival Time: 44\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~3.91\n",
            "Total Steps: 326684, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.02506/ Trailing Loss: 0.02702\n",
            "\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s1_MAT_ImageClassifier_v50006/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50006/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50006/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_Agent_Testing_v20006/assets\n",
            "\n",
            "Episode: 6250/50000, Episode Score: 2.0, Avg Episode Score: 1.013, Survival Time: 99\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~4.17\n",
            "Total Steps: 367365, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.02331/ Trailing Loss: 0.02404\n",
            "\n",
            "Episode: 6500/50000, Episode Score: 1.0, Avg Episode Score: 1.167, Survival Time: 43\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~4.99\n",
            "Total Steps: 413871, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.02464/ Trailing Loss: 0.02649\n",
            "\n",
            "Episode: 6750/50000, Episode Score: 1.0, Avg Episode Score: 1.321, Survival Time: 52\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~5.89\n",
            "Total Steps: 462266, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.02034/ Trailing Loss: 0.02262\n",
            "\n",
            "Episode: 7000/50000, Episode Score: 7.0, Avg Episode Score: 1.495, Survival Time: 164\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~6.19\n",
            "Total Steps: 515393, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.01681/ Trailing Loss: 0.01664\n",
            "\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s1_MAT_ImageClassifier_v50007/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50007/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_ImageClassifier_v50007/assets\n",
            "INFO:tensorflow:Assets written to: ISAR_Model_Data/ISAR_Production_Models/BreakoutNoFrameskip-v4s2_MAT_Agent_Testing_v20007/assets\n",
            "\n",
            "Episode: 7250/50000, Episode Score: 6.0, Avg Episode Score: 1.658, Survival Time: 212\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~6.73\n",
            "Total Steps: 570343, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.01554/ Trailing Loss: 0.01628\n",
            "\n",
            "Episode: 7500/50000, Episode Score: 4.0, Avg Episode Score: 1.813, Survival Time: 92\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~6.69\n",
            "Total Steps: 623128, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.01925/ Trailing Loss: 0.0166\n",
            "\n",
            "Episode: 7750/50000, Episode Score: 1.0, Avg Episode Score: 1.978, Survival Time: 53\n",
            "Your agent has a running average Score per 100 episodes of: \\{^,^}~7.12\n",
            "Total Steps: 680378, Memory Size: 100000, Current Epsilon Value: 0.1, Leading Loss: 0.01559/ Trailing Loss: 0.01701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZKKOtCtCcA7"
      },
      "source": [
        "#### Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZbLrYRH-Ouv"
      },
      "source": [
        "When we are all done building this last box will zip our progress for download so we can continue our work later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE2np3HdCla9"
      },
      "source": [
        "ZIP_OUTPUT = False #@param {type:\"boolean\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOaTq8Gi-I68"
      },
      "source": [
        "%%capture\n",
        "if ZIP_OUTPUT:\n",
        "  zip = !zip -r ISAR_Model_Data_Output.zip ISAR_Model_Data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CVQf5Yt-t5z"
      },
      "source": [
        "###### Reference Notes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BSpjtz3wlnvQ"
      },
      "source": [
        "# Executing (12h 4m 14s)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HsvGwWyzoCc"
      },
      "source": [
        "# import gc\n",
        "# gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfOSMeM1CcBC"
      },
      "source": [
        "# running reward: 0.38 at episode 293, frame count 10000\n",
        "# running reward: 0.31 at episode 589, frame count 20000\n",
        "# running reward: 0.19 at episode 905, frame count 30000\n",
        "# running reward: 0.23 at episode 1214, frame count 40000\n",
        "# running reward: 0.39 at episode 1512, frame count 50000\n",
        "# running reward: 0.33 at episode 1802, frame count 60000\n",
        "# running reward: 0.32 at episode 2087, frame count 70000\n",
        "# running reward: 0.23 at episode 2390, frame count 80000\n",
        "# running reward: 0.33 at episode 2680, frame count 90000\n",
        "# running reward: 0.17 at episode 2977, frame count 100000\n",
        "# running reward: 0.19 at episode 3274, frame count 110000\n",
        "# running reward: 0.28 at episode 3558, frame count 120000\n",
        "# running reward: 0.38 at episode 3839, frame count 130000\n",
        "# running reward: 0.39 at episode 4110, frame count 140000\n",
        "# running reward: 0.48 at episode 4354, frame count 150000\n",
        "# running reward: 0.38 at episode 4600, frame count 160000\n",
        "# running reward: 0.37 at episode 4858, frame count 170000\n",
        "# running reward: 0.47 at episode 5099, frame count 180000\n",
        "# running reward: 0.47 at episode 5315, frame count 190000\n",
        "# running reward: 0.60 at episode 5529, frame count 200000\n",
        "# running reward: 0.60 at episode 5744, frame count 210000\n",
        "# running reward: 0.63 at episode 5956, frame count 220000\n",
        "# running reward: 0.59 at episode 6181, frame count 230000\n",
        "# running reward: 0.59 at episode 6407, frame count 240000\n",
        "# running reward: 0.53 at episode 6625, frame count 250000\n",
        "# running reward: 0.62 at episode 6842, frame count 260000\n",
        "# running reward: 0.62 at episode 7057, frame count 270000\n",
        "# running reward: 0.50 at episode 7285, frame count 280000\n",
        "# running reward: 0.68 at episode 7505, frame count 290000\n",
        "# running reward: 0.55 at episode 7726, frame count 300000\n",
        "# running reward: 0.65 at episode 7930, frame count 310000\n",
        "# running reward: 0.70 at episode 8136, frame count 320000\n",
        "# running reward: 0.86 at episode 8321, frame count 330000\n",
        "# running reward: 0.57 at episode 8535, frame count 340000\n",
        "# running reward: 0.69 at episode 8732, frame count 350000\n",
        "# running reward: 0.51 at episode 8953, frame count 360000\n",
        "# running reward: 0.76 at episode 9144, frame count 370000\n",
        "# running reward: 0.72 at episode 9357, frame count 380000\n",
        "# running reward: 0.86 at episode 9550, frame count 390000\n",
        "# running reward: 0.63 at episode 9750, frame count 400000\n",
        "# running reward: 0.55 at episode 9989, frame count 410000\n",
        "# running reward: 0.88 at episode 10175, frame count 420000\n",
        "# running reward: 0.68 at episode 10371, frame count 430000\n",
        "# running reward: 0.78 at episode 10569, frame count 440000\n",
        "# running reward: 0.83 at episode 10736, frame count 450000\n",
        "# running reward: 1.00 at episode 10900, frame count 460000\n",
        "# running reward: 0.82 at episode 11090, frame count 470000\n",
        "# running reward: 1.11 at episode 11237, frame count 480000\n",
        "# running reward: 1.11 at episode 11393, frame count 490000\n",
        "# running reward: 1.16 at episode 11543, frame count 500000\n",
        "# running reward: 1.38 at episode 11686, frame count 510000\n",
        "# running reward: 1.23 at episode 11833, frame count 520000\n",
        "# running reward: 1.31 at episode 11976, frame count 530000\n",
        "# running reward: 1.32 at episode 12122, frame count 540000\n",
        "# running reward: 1.59 at episode 12246, frame count 550000\n",
        "# running reward: 1.65 at episode 12371, frame count 560000\n",
        "# running reward: 1.53 at episode 12500, frame count 570000\n",
        "# running reward: 1.44 at episode 12643, frame count 580000\n",
        "# running reward: 1.54 at episode 12776, frame count 590000\n",
        "# running reward: 1.61 at episode 12900, frame count 600000\n",
        "# running reward: 1.83 at episode 13012, frame count 610000\n",
        "# running reward: 1.98 at episode 13123, frame count 620000\n",
        "# running reward: 2.08 at episode 13230, frame count 630000\n",
        "# running reward: 1.96 at episode 13336, frame count 640000\n",
        "# running reward: 2.01 at episode 13449, frame count 650000\n",
        "# running reward: 2.21 at episode 13548, frame count 660000\n",
        "# running reward: 2.29 at episode 13648, frame count 670000\n",
        "# running reward: 2.35 at episode 13744, frame count 680000\n",
        "# running reward: 2.40 at episode 13837, frame count 690000\n",
        "# running reward: 2.29 at episode 13934, frame count 700000\n",
        "# running reward: 2.54 at episode 14026, frame count 710000\n",
        "# running reward: 2.30 at episode 14121, frame count 720000\n",
        "# running reward: 2.20 at episode 14218, frame count 730000\n",
        "# running reward: 2.79 at episode 14301, frame count 740000\n",
        "# running reward: 2.73 at episode 14387, frame count 750000\n",
        "# running reward: 2.93 at episode 14465, frame count 760000\n",
        "# running reward: 2.72 at episode 14553, frame count 770000\n",
        "# running reward: 2.57 at episode 14640, frame count 780000\n",
        "# running reward: 2.60 at episode 14719, frame count 790000\n",
        "# running reward: 2.79 at episode 14797, frame count 800000\n",
        "# running reward: 3.20 at episode 14865, frame count 810000\n",
        "# running reward: 3.27 at episode 14938, frame count 820000\n",
        "# running reward: 3.06 at episode 15006, frame count 830000\n",
        "# running reward: 3.50 at episode 15073, frame count 840000\n",
        "# running reward: 3.63 at episode 15139, frame count 850000\n",
        "# running reward: 3.78 at episode 15203, frame count 860000\n",
        "# running reward: 3.67 at episode 15270, frame count 870000\n",
        "# running reward: 3.69 at episode 15333, frame count 880000\n",
        "# running reward: 4.40 at episode 15384, frame count 890000\n",
        "# running reward: 4.70 at episode 15439, frame count 900000\n",
        "# running reward: 5.28 at episode 15482, frame count 910000\n",
        "# running reward: 5.98 at episode 15525, frame count 920000\n",
        "# running reward: 5.74 at episode 15575, frame count 930000\n",
        "# running reward: 6.22 at episode 15612, frame count 940000\n",
        "# running reward: 5.76 at episode 15662, frame count 950000\n",
        "# running reward: 6.37 at episode 15695, frame count 960000\n",
        "# running reward: 7.02 at episode 15736, frame count 970000\n",
        "# running reward: 7.24 at episode 15777, frame count 980000\n",
        "# running reward: 7.35 at episode 15816, frame count 990000\n",
        "# running reward: 7.41 at episode 15852, frame count 1000000\n",
        "# running reward: 8.06 at episode 15889, frame count 1010000\n",
        "# running reward: 8.18 at episode 15928, frame count 1020000\n",
        "# running reward: 8.00 at episode 15965, frame count 1030000\n",
        "# running reward: 7.38 at episode 16008, frame count 1040000\n",
        "# running reward: 7.24 at episode 16048, frame count 1050000\n",
        "# running reward: 6.93 at episode 16091, frame count 1060000\n",
        "# running reward: 7.73 at episode 16125, frame count 1070000\n",
        "# running reward: 7.16 at episode 16172, frame count 1080000\n",
        "# running reward: 7.08 at episode 16210, frame count 1090000\n",
        "# running reward: 7.20 at episode 16246, frame count 1100000\n",
        "# running reward: 7.89 at episode 16285, frame count 1110000\n",
        "# running reward: 7.83 at episode 16321, frame count 1120000\n",
        "# running reward: 7.19 at episode 16366, frame count 1130000\n",
        "# running reward: 7.99 at episode 16399, frame count 1140000\n",
        "# running reward: 7.90 at episode 16440, frame count 1150000\n",
        "# running reward: 8.47 at episode 16474, frame count 1160000\n",
        "# running reward: 8.40 at episode 16511, frame count 1170000\n",
        "# running reward: 7.72 at episode 16555, frame count 1180000\n",
        "# running reward: 8.15 at episode 16591, frame count 1190000\n",
        "# running reward: 8.84 at episode 16628, frame count 1200000\n",
        "# running reward: 8.93 at episode 16662, frame count 1210000\n",
        "# running reward: 9.66 at episode 16695, frame count 1220000\n",
        "# running reward: 9.07 at episode 16733, frame count 1230000\n",
        "# running reward: 8.91 at episode 16769, frame count 1240000\n",
        "# running reward: 8.84 at episode 16803, frame count 1250000\n",
        "# running reward: 9.18 at episode 16838, frame count 1260000\n",
        "# running reward: 8.79 at episode 16875, frame count 1270000\n",
        "# running reward: 9.07 at episode 16908, frame count 1280000\n",
        "# running reward: 9.27 at episode 16943, frame count 1290000\n",
        "# running reward: 9.23 at episode 16979, frame count 1300000\n",
        "# running reward: 8.59 at episode 17015, frame count 1310000\n",
        "# running reward: 8.49 at episode 17055, frame count 1320000\n",
        "# running reward: 8.39 at episode 17091, frame count 1330000\n",
        "# running reward: 9.11 at episode 17128, frame count 1340000\n",
        "# running reward: 9.36 at episode 17164, frame count 1350000\n",
        "# running reward: 9.98 at episode 17198, frame count 1360000\n",
        "# running reward: 9.91 at episode 17236, frame count 1370000\n",
        "# running reward: 10.31 at episode 17272, frame count 1380000\n",
        "# running reward: 10.09 at episode 17305, frame count 1390000\n",
        "# running reward: 10.39 at episode 17337, frame count 1400000\n",
        "# running reward: 9.91 at episode 17371, frame count 1410000\n",
        "# running reward: 10.86 at episode 17402, frame count 1420000\n",
        "# running reward: 10.22 at episode 17438, frame count 1430000\n",
        "# running reward: 10.34 at episode 17474, frame count 1440000\n",
        "# running reward: 10.38 at episode 17505, frame count 1450000\n",
        "# running reward: 10.91 at episode 17538, frame count 1460000\n",
        "# running reward: 10.25 at episode 17576, frame count 1470000\n",
        "# running reward: 9.19 at episode 17613, frame count 1480000\n",
        "# running reward: 9.26 at episode 17652, frame count 1490000\n",
        "# running reward: 9.89 at episode 17685, frame count 1500000\n",
        "# running reward: 9.43 at episode 17724, frame count 1510000\n",
        "# running reward: 9.27 at episode 17758, frame count 1520000\n",
        "# running reward: 9.21 at episode 17791, frame count 1530000\n",
        "# running reward: 9.28 at episode 17831, frame count 1540000\n",
        "# running reward: 9.60 at episode 17866, frame count 1550000\n",
        "# running reward: 9.39 at episode 17902, frame count 1560000\n",
        "# running reward: 10.39 at episode 17937, frame count 1570000\n",
        "# running reward: 9.72 at episode 17975, frame count 1580000\n",
        "# running reward: 10.11 at episode 18005, frame count 1590000\n",
        "# running reward: 9.97 at episode 18040, frame count 1600000\n",
        "# running reward: 10.07 at episode 18077, frame count 1610000\n",
        "# running reward: 9.36 at episode 18114, frame count 1620000\n",
        "# running reward: 9.33 at episode 18151, frame count 1630000\n",
        "# running reward: 8.81 at episode 18191, frame count 1640000\n",
        "# running reward: 7.91 at episode 18230, frame count 1650000\n",
        "# running reward: 8.76 at episode 18264, frame count 1660000\n",
        "# running reward: 9.51 at episode 18304, frame count 1670000\n",
        "# running reward: 8.86 at episode 18346, frame count 1680000\n",
        "# running reward: 8.52 at episode 18383, frame count 1690000\n",
        "# running reward: 8.88 at episode 18416, frame count 1700000\n",
        "# running reward: 9.83 at episode 18451, frame count 1710000\n",
        "# running reward: 9.74 at episode 18487, frame count 1720000\n",
        "# running reward: 10.33 at episode 18520, frame count 1730000\n",
        "# running reward: 10.37 at episode 18551, frame count 1740000\n",
        "# running reward: 10.67 at episode 18590, frame count 1750000\n",
        "# running reward: 9.78 at episode 18624, frame count 1760000\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXTQ524BGpfh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}